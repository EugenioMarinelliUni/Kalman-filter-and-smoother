{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cec3479-f742-43b9-bf18-465c8f0cea8c",
   "metadata": {
    "id": "1cec3479-f742-43b9-bf18-465c8f0cea8c"
   },
   "source": [
    "# Libraries and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c34386-4299-4e42-96f6-427a80c88606",
   "metadata": {
    "id": "d0c34386-4299-4e42-96f6-427a80c88606"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6eb48-04be-44ec-bebe-10ede8b098ea",
   "metadata": {
    "id": "68b6eb48-04be-44ec-bebe-10ede8b098ea",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351fe608-9cb7-415c-a376-fdabdb9f7f12",
   "metadata": {
    "id": "351fe608-9cb7-415c-a376-fdabdb9f7f12"
   },
   "outputs": [],
   "source": [
    "%pip install google-colab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bf8dc-4ee5-401a-8f01-f00f66920ef0",
   "metadata": {
    "id": "724bf8dc-4ee5-401a-8f01-f00f66920ef0"
   },
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import io\n",
    "from google.colab import files # (used to import dataset)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa618b-b2d0-440b-9127-a99513100168",
   "metadata": {
    "id": "31fa618b-b2d0-440b-9127-a99513100168"
   },
   "outputs": [],
   "source": [
    "# time-series-specific libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "# from tsmoothie.smoother import *\n",
    "# from tsmoothie.utils_func import create_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2a606-2faf-4b54-8311-ee1fb6c8a2d0",
   "metadata": {
    "id": "7dd2a606-2faf-4b54-8311-ee1fb6c8a2d0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cf629-d4a1-4fc5-87a0-0e3fcaf3e478",
   "metadata": {
    "id": "530cf629-d4a1-4fc5-87a0-0e3fcaf3e478"
   },
   "source": [
    "## Uploading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15f66f-5bcf-41bc-be61-3bc627526031",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b15f66f-5bcf-41bc-be61-3bc627526031",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733165743473,
     "user_tz": -60,
     "elapsed": 1258,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "e252f1bd-3c28-4499-a1c1-049d6a027965"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FRED-MD dataset downloaded successfully.\n",
      "                 RPI  W875RX1  DPCERA3M086SBEA    CMRMTSPLx      RETAILx  \\\n",
      "sasdate                                                                    \n",
      "Transform:     5.000      5.0            5.000       5.0000      5.00000   \n",
      "1/1/1959    2583.560   2426.0           15.188  276676.8154  18235.77392   \n",
      "2/1/1959    2593.596   2434.8           15.346  278713.9773  18369.56308   \n",
      "3/1/1959    2610.396   2452.7           15.491  277775.2539  18523.05762   \n",
      "4/1/1959    2627.446   2470.0           15.435  283362.7075  18534.46600   \n",
      "\n",
      "             INDPRO  IPFPNSS  IPFINAL  IPCONGD  IPDCONGD  ...  \\\n",
      "sasdate                                                   ...   \n",
      "Transform:   5.0000   5.0000   5.0000   5.0000    5.0000  ...   \n",
      "1/1/1959    21.9616  23.3868  22.2620  31.6664   18.9498  ...   \n",
      "2/1/1959    22.3917  23.7024  22.4549  31.8987   19.0492  ...   \n",
      "3/1/1959    22.7142  23.8459  22.5651  31.8987   19.4223  ...   \n",
      "4/1/1959    23.1981  24.1903  22.8957  32.4019   19.5466  ...   \n",
      "\n",
      "            DNDGRG3M086SBEA  DSERRG3M086SBEA  CES0600000008  CES2000000008  \\\n",
      "sasdate                                                                      \n",
      "Transform:            6.000            6.000           6.00           6.00   \n",
      "1/1/1959             18.294           10.152           2.13           2.45   \n",
      "2/1/1959             18.302           10.167           2.14           2.46   \n",
      "3/1/1959             18.289           10.185           2.15           2.45   \n",
      "4/1/1959             18.300           10.221           2.16           2.47   \n",
      "\n",
      "            CES3000000008  UMCSENTx  DTCOLNVHFNM  DTCTHFNM   INVEST  VIXCLSx  \n",
      "sasdate                                                                       \n",
      "Transform:           6.00       2.0          6.0       6.0   6.0000      1.0  \n",
      "1/1/1959             2.04       NaN       6476.0   12298.0  84.2043      NaN  \n",
      "2/1/1959             2.05       NaN       6476.0   12298.0  83.5280      NaN  \n",
      "3/1/1959             2.07       NaN       6508.0   12349.0  81.6405      NaN  \n",
      "4/1/1959             2.08       NaN       6620.0   12484.0  81.8099      NaN  \n",
      "\n",
      "[5 rows x 126 columns]\n",
      "The column 'sasdate' is not present in the DataFrame.\n",
      "0: RPI\n",
      "1: W875RX1\n",
      "2: DPCERA3M086SBEA\n",
      "3: CMRMTSPLx\n",
      "4: RETAILx\n",
      "5: INDPRO\n",
      "6: IPFPNSS\n",
      "7: IPFINAL\n",
      "8: IPCONGD\n",
      "9: IPDCONGD\n",
      "10: IPNCONGD\n",
      "11: IPBUSEQ\n",
      "12: IPMAT\n",
      "13: IPDMAT\n",
      "14: IPNMAT\n",
      "15: IPMANSICS\n",
      "16: IPB51222S\n",
      "17: IPFUELS\n",
      "18: CUMFNS\n",
      "19: HWI\n",
      "20: HWIURATIO\n",
      "21: CLF16OV\n",
      "22: CE16OV\n",
      "23: UNRATE\n",
      "24: UEMPMEAN\n",
      "25: UEMPLT5\n",
      "26: UEMP5TO14\n",
      "27: UEMP15OV\n",
      "28: UEMP15T26\n",
      "29: UEMP27OV\n",
      "30: CLAIMSx\n",
      "31: PAYEMS\n",
      "32: USGOOD\n",
      "33: CES1021000001\n",
      "34: USCONS\n",
      "35: MANEMP\n",
      "36: DMANEMP\n",
      "37: NDMANEMP\n",
      "38: SRVPRD\n",
      "39: USTPU\n",
      "40: USWTRADE\n",
      "41: USTRADE\n",
      "42: USFIRE\n",
      "43: USGOVT\n",
      "44: CES0600000007\n",
      "45: AWOTMAN\n",
      "46: AWHMAN\n",
      "47: HOUST\n",
      "48: HOUSTNE\n",
      "49: HOUSTMW\n",
      "50: HOUSTS\n",
      "51: HOUSTW\n",
      "52: PERMIT\n",
      "53: PERMITNE\n",
      "54: PERMITMW\n",
      "55: PERMITS\n",
      "56: PERMITW\n",
      "57: ACOGNO\n",
      "58: AMDMNOx\n",
      "59: ANDENOx\n",
      "60: AMDMUOx\n",
      "61: BUSINVx\n",
      "62: ISRATIOx\n",
      "63: M1SL\n",
      "64: M2SL\n",
      "65: M2REAL\n",
      "66: BOGMBASE\n",
      "67: TOTRESNS\n",
      "68: NONBORRES\n",
      "69: BUSLOANS\n",
      "70: REALLN\n",
      "71: NONREVSL\n",
      "72: CONSPI\n",
      "73: S&P 500\n",
      "74: S&P div yield\n",
      "75: S&P PE ratio\n",
      "76: FEDFUNDS\n",
      "77: CP3Mx\n",
      "78: TB3MS\n",
      "79: TB6MS\n",
      "80: GS1\n",
      "81: GS5\n",
      "82: GS10\n",
      "83: AAA\n",
      "84: BAA\n",
      "85: COMPAPFFx\n",
      "86: TB3SMFFM\n",
      "87: TB6SMFFM\n",
      "88: T1YFFM\n",
      "89: T5YFFM\n",
      "90: T10YFFM\n",
      "91: AAAFFM\n",
      "92: BAAFFM\n",
      "93: TWEXAFEGSMTHx\n",
      "94: EXSZUSx\n",
      "95: EXJPUSx\n",
      "96: EXUSUKx\n",
      "97: EXCAUSx\n",
      "98: WPSFD49207\n",
      "99: WPSFD49502\n",
      "100: WPSID61\n",
      "101: WPSID62\n",
      "102: OILPRICEx\n",
      "103: PPICMM\n",
      "104: CPIAUCSL\n",
      "105: CPIAPPSL\n",
      "106: CPITRNSL\n",
      "107: CPIMEDSL\n",
      "108: CUSR0000SAC\n",
      "109: CUSR0000SAD\n",
      "110: CUSR0000SAS\n",
      "111: CPIULFSL\n",
      "112: CUSR0000SA0L2\n",
      "113: CUSR0000SA0L5\n",
      "114: PCEPI\n",
      "115: DDURRG3M086SBEA\n",
      "116: DNDGRG3M086SBEA\n",
      "117: DSERRG3M086SBEA\n",
      "118: CES0600000008\n",
      "119: CES2000000008\n",
      "120: CES3000000008\n",
      "121: UMCSENTx\n",
      "122: DTCOLNVHFNM\n",
      "123: DTCTHFNM\n",
      "124: INVEST\n",
      "125: VIXCLSx\n",
      "Index(['Transform:', '1/1/1959', '2/1/1959', '3/1/1959', '4/1/1959',\n",
      "       '5/1/1959', '6/1/1959', '7/1/1959', '8/1/1959', '9/1/1959',\n",
      "       ...\n",
      "       '12/1/2023', '1/1/2024', '2/1/2024', '3/1/2024', '4/1/2024', '5/1/2024',\n",
      "       '6/1/2024', '7/1/2024', '8/1/2024', '9/1/2024'],\n",
      "      dtype='object', name='sasdate', length=790)\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Check if the dataset already exists\n",
    "file_path = 'fred_md.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    url = \"https://files.stlouisfed.org/files/htdocs/fred-md/monthly/current.csv\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"FRED-MD dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"FRED-MD dataset already exists locally.\")\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "fred_md_df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(fred_md_df.head())\n",
    "\n",
    "if 'sasdate' in fred_md_df.columns:\n",
    "    print(\"The column 'sasdate' is present in the DataFrame.\")\n",
    "else:\n",
    "    print(\"The column 'sasdate' is not present in the DataFrame.\")\n",
    "\n",
    "for idx, col in enumerate(fred_md_df.columns):\n",
    "    print(f\"{idx}: {col}\")\n",
    "\n",
    "print(fred_md_df.index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "From dataset to dataframe\n"
   ],
   "metadata": {
    "id": "tPMEBdqMzAAK"
   },
   "id": "tPMEBdqMzAAK"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "!pip install fredapi\n",
    "from fredapi import Fred\n",
    "\n",
    "# Ensure required libraries are installed\n",
    "# !pip install pandas fredapi\n",
    "\n",
    "# Replace 'your_api_key_here' with your actual FRED API key\n",
    "fred = Fred(api_key='feabb7180fc1f516e63d0b320e07e6dd')\n",
    "\n",
    "# File path for the locally saved data\n",
    "file_path = 'GDP_Percent_Change.csv'\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    # Download GDP Percent Change from FRED\n",
    "    gdp_data = fred.get_series('A191RL1Q225SBEA', observation_start='1947-04-01', observation_end='2024-09-30')\n",
    "\n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    gdp_df = pd.DataFrame({'Date': gdp_data.index, 'GDP Percent Change': gdp_data.values})\n",
    "\n",
    "    # Save to CSV\n",
    "    gdp_df.to_csv(file_path, index=False)\n",
    "    print(\"Data downloaded and saved to CSV.\")\n",
    "else:\n",
    "    # Load the data from the existing CSV file\n",
    "    gdp_df = pd.read_csv(file_path)\n",
    "    print(\"Data loaded from existing CSV file.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(gdp_df.head())\n",
    "\n",
    "for idx, col in enumerate(gdp_df.columns):\n",
    "    print(f\"{idx}: {col}\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWPIA67LzIH3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733165754699,
     "user_tz": -60,
     "elapsed": 6240,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "ccf65eb6-de4f-4c36-eaac-6a07fbb9f5b0"
   },
   "id": "FWPIA67LzIH3",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting fredapi\n",
      "  Downloading fredapi-0.5.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fredapi) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.16.0)\n",
      "Downloading fredapi-0.5.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: fredapi\n",
      "Successfully installed fredapi-0.5.2\n",
      "Data downloaded and saved to CSV.\n",
      "        Date  GDP Percent Change\n",
      "0 1947-04-01                -1.0\n",
      "1 1947-07-01                -0.8\n",
      "2 1947-10-01                 6.4\n",
      "3 1948-01-01                 6.2\n",
      "4 1948-04-01                 6.8\n",
      "0: Date\n",
      "1: GDP Percent Change\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data manipulation"
   ],
   "metadata": {
    "id": "SuZ3CHrhz-VS"
   },
   "id": "SuZ3CHrhz-VS"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gdp dataframe manipulation"
   ],
   "metadata": {
    "id": "2pw03oHp0m53"
   },
   "id": "2pw03oHp0m53"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import numpy as np\n",
    "\n",
    "# Check if the 'Date' column exists and handle accordingly\n",
    "if 'Date' in gdp_df.columns:\n",
    "    # Convert 'Date' column to datetime if not already done\n",
    "    if gdp_df['Date'].dtype != 'datetime64[ns]':\n",
    "        gdp_df['Date'] = pd.to_datetime(gdp_df['Date'])\n",
    "        print(\"Date column converted to datetime format.\")\n",
    "    else:\n",
    "        print(\"Date column already in datetime format.\")\n",
    "\n",
    "    # Apply the date offset transformation\n",
    "    gdp_df['Date'] = gdp_df['Date'] - DateOffset(months=1)\n",
    "    gdp_df = gdp_df.set_index('Date')\n",
    "    print(\"Date offset applied, and 'Date' set as index.\")\n",
    "else:\n",
    "    print(\"Error: 'Date' column is not present in the DataFrame.\")\n",
    "\n",
    "for col in gdp_df.columns:\n",
    "    print(col)\n",
    "\n",
    "## Checking for missing values in the GDP series ##\n",
    "column_names = gdp_df.columns\n",
    "print(column_names)\n",
    "missing_data = gdp_df[gdp_df['GDP Percent Change'].isnull()]\n",
    "indexes = missing_data.index.tolist()\n",
    "print(indexes)\n",
    "\n",
    "# Function to check stationarity and transform if necessary\n",
    "def ensure_stationarity(series, name):\n",
    "    adf_test = adfuller(series.dropna())\n",
    "    print(f\"{name}: ADF Statistic={adf_test[0]}, p-value={adf_test[1]}\")\n",
    "\n",
    "    if adf_test[1] < 0.05:\n",
    "        print(f\"{name} is stationary.\")\n",
    "        return series  # Already stationary\n",
    "    else:\n",
    "        print(f\"{name} is not stationary. Differencing will be applied.\")\n",
    "        return series.diff().dropna()  # Apply differencing to ensure stationarity\n",
    "\n",
    "# Ensure GDP stationarity\n",
    "gdp_df['GDP Percent Change'] = ensure_stationarity(gdp_df['GDP Percent Change'], 'GDP Percent Change')\n",
    "\n",
    "# Display the first few rows\n",
    "print(gdp_df.head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOZnWMiBl8wd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733165766176,
     "user_tz": -60,
     "elapsed": 289,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "5614f024-7b14-4ae3-a682-f6e9dcdee06f"
   },
   "id": "VOZnWMiBl8wd",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Date column already in datetime format.\n",
      "Date offset applied, and 'Date' set as index.\n",
      "GDP Percent Change\n",
      "Index(['GDP Percent Change'], dtype='object')\n",
      "[]\n",
      "GDP Percent Change: ADF Statistic=-15.364361650427478, p-value=3.5773742513290027e-28\n",
      "GDP Percent Change is stationary.\n",
      "            GDP Percent Change\n",
      "Date                          \n",
      "1947-03-01                -1.0\n",
      "1947-06-01                -0.8\n",
      "1947-09-01                 6.4\n",
      "1947-12-01                 6.2\n",
      "1948-03-01                 6.8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fred Md dataframe manipulations"
   ],
   "metadata": {
    "id": "cccEzk9M-k8O"
   },
   "id": "cccEzk9M-k8O"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "\n",
    "# Initialize FRED API\n",
    "fred = Fred(api_key='feabb7180fc1f516e63d0b320e07e6dd')\n",
    "\n",
    "# Display column indexes for verification\n",
    "for idx, col in enumerate(fred_md_df.columns):\n",
    "    print(f\"{idx}: {col}\")\n",
    "\n",
    "\n",
    "\n",
    "# Check if the file 'modified_fred_md_df.csv' already exists\n",
    "if os.path.exists('modified_fred_md_df.csv'):\n",
    "    # Load the existing file\n",
    "    modified_fred_md_df = pd.read_csv('modified_fred_md_df.csv', index_col='Date', parse_dates=True)\n",
    "    print(\"Dataframe ready. File with 'Date' as index already exists.\")\n",
    "else:\n",
    "    try:\n",
    "        # Check if 'sasdate' is the index\n",
    "        if fred_md_df.index.name == 'sasdate':\n",
    "            # Ensure 'sasdate' becomes a column\n",
    "            fred_md_df.reset_index(inplace=True)\n",
    "            print(\"'sasdate' was an index and has been reset to a column.\")\n",
    "\n",
    "            # Perform initial transformations\n",
    "            fred_md_df.drop(fred_md_df.index[:1], inplace=True)\n",
    "            fred_md_df.rename(columns={'sasdate': 'Date'}, inplace=True)\n",
    "            fred_md_df['Date'] = pd.to_datetime(fred_md_df['Date'])\n",
    "            fred_md_df.set_index('Date', inplace=True)\n",
    "            fred_md_df.sort_index(inplace=True)\n",
    "            print(\"Initial transformations completed.\")\n",
    "\n",
    "            # Additional operations: Remove specified series and add new series\n",
    "            indexes_to_remove = [19, 20, 57, 93, 121]\n",
    "            fred_md_df.drop(fred_md_df.columns[indexes_to_remove], axis=1, inplace=True)\n",
    "            print(\"Specified series removed by index.\")\n",
    "\n",
    "            # Save the modified DataFrame\n",
    "            fred_md_df.to_csv('modified_fred_md_df.csv')\n",
    "            print(\"Dataset updated and saved as 'modified_fred_md_df.csv'.\")\n",
    "            modified_fred_md_df = fred_md_df\n",
    "        else:\n",
    "            print(\"Error: 'sasdate' index not found in the DataFrame.\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - The specified columns or indexes might be missing.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e} - Issue with data type conversion or merging.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Display the columns of the modified DataFrame to confirm successful transformation\n",
    "print(\"\\nColumns in the modified DataFrame:\")\n",
    "for idx, col in enumerate(modified_fred_md_df.columns):\n",
    "     print(f\"{idx}: {col}\")\n",
    "\n",
    "# # Display the first few rows of the modified dataset\n",
    "# print(fred_md_df.head())\n",
    "\n",
    "# # Display column indexes for verification\n",
    "# for idx, col in enumerate(fred_md_df.columns):\n",
    "#     print(f\"{idx}: {col}\")\n",
    "\n",
    "# # Display column indexes for verification\n",
    "#\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "072UKUW7-qTs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733165775243,
     "user_tz": -60,
     "elapsed": 596,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "0d5194e7-6ae1-477c-e427-4fa35f78545a"
   },
   "id": "072UKUW7-qTs",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0: RPI\n",
      "1: W875RX1\n",
      "2: DPCERA3M086SBEA\n",
      "3: CMRMTSPLx\n",
      "4: RETAILx\n",
      "5: INDPRO\n",
      "6: IPFPNSS\n",
      "7: IPFINAL\n",
      "8: IPCONGD\n",
      "9: IPDCONGD\n",
      "10: IPNCONGD\n",
      "11: IPBUSEQ\n",
      "12: IPMAT\n",
      "13: IPDMAT\n",
      "14: IPNMAT\n",
      "15: IPMANSICS\n",
      "16: IPB51222S\n",
      "17: IPFUELS\n",
      "18: CUMFNS\n",
      "19: HWI\n",
      "20: HWIURATIO\n",
      "21: CLF16OV\n",
      "22: CE16OV\n",
      "23: UNRATE\n",
      "24: UEMPMEAN\n",
      "25: UEMPLT5\n",
      "26: UEMP5TO14\n",
      "27: UEMP15OV\n",
      "28: UEMP15T26\n",
      "29: UEMP27OV\n",
      "30: CLAIMSx\n",
      "31: PAYEMS\n",
      "32: USGOOD\n",
      "33: CES1021000001\n",
      "34: USCONS\n",
      "35: MANEMP\n",
      "36: DMANEMP\n",
      "37: NDMANEMP\n",
      "38: SRVPRD\n",
      "39: USTPU\n",
      "40: USWTRADE\n",
      "41: USTRADE\n",
      "42: USFIRE\n",
      "43: USGOVT\n",
      "44: CES0600000007\n",
      "45: AWOTMAN\n",
      "46: AWHMAN\n",
      "47: HOUST\n",
      "48: HOUSTNE\n",
      "49: HOUSTMW\n",
      "50: HOUSTS\n",
      "51: HOUSTW\n",
      "52: PERMIT\n",
      "53: PERMITNE\n",
      "54: PERMITMW\n",
      "55: PERMITS\n",
      "56: PERMITW\n",
      "57: ACOGNO\n",
      "58: AMDMNOx\n",
      "59: ANDENOx\n",
      "60: AMDMUOx\n",
      "61: BUSINVx\n",
      "62: ISRATIOx\n",
      "63: M1SL\n",
      "64: M2SL\n",
      "65: M2REAL\n",
      "66: BOGMBASE\n",
      "67: TOTRESNS\n",
      "68: NONBORRES\n",
      "69: BUSLOANS\n",
      "70: REALLN\n",
      "71: NONREVSL\n",
      "72: CONSPI\n",
      "73: S&P 500\n",
      "74: S&P div yield\n",
      "75: S&P PE ratio\n",
      "76: FEDFUNDS\n",
      "77: CP3Mx\n",
      "78: TB3MS\n",
      "79: TB6MS\n",
      "80: GS1\n",
      "81: GS5\n",
      "82: GS10\n",
      "83: AAA\n",
      "84: BAA\n",
      "85: COMPAPFFx\n",
      "86: TB3SMFFM\n",
      "87: TB6SMFFM\n",
      "88: T1YFFM\n",
      "89: T5YFFM\n",
      "90: T10YFFM\n",
      "91: AAAFFM\n",
      "92: BAAFFM\n",
      "93: TWEXAFEGSMTHx\n",
      "94: EXSZUSx\n",
      "95: EXJPUSx\n",
      "96: EXUSUKx\n",
      "97: EXCAUSx\n",
      "98: WPSFD49207\n",
      "99: WPSFD49502\n",
      "100: WPSID61\n",
      "101: WPSID62\n",
      "102: OILPRICEx\n",
      "103: PPICMM\n",
      "104: CPIAUCSL\n",
      "105: CPIAPPSL\n",
      "106: CPITRNSL\n",
      "107: CPIMEDSL\n",
      "108: CUSR0000SAC\n",
      "109: CUSR0000SAD\n",
      "110: CUSR0000SAS\n",
      "111: CPIULFSL\n",
      "112: CUSR0000SA0L2\n",
      "113: CUSR0000SA0L5\n",
      "114: PCEPI\n",
      "115: DDURRG3M086SBEA\n",
      "116: DNDGRG3M086SBEA\n",
      "117: DSERRG3M086SBEA\n",
      "118: CES0600000008\n",
      "119: CES2000000008\n",
      "120: CES3000000008\n",
      "121: UMCSENTx\n",
      "122: DTCOLNVHFNM\n",
      "123: DTCTHFNM\n",
      "124: INVEST\n",
      "125: VIXCLSx\n",
      "'sasdate' was an index and has been reset to a column.\n",
      "Initial transformations completed.\n",
      "Specified series removed by index.\n",
      "Dataset updated and saved as 'modified_fred_md_df.csv'.\n",
      "\n",
      "Columns in the modified DataFrame:\n",
      "0: RPI\n",
      "1: W875RX1\n",
      "2: DPCERA3M086SBEA\n",
      "3: CMRMTSPLx\n",
      "4: RETAILx\n",
      "5: INDPRO\n",
      "6: IPFPNSS\n",
      "7: IPFINAL\n",
      "8: IPCONGD\n",
      "9: IPDCONGD\n",
      "10: IPNCONGD\n",
      "11: IPBUSEQ\n",
      "12: IPMAT\n",
      "13: IPDMAT\n",
      "14: IPNMAT\n",
      "15: IPMANSICS\n",
      "16: IPB51222S\n",
      "17: IPFUELS\n",
      "18: CUMFNS\n",
      "19: CLF16OV\n",
      "20: CE16OV\n",
      "21: UNRATE\n",
      "22: UEMPMEAN\n",
      "23: UEMPLT5\n",
      "24: UEMP5TO14\n",
      "25: UEMP15OV\n",
      "26: UEMP15T26\n",
      "27: UEMP27OV\n",
      "28: CLAIMSx\n",
      "29: PAYEMS\n",
      "30: USGOOD\n",
      "31: CES1021000001\n",
      "32: USCONS\n",
      "33: MANEMP\n",
      "34: DMANEMP\n",
      "35: NDMANEMP\n",
      "36: SRVPRD\n",
      "37: USTPU\n",
      "38: USWTRADE\n",
      "39: USTRADE\n",
      "40: USFIRE\n",
      "41: USGOVT\n",
      "42: CES0600000007\n",
      "43: AWOTMAN\n",
      "44: AWHMAN\n",
      "45: HOUST\n",
      "46: HOUSTNE\n",
      "47: HOUSTMW\n",
      "48: HOUSTS\n",
      "49: HOUSTW\n",
      "50: PERMIT\n",
      "51: PERMITNE\n",
      "52: PERMITMW\n",
      "53: PERMITS\n",
      "54: PERMITW\n",
      "55: AMDMNOx\n",
      "56: ANDENOx\n",
      "57: AMDMUOx\n",
      "58: BUSINVx\n",
      "59: ISRATIOx\n",
      "60: M1SL\n",
      "61: M2SL\n",
      "62: M2REAL\n",
      "63: BOGMBASE\n",
      "64: TOTRESNS\n",
      "65: NONBORRES\n",
      "66: BUSLOANS\n",
      "67: REALLN\n",
      "68: NONREVSL\n",
      "69: CONSPI\n",
      "70: S&P 500\n",
      "71: S&P div yield\n",
      "72: S&P PE ratio\n",
      "73: FEDFUNDS\n",
      "74: CP3Mx\n",
      "75: TB3MS\n",
      "76: TB6MS\n",
      "77: GS1\n",
      "78: GS5\n",
      "79: GS10\n",
      "80: AAA\n",
      "81: BAA\n",
      "82: COMPAPFFx\n",
      "83: TB3SMFFM\n",
      "84: TB6SMFFM\n",
      "85: T1YFFM\n",
      "86: T5YFFM\n",
      "87: T10YFFM\n",
      "88: AAAFFM\n",
      "89: BAAFFM\n",
      "90: EXSZUSx\n",
      "91: EXJPUSx\n",
      "92: EXUSUKx\n",
      "93: EXCAUSx\n",
      "94: WPSFD49207\n",
      "95: WPSFD49502\n",
      "96: WPSID61\n",
      "97: WPSID62\n",
      "98: OILPRICEx\n",
      "99: PPICMM\n",
      "100: CPIAUCSL\n",
      "101: CPIAPPSL\n",
      "102: CPITRNSL\n",
      "103: CPIMEDSL\n",
      "104: CUSR0000SAC\n",
      "105: CUSR0000SAD\n",
      "106: CUSR0000SAS\n",
      "107: CPIULFSL\n",
      "108: CUSR0000SA0L2\n",
      "109: CUSR0000SA0L5\n",
      "110: PCEPI\n",
      "111: DDURRG3M086SBEA\n",
      "112: DNDGRG3M086SBEA\n",
      "113: DSERRG3M086SBEA\n",
      "114: CES0600000008\n",
      "115: CES2000000008\n",
      "116: CES3000000008\n",
      "117: DTCOLNVHFNM\n",
      "118: DTCTHFNM\n",
      "119: INVEST\n",
      "120: VIXCLSx\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transforming the series to ensure stationarity; grouping the modified  series by type (Soybilgen and Yazgan, 2021)\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "skAA2kQKN-Rl"
   },
   "id": "skAA2kQKN-Rl"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv('modified_fred_md_df.csv', index_col=0, parse_dates=True)\n",
    "print(df.head())\n",
    "# # Filter data to include only from January 2000 onward\n",
    "# df = df.loc['2000-01-01':]\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "## Step 1: Removing NONBORRES, adding BORROW, then performing checks ##\n",
    "## Why did I decide to remove NONBORRES? Due to the presence of negative values, that made impossible to apply the proposed transformation (monthly growth rate).\n",
    "## I decided to drop this series and replace it with BORROW, due to the relation Nonborrowed reserves (NONBORRES) equals total reserves (TOTRESNS), less total borrowings from the Federal Reserve (BORROW).\n",
    "\n",
    "# Replace NONBORRES with BORROW\n",
    "nonborres_index = df.columns.get_loc(\"NONBORRES\")  # Get the index of NONBORRES\n",
    "nonborres_column = df.columns[nonborres_index]    # Get the column name\n",
    "\n",
    "print(f\"Index of NONBORRES: {nonborres_index}\")\n",
    "print(f\"Column name of NONBORRES: {nonborres_column}\")\n",
    "\n",
    "##### Insert BORROW as new column####\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "\n",
    "### Step 1: Fetch the entire BORROW series from FRED using fredapi ###\n",
    "\n",
    "fred = Fred(api_key=\"feabb7180fc1f516e63d0b320e07e6dd\")\n",
    "borrow_series = fred.get_series(\"BORROW\")\n",
    "\n",
    "### Step 2: Inspect the BORROW series ###\n",
    "print(\"First few rows of BORROW series:\")\n",
    "print(borrow_series.head())\n",
    "\n",
    "print(\"\\nIndex of BORROW series:\")\n",
    "print(borrow_series.index)\n",
    "\n",
    "print(\"\\nSummary of BORROW series:\")\n",
    "print(borrow_series.info())\n",
    "\n",
    "### Step 3: Align BORROW series with the existing DataFrame's index ###\n",
    "### Filter BORROW series to start from the earliest date in df ###\n",
    "oldest_date_in_df = df.index.min()\n",
    "print(f\"Earliest date in df: {oldest_date_in_df}\")\n",
    "borrow_series_filtered = borrow_series.loc[oldest_date_in_df:]\n",
    "print(f\"Filtered BORROW series (from {oldest_date_in_df}): {borrow_series_filtered}\")\n",
    "\n",
    "### Step 4: Reindex the BORROW series to match the index of df ###\n",
    "borrow_series_filtered = borrow_series_filtered.reindex(df.index)\n",
    "\n",
    "### Step 5: Add the BORROW series as a new column in your DataFrame ###\n",
    "df['BORROW'] = borrow_series_filtered\n",
    "\n",
    "### Step 6: Check the columns of the DataFrame ###\n",
    "print(\"\\nColumns in the DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nColumns in the modified DataFrame:\")\n",
    "for idx, col in enumerate(df.columns):\n",
    "     print(f\"{idx}: {col}\")\n",
    "\n",
    "### Step 7: Print the last column of the DataFrame (which corresponds to the last series added) ###\n",
    "print(\"\\nLast column (series added):\")\n",
    "print(df.iloc[:, -1])  # Access the last column using iloc\n",
    "\n",
    "### Step 8: Print a list of column names ###\n",
    "print(\"\\nList of column names:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "### Drop NONBORRES ###\n",
    "df.drop(columns=[nonborres_column], inplace=True)\n",
    "\n",
    "### Step 8: Print a list of column names ###\n",
    "print(\"\\nList of column names:\")\n",
    "print(list(df.columns))\n",
    "print(df)\n",
    "################################################################################\n",
    "\n",
    "## Step 2: Define the indices for each transformation ##\n",
    "monthly_growth_rate_indices = list(range(0, 18)) + [19, 20] + list(range(23, 42)) + list(range(45, 59)) + list(range(60, 72)) + list(range(89, 93)) + list(range(116, 121))\n",
    "monthly_difference_indices = [18, 21, 22] + list(range(42, 45)) + [59] + list(range(72, 81))\n",
    "monthly_diff_yearly_growth_rate_indices = list(range(93, 116))\n",
    "no_transformation_indices = list(range(81, 89))\n",
    "print(monthly_growth_rate_indices)\n",
    "print(monthly_difference_indices)\n",
    "print(monthly_diff_yearly_growth_rate_indices)\n",
    "print(no_transformation_indices)\n",
    "################################################################################\n",
    "\n",
    "## Step 3: Checking for the presence of missing values and non-positive values in the dataframe ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a report for missing and non-positive values\n",
    "report = {}\n",
    "\n",
    "# Check for missing and non-positive values\n",
    "for group_name, indices in {\n",
    "    \"monthly_growth_rate\": monthly_growth_rate_indices,\n",
    "    \"monthly_difference\": monthly_difference_indices,\n",
    "    \"monthly_diff_yearly_growth_rate\": monthly_diff_yearly_growth_rate_indices,\n",
    "    \"no_transformation\": no_transformation_indices,\n",
    "}.items():\n",
    "    group_report = {}\n",
    "    for idx in indices:\n",
    "        column_name = df.columns[idx]\n",
    "        column_data = df.iloc[:, idx]\n",
    "\n",
    "        # Missing values\n",
    "        missing = column_data.isna()\n",
    "        missing_periods = column_data[missing].index.tolist()\n",
    "\n",
    "        # Non-positive values (only for log-transformed columns)\n",
    "        if group_name in {\"monthly_growth_rate\", \"monthly_diff_yearly_growth_rate\"}:\n",
    "            non_positive = column_data <= 0\n",
    "            non_positive_periods = column_data[non_positive].index.tolist()\n",
    "        else:\n",
    "            non_positive_periods = []\n",
    "\n",
    "        # Record issues\n",
    "        group_report[column_name] = {\n",
    "            \"missing_count\": missing.sum(),\n",
    "            \"missing_periods\": missing_periods,\n",
    "            \"non_positive_count\": len(non_positive_periods),\n",
    "            \"non_positive_periods\": non_positive_periods,\n",
    "        }\n",
    "\n",
    "    report[group_name] = group_report\n",
    "\n",
    "# Display the report\n",
    "for group_name, group_report in report.items():\n",
    "    print(f\"Group: {group_name}\")\n",
    "    for column_name, column_report in group_report.items():\n",
    "        print(f\"  Column: {column_name}\")\n",
    "        print(f\"    Missing values: {column_report['missing_count']}\")\n",
    "        if column_report['missing_periods']:\n",
    "            print(f\"    Missing periods: {column_report['missing_periods']}\")\n",
    "        if \"non_positive_count\" in column_report:\n",
    "            print(f\"    Non-positive values: {column_report['non_positive_count']}\")\n",
    "            if column_report['non_positive_periods']:\n",
    "                print(f\"    Non-positive periods: {column_report['non_positive_periods']}\")\n",
    "        print()\n",
    "\n",
    "#################### EXECUTE CODE SEQUENTIALLY UNTIL THIS LINE #########################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def interpolate_and_save(df, column_name, additional_column, missing_date, output_file):\n",
    "    \"\"\"\n",
    "    Fills missing values in specified columns using linear interpolation and saves the updated DataFrame to a file.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_name (str): The name of the primary column to interpolate.\n",
    "        additional_column (str): The name of the additional column to interpolate.\n",
    "        missing_date (str): The date of the missing value in 'YYYY-MM-DD' format.\n",
    "        output_file (str): The file path to save the updated DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the index is a datetime type for interpolation\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Perform linear interpolation on the specified columns\n",
    "    df[column_name] = df[column_name].interpolate(method='linear')\n",
    "    df[additional_column] = df[additional_column].interpolate(method='linear')\n",
    "\n",
    "    # Verify the missing values are filled\n",
    "    interpolated_value_main = df.loc[missing_date, column_name]\n",
    "    interpolated_value_additional = df.loc[missing_date, additional_column]\n",
    "\n",
    "    print(f\"Interpolated value for {column_name} on {missing_date}: {interpolated_value_main}\")\n",
    "    print(f\"Interpolated value for {additional_column} on {missing_date}: {interpolated_value_additional}\")\n",
    "\n",
    "    # Save the updated DataFrame to a file\n",
    "    df.to_csv(output_file)\n",
    "    print(f\"Updated DataFrame saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "interpolate_and_save(\n",
    "    df,  # The existing DataFrame\n",
    "    column_name=\"CP3Mx\",  # Primary column to interpolate\n",
    "    additional_column=\"COMPAPFFx\",  # Additional column to interpolate\n",
    "    missing_date=\"2020-04-01\",  # Date of the missing value\n",
    "    output_file=\"updated_dataframe.csv\"  # File to save the updated DataFrame\n",
    ")\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def check_missing_values_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Loads a DataFrame from a CSV file and checks for missing values,\n",
    "    specifying the column indexes and dates with missing data.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the missing data information.\n",
    "    \"\"\"\n",
    "    # Load the DataFrame from the CSV file\n",
    "    df = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n",
    "    print(f\"Loaded DataFrame from {file_path} with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_data = df.isna()\n",
    "\n",
    "    # Iterate through columns and find missing values\n",
    "    for idx, column in enumerate(df.columns):\n",
    "        missing_dates = missing_data.index[missing_data[column]].tolist()\n",
    "        if missing_dates:\n",
    "            print(f\"Column Index: {idx}, Column Name: {column}\")\n",
    "            print(f\"  Missing Dates: {missing_dates}\")\n",
    "\n",
    "# Example Usage\n",
    "csv_file_path = 'updated_dataframe.csv'  # Replace with the actual path to your CSV file\n",
    "check_missing_values_from_csv(csv_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJJRANkGN4Uh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733166009257,
     "user_tz": -60,
     "elapsed": 461,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "4588ad00-eda7-4886-a456-7b2037c9bc8d"
   },
   "id": "UJJRANkGN4Uh",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded DataFrame from updated_dataframe.csv with 789 rows and 121 columns.\n",
      "Column Index: 3, Column Name: CMRMTSPLx\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 50, Column Name: PERMIT\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00')]\n",
      "Column Index: 51, Column Name: PERMITNE\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00')]\n",
      "Column Index: 52, Column Name: PERMITMW\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00')]\n",
      "Column Index: 53, Column Name: PERMITS\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00')]\n",
      "Column Index: 54, Column Name: PERMITW\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00')]\n",
      "Column Index: 55, Column Name: AMDMNOx\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00'), Timestamp('1960-01-01 00:00:00'), Timestamp('1960-02-01 00:00:00'), Timestamp('1960-03-01 00:00:00'), Timestamp('1960-04-01 00:00:00'), Timestamp('1960-05-01 00:00:00'), Timestamp('1960-06-01 00:00:00'), Timestamp('1960-07-01 00:00:00'), Timestamp('1960-08-01 00:00:00'), Timestamp('1960-09-01 00:00:00'), Timestamp('1960-10-01 00:00:00'), Timestamp('1960-11-01 00:00:00'), Timestamp('1960-12-01 00:00:00'), Timestamp('1961-01-01 00:00:00'), Timestamp('1961-02-01 00:00:00'), Timestamp('1961-03-01 00:00:00'), Timestamp('1961-04-01 00:00:00'), Timestamp('1961-05-01 00:00:00'), Timestamp('1961-06-01 00:00:00'), Timestamp('1961-07-01 00:00:00'), Timestamp('1961-08-01 00:00:00'), Timestamp('1961-09-01 00:00:00'), Timestamp('1961-10-01 00:00:00'), Timestamp('1961-11-01 00:00:00'), Timestamp('1961-12-01 00:00:00'), Timestamp('1962-01-01 00:00:00'), Timestamp('1962-02-01 00:00:00'), Timestamp('1962-03-01 00:00:00'), Timestamp('1962-04-01 00:00:00'), Timestamp('1962-05-01 00:00:00'), Timestamp('1962-06-01 00:00:00'), Timestamp('1962-07-01 00:00:00'), Timestamp('1962-08-01 00:00:00'), Timestamp('1962-09-01 00:00:00'), Timestamp('1962-10-01 00:00:00'), Timestamp('1962-11-01 00:00:00'), Timestamp('1962-12-01 00:00:00'), Timestamp('1963-01-01 00:00:00'), Timestamp('1963-02-01 00:00:00'), Timestamp('1963-03-01 00:00:00'), Timestamp('1963-04-01 00:00:00'), Timestamp('1963-05-01 00:00:00'), Timestamp('1963-06-01 00:00:00'), Timestamp('1963-07-01 00:00:00'), Timestamp('1963-08-01 00:00:00'), Timestamp('1963-09-01 00:00:00'), Timestamp('1963-10-01 00:00:00'), Timestamp('1963-11-01 00:00:00'), Timestamp('1963-12-01 00:00:00'), Timestamp('1964-01-01 00:00:00'), Timestamp('1964-02-01 00:00:00'), Timestamp('1964-03-01 00:00:00'), Timestamp('1964-04-01 00:00:00'), Timestamp('1964-05-01 00:00:00'), Timestamp('1964-06-01 00:00:00'), Timestamp('1964-07-01 00:00:00'), Timestamp('1964-08-01 00:00:00'), Timestamp('1964-09-01 00:00:00'), Timestamp('1964-10-01 00:00:00'), Timestamp('1964-11-01 00:00:00'), Timestamp('1964-12-01 00:00:00'), Timestamp('1965-01-01 00:00:00'), Timestamp('1965-02-01 00:00:00'), Timestamp('1965-03-01 00:00:00'), Timestamp('1965-04-01 00:00:00'), Timestamp('1965-05-01 00:00:00'), Timestamp('1965-06-01 00:00:00'), Timestamp('1965-07-01 00:00:00'), Timestamp('1965-08-01 00:00:00'), Timestamp('1965-09-01 00:00:00'), Timestamp('1965-10-01 00:00:00'), Timestamp('1965-11-01 00:00:00'), Timestamp('1965-12-01 00:00:00'), Timestamp('1966-01-01 00:00:00'), Timestamp('1966-02-01 00:00:00'), Timestamp('1966-03-01 00:00:00'), Timestamp('1966-04-01 00:00:00'), Timestamp('1966-05-01 00:00:00'), Timestamp('1966-06-01 00:00:00'), Timestamp('1966-07-01 00:00:00'), Timestamp('1966-08-01 00:00:00'), Timestamp('1966-09-01 00:00:00'), Timestamp('1966-10-01 00:00:00'), Timestamp('1966-11-01 00:00:00'), Timestamp('1966-12-01 00:00:00'), Timestamp('1967-01-01 00:00:00'), Timestamp('1967-02-01 00:00:00'), Timestamp('1967-03-01 00:00:00'), Timestamp('1967-04-01 00:00:00'), Timestamp('1967-05-01 00:00:00'), Timestamp('1967-06-01 00:00:00'), Timestamp('1967-07-01 00:00:00'), Timestamp('1967-08-01 00:00:00'), Timestamp('1967-09-01 00:00:00'), Timestamp('1967-10-01 00:00:00'), Timestamp('1967-11-01 00:00:00'), Timestamp('1967-12-01 00:00:00'), Timestamp('1968-01-01 00:00:00'), Timestamp('1968-02-01 00:00:00'), Timestamp('1968-03-01 00:00:00'), Timestamp('1968-04-01 00:00:00'), Timestamp('1968-05-01 00:00:00'), Timestamp('1968-06-01 00:00:00'), Timestamp('1968-07-01 00:00:00'), Timestamp('1968-08-01 00:00:00'), Timestamp('1968-09-01 00:00:00'), Timestamp('1968-10-01 00:00:00'), Timestamp('1968-11-01 00:00:00'), Timestamp('1968-12-01 00:00:00'), Timestamp('1969-01-01 00:00:00'), Timestamp('1969-02-01 00:00:00'), Timestamp('1969-03-01 00:00:00'), Timestamp('1969-04-01 00:00:00'), Timestamp('1969-05-01 00:00:00'), Timestamp('1969-06-01 00:00:00'), Timestamp('1969-07-01 00:00:00'), Timestamp('1969-08-01 00:00:00'), Timestamp('1969-09-01 00:00:00'), Timestamp('1969-10-01 00:00:00'), Timestamp('1969-11-01 00:00:00'), Timestamp('1969-12-01 00:00:00'), Timestamp('1970-01-01 00:00:00'), Timestamp('1970-02-01 00:00:00'), Timestamp('1970-03-01 00:00:00'), Timestamp('1970-04-01 00:00:00'), Timestamp('1970-05-01 00:00:00'), Timestamp('1970-06-01 00:00:00'), Timestamp('1970-07-01 00:00:00'), Timestamp('1970-08-01 00:00:00'), Timestamp('1970-09-01 00:00:00'), Timestamp('1970-10-01 00:00:00'), Timestamp('1970-11-01 00:00:00'), Timestamp('1970-12-01 00:00:00'), Timestamp('1971-01-01 00:00:00'), Timestamp('1971-02-01 00:00:00'), Timestamp('1971-03-01 00:00:00'), Timestamp('1971-04-01 00:00:00'), Timestamp('1971-05-01 00:00:00'), Timestamp('1971-06-01 00:00:00'), Timestamp('1971-07-01 00:00:00'), Timestamp('1971-08-01 00:00:00'), Timestamp('1971-09-01 00:00:00'), Timestamp('1971-10-01 00:00:00'), Timestamp('1971-11-01 00:00:00'), Timestamp('1971-12-01 00:00:00'), Timestamp('1972-01-01 00:00:00'), Timestamp('1972-02-01 00:00:00'), Timestamp('1972-03-01 00:00:00'), Timestamp('1972-04-01 00:00:00'), Timestamp('1972-05-01 00:00:00'), Timestamp('1972-06-01 00:00:00'), Timestamp('1972-07-01 00:00:00'), Timestamp('1972-08-01 00:00:00'), Timestamp('1972-09-01 00:00:00'), Timestamp('1972-10-01 00:00:00'), Timestamp('1972-11-01 00:00:00'), Timestamp('1972-12-01 00:00:00'), Timestamp('1973-01-01 00:00:00'), Timestamp('1973-02-01 00:00:00'), Timestamp('1973-03-01 00:00:00'), Timestamp('1973-04-01 00:00:00'), Timestamp('1973-05-01 00:00:00'), Timestamp('1973-06-01 00:00:00'), Timestamp('1973-07-01 00:00:00'), Timestamp('1973-08-01 00:00:00'), Timestamp('1973-09-01 00:00:00'), Timestamp('1973-10-01 00:00:00'), Timestamp('1973-11-01 00:00:00'), Timestamp('1973-12-01 00:00:00'), Timestamp('1974-01-01 00:00:00'), Timestamp('1974-02-01 00:00:00'), Timestamp('1974-03-01 00:00:00'), Timestamp('1974-04-01 00:00:00'), Timestamp('1974-05-01 00:00:00'), Timestamp('1974-06-01 00:00:00'), Timestamp('1974-07-01 00:00:00'), Timestamp('1974-08-01 00:00:00'), Timestamp('1974-09-01 00:00:00'), Timestamp('1974-10-01 00:00:00'), Timestamp('1974-11-01 00:00:00'), Timestamp('1974-12-01 00:00:00'), Timestamp('1975-01-01 00:00:00'), Timestamp('1975-02-01 00:00:00'), Timestamp('1975-03-01 00:00:00'), Timestamp('1975-04-01 00:00:00'), Timestamp('1975-05-01 00:00:00'), Timestamp('1975-06-01 00:00:00'), Timestamp('1975-07-01 00:00:00'), Timestamp('1975-08-01 00:00:00'), Timestamp('1975-09-01 00:00:00'), Timestamp('1975-10-01 00:00:00'), Timestamp('1975-11-01 00:00:00'), Timestamp('1975-12-01 00:00:00'), Timestamp('1976-01-01 00:00:00'), Timestamp('1976-02-01 00:00:00'), Timestamp('1976-03-01 00:00:00'), Timestamp('1976-04-01 00:00:00'), Timestamp('1976-05-01 00:00:00'), Timestamp('1976-06-01 00:00:00'), Timestamp('1976-07-01 00:00:00'), Timestamp('1976-08-01 00:00:00'), Timestamp('1976-09-01 00:00:00'), Timestamp('1976-10-01 00:00:00'), Timestamp('1976-11-01 00:00:00'), Timestamp('1976-12-01 00:00:00'), Timestamp('1977-01-01 00:00:00'), Timestamp('1977-02-01 00:00:00'), Timestamp('1977-03-01 00:00:00'), Timestamp('1977-04-01 00:00:00'), Timestamp('1977-05-01 00:00:00'), Timestamp('1977-06-01 00:00:00'), Timestamp('1977-07-01 00:00:00'), Timestamp('1977-08-01 00:00:00'), Timestamp('1977-09-01 00:00:00'), Timestamp('1977-10-01 00:00:00'), Timestamp('1977-11-01 00:00:00'), Timestamp('1977-12-01 00:00:00'), Timestamp('1978-01-01 00:00:00'), Timestamp('1978-02-01 00:00:00'), Timestamp('1978-03-01 00:00:00'), Timestamp('1978-04-01 00:00:00'), Timestamp('1978-05-01 00:00:00'), Timestamp('1978-06-01 00:00:00'), Timestamp('1978-07-01 00:00:00'), Timestamp('1978-08-01 00:00:00'), Timestamp('1978-09-01 00:00:00'), Timestamp('1978-10-01 00:00:00'), Timestamp('1978-11-01 00:00:00'), Timestamp('1978-12-01 00:00:00'), Timestamp('1979-01-01 00:00:00'), Timestamp('1979-02-01 00:00:00'), Timestamp('1979-03-01 00:00:00'), Timestamp('1979-04-01 00:00:00'), Timestamp('1979-05-01 00:00:00'), Timestamp('1979-06-01 00:00:00'), Timestamp('1979-07-01 00:00:00'), Timestamp('1979-08-01 00:00:00'), Timestamp('1979-09-01 00:00:00'), Timestamp('1979-10-01 00:00:00'), Timestamp('1979-11-01 00:00:00'), Timestamp('1979-12-01 00:00:00'), Timestamp('1980-01-01 00:00:00'), Timestamp('1980-02-01 00:00:00'), Timestamp('1980-03-01 00:00:00'), Timestamp('1980-04-01 00:00:00'), Timestamp('1980-05-01 00:00:00'), Timestamp('1980-06-01 00:00:00'), Timestamp('1980-07-01 00:00:00'), Timestamp('1980-08-01 00:00:00'), Timestamp('1980-09-01 00:00:00'), Timestamp('1980-10-01 00:00:00'), Timestamp('1980-11-01 00:00:00'), Timestamp('1980-12-01 00:00:00'), Timestamp('1981-01-01 00:00:00'), Timestamp('1981-02-01 00:00:00'), Timestamp('1981-03-01 00:00:00'), Timestamp('1981-04-01 00:00:00'), Timestamp('1981-05-01 00:00:00'), Timestamp('1981-06-01 00:00:00'), Timestamp('1981-07-01 00:00:00'), Timestamp('1981-08-01 00:00:00'), Timestamp('1981-09-01 00:00:00'), Timestamp('1981-10-01 00:00:00'), Timestamp('1981-11-01 00:00:00'), Timestamp('1981-12-01 00:00:00'), Timestamp('1982-01-01 00:00:00'), Timestamp('1982-02-01 00:00:00'), Timestamp('1982-03-01 00:00:00'), Timestamp('1982-04-01 00:00:00'), Timestamp('1982-05-01 00:00:00'), Timestamp('1982-06-01 00:00:00'), Timestamp('1982-07-01 00:00:00'), Timestamp('1982-08-01 00:00:00'), Timestamp('1982-09-01 00:00:00'), Timestamp('1982-10-01 00:00:00'), Timestamp('1982-11-01 00:00:00'), Timestamp('1982-12-01 00:00:00'), Timestamp('1983-01-01 00:00:00'), Timestamp('1983-02-01 00:00:00'), Timestamp('1983-03-01 00:00:00'), Timestamp('1983-04-01 00:00:00'), Timestamp('1983-05-01 00:00:00'), Timestamp('1983-06-01 00:00:00'), Timestamp('1983-07-01 00:00:00'), Timestamp('1983-08-01 00:00:00'), Timestamp('1983-09-01 00:00:00'), Timestamp('1983-10-01 00:00:00'), Timestamp('1983-11-01 00:00:00'), Timestamp('1983-12-01 00:00:00'), Timestamp('1984-01-01 00:00:00'), Timestamp('1984-02-01 00:00:00'), Timestamp('1984-03-01 00:00:00'), Timestamp('1984-04-01 00:00:00'), Timestamp('1984-05-01 00:00:00'), Timestamp('1984-06-01 00:00:00'), Timestamp('1984-07-01 00:00:00'), Timestamp('1984-08-01 00:00:00'), Timestamp('1984-09-01 00:00:00'), Timestamp('1984-10-01 00:00:00'), Timestamp('1984-11-01 00:00:00'), Timestamp('1984-12-01 00:00:00'), Timestamp('1985-01-01 00:00:00'), Timestamp('1985-02-01 00:00:00'), Timestamp('1985-03-01 00:00:00'), Timestamp('1985-04-01 00:00:00'), Timestamp('1985-05-01 00:00:00'), Timestamp('1985-06-01 00:00:00'), Timestamp('1985-07-01 00:00:00'), Timestamp('1985-08-01 00:00:00'), Timestamp('1985-09-01 00:00:00'), Timestamp('1985-10-01 00:00:00'), Timestamp('1985-11-01 00:00:00'), Timestamp('1985-12-01 00:00:00'), Timestamp('1986-01-01 00:00:00'), Timestamp('1986-02-01 00:00:00'), Timestamp('1986-03-01 00:00:00'), Timestamp('1986-04-01 00:00:00'), Timestamp('1986-05-01 00:00:00'), Timestamp('1986-06-01 00:00:00'), Timestamp('1986-07-01 00:00:00'), Timestamp('1986-08-01 00:00:00'), Timestamp('1986-09-01 00:00:00'), Timestamp('1986-10-01 00:00:00'), Timestamp('1986-11-01 00:00:00'), Timestamp('1986-12-01 00:00:00'), Timestamp('1987-01-01 00:00:00'), Timestamp('1987-02-01 00:00:00'), Timestamp('1987-03-01 00:00:00'), Timestamp('1987-04-01 00:00:00'), Timestamp('1987-05-01 00:00:00'), Timestamp('1987-06-01 00:00:00'), Timestamp('1987-07-01 00:00:00'), Timestamp('1987-08-01 00:00:00'), Timestamp('1987-09-01 00:00:00'), Timestamp('1987-10-01 00:00:00'), Timestamp('1987-11-01 00:00:00'), Timestamp('1987-12-01 00:00:00'), Timestamp('1988-01-01 00:00:00'), Timestamp('1988-02-01 00:00:00'), Timestamp('1988-03-01 00:00:00'), Timestamp('1988-04-01 00:00:00'), Timestamp('1988-05-01 00:00:00'), Timestamp('1988-06-01 00:00:00'), Timestamp('1988-07-01 00:00:00'), Timestamp('1988-08-01 00:00:00'), Timestamp('1988-09-01 00:00:00'), Timestamp('1988-10-01 00:00:00'), Timestamp('1988-11-01 00:00:00'), Timestamp('1988-12-01 00:00:00'), Timestamp('1989-01-01 00:00:00'), Timestamp('1989-02-01 00:00:00'), Timestamp('1989-03-01 00:00:00'), Timestamp('1989-04-01 00:00:00'), Timestamp('1989-05-01 00:00:00'), Timestamp('1989-06-01 00:00:00'), Timestamp('1989-07-01 00:00:00'), Timestamp('1989-08-01 00:00:00'), Timestamp('1989-09-01 00:00:00'), Timestamp('1989-10-01 00:00:00'), Timestamp('1989-11-01 00:00:00'), Timestamp('1989-12-01 00:00:00'), Timestamp('1990-01-01 00:00:00'), Timestamp('1990-02-01 00:00:00'), Timestamp('1990-03-01 00:00:00'), Timestamp('1990-04-01 00:00:00'), Timestamp('1990-05-01 00:00:00'), Timestamp('1990-06-01 00:00:00'), Timestamp('1990-07-01 00:00:00'), Timestamp('1990-08-01 00:00:00'), Timestamp('1990-09-01 00:00:00'), Timestamp('1990-10-01 00:00:00'), Timestamp('1990-11-01 00:00:00'), Timestamp('1990-12-01 00:00:00'), Timestamp('1991-01-01 00:00:00'), Timestamp('1991-02-01 00:00:00'), Timestamp('1991-03-01 00:00:00'), Timestamp('1991-04-01 00:00:00'), Timestamp('1991-05-01 00:00:00'), Timestamp('1991-06-01 00:00:00'), Timestamp('1991-07-01 00:00:00'), Timestamp('1991-08-01 00:00:00'), Timestamp('1991-09-01 00:00:00'), Timestamp('1991-10-01 00:00:00'), Timestamp('1991-11-01 00:00:00'), Timestamp('1991-12-01 00:00:00'), Timestamp('1992-01-01 00:00:00')]\n",
      "Column Index: 56, Column Name: ANDENOx\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00'), Timestamp('1960-01-01 00:00:00'), Timestamp('1960-02-01 00:00:00'), Timestamp('1960-03-01 00:00:00'), Timestamp('1960-04-01 00:00:00'), Timestamp('1960-05-01 00:00:00'), Timestamp('1960-06-01 00:00:00'), Timestamp('1960-07-01 00:00:00'), Timestamp('1960-08-01 00:00:00'), Timestamp('1960-09-01 00:00:00'), Timestamp('1960-10-01 00:00:00'), Timestamp('1960-11-01 00:00:00'), Timestamp('1960-12-01 00:00:00'), Timestamp('1961-01-01 00:00:00'), Timestamp('1961-02-01 00:00:00'), Timestamp('1961-03-01 00:00:00'), Timestamp('1961-04-01 00:00:00'), Timestamp('1961-05-01 00:00:00'), Timestamp('1961-06-01 00:00:00'), Timestamp('1961-07-01 00:00:00'), Timestamp('1961-08-01 00:00:00'), Timestamp('1961-09-01 00:00:00'), Timestamp('1961-10-01 00:00:00'), Timestamp('1961-11-01 00:00:00'), Timestamp('1961-12-01 00:00:00'), Timestamp('1962-01-01 00:00:00'), Timestamp('1962-02-01 00:00:00'), Timestamp('1962-03-01 00:00:00'), Timestamp('1962-04-01 00:00:00'), Timestamp('1962-05-01 00:00:00'), Timestamp('1962-06-01 00:00:00'), Timestamp('1962-07-01 00:00:00'), Timestamp('1962-08-01 00:00:00'), Timestamp('1962-09-01 00:00:00'), Timestamp('1962-10-01 00:00:00'), Timestamp('1962-11-01 00:00:00'), Timestamp('1962-12-01 00:00:00'), Timestamp('1963-01-01 00:00:00'), Timestamp('1963-02-01 00:00:00'), Timestamp('1963-03-01 00:00:00'), Timestamp('1963-04-01 00:00:00'), Timestamp('1963-05-01 00:00:00'), Timestamp('1963-06-01 00:00:00'), Timestamp('1963-07-01 00:00:00'), Timestamp('1963-08-01 00:00:00'), Timestamp('1963-09-01 00:00:00'), Timestamp('1963-10-01 00:00:00'), Timestamp('1963-11-01 00:00:00'), Timestamp('1963-12-01 00:00:00'), Timestamp('1964-01-01 00:00:00'), Timestamp('1964-02-01 00:00:00'), Timestamp('1964-03-01 00:00:00'), Timestamp('1964-04-01 00:00:00'), Timestamp('1964-05-01 00:00:00'), Timestamp('1964-06-01 00:00:00'), Timestamp('1964-07-01 00:00:00'), Timestamp('1964-08-01 00:00:00'), Timestamp('1964-09-01 00:00:00'), Timestamp('1964-10-01 00:00:00'), Timestamp('1964-11-01 00:00:00'), Timestamp('1964-12-01 00:00:00'), Timestamp('1965-01-01 00:00:00'), Timestamp('1965-02-01 00:00:00'), Timestamp('1965-03-01 00:00:00'), Timestamp('1965-04-01 00:00:00'), Timestamp('1965-05-01 00:00:00'), Timestamp('1965-06-01 00:00:00'), Timestamp('1965-07-01 00:00:00'), Timestamp('1965-08-01 00:00:00'), Timestamp('1965-09-01 00:00:00'), Timestamp('1965-10-01 00:00:00'), Timestamp('1965-11-01 00:00:00'), Timestamp('1965-12-01 00:00:00'), Timestamp('1966-01-01 00:00:00'), Timestamp('1966-02-01 00:00:00'), Timestamp('1966-03-01 00:00:00'), Timestamp('1966-04-01 00:00:00'), Timestamp('1966-05-01 00:00:00'), Timestamp('1966-06-01 00:00:00'), Timestamp('1966-07-01 00:00:00'), Timestamp('1966-08-01 00:00:00'), Timestamp('1966-09-01 00:00:00'), Timestamp('1966-10-01 00:00:00'), Timestamp('1966-11-01 00:00:00'), Timestamp('1966-12-01 00:00:00'), Timestamp('1967-01-01 00:00:00'), Timestamp('1967-02-01 00:00:00'), Timestamp('1967-03-01 00:00:00'), Timestamp('1967-04-01 00:00:00'), Timestamp('1967-05-01 00:00:00'), Timestamp('1967-06-01 00:00:00'), Timestamp('1967-07-01 00:00:00'), Timestamp('1967-08-01 00:00:00'), Timestamp('1967-09-01 00:00:00'), Timestamp('1967-10-01 00:00:00'), Timestamp('1967-11-01 00:00:00'), Timestamp('1967-12-01 00:00:00'), Timestamp('1968-01-01 00:00:00'), Timestamp('1968-02-01 00:00:00'), Timestamp('1968-03-01 00:00:00'), Timestamp('1968-04-01 00:00:00'), Timestamp('1968-05-01 00:00:00'), Timestamp('1968-06-01 00:00:00'), Timestamp('1968-07-01 00:00:00'), Timestamp('1968-08-01 00:00:00'), Timestamp('1968-09-01 00:00:00'), Timestamp('1968-10-01 00:00:00'), Timestamp('1968-11-01 00:00:00'), Timestamp('1968-12-01 00:00:00'), Timestamp('1969-01-01 00:00:00'), Timestamp('1969-02-01 00:00:00'), Timestamp('1969-03-01 00:00:00'), Timestamp('1969-04-01 00:00:00'), Timestamp('1969-05-01 00:00:00'), Timestamp('1969-06-01 00:00:00'), Timestamp('1969-07-01 00:00:00'), Timestamp('1969-08-01 00:00:00'), Timestamp('1969-09-01 00:00:00'), Timestamp('1969-10-01 00:00:00'), Timestamp('1969-11-01 00:00:00'), Timestamp('1969-12-01 00:00:00'), Timestamp('1970-01-01 00:00:00'), Timestamp('1970-02-01 00:00:00'), Timestamp('1970-03-01 00:00:00'), Timestamp('1970-04-01 00:00:00'), Timestamp('1970-05-01 00:00:00'), Timestamp('1970-06-01 00:00:00'), Timestamp('1970-07-01 00:00:00'), Timestamp('1970-08-01 00:00:00'), Timestamp('1970-09-01 00:00:00'), Timestamp('1970-10-01 00:00:00'), Timestamp('1970-11-01 00:00:00'), Timestamp('1970-12-01 00:00:00'), Timestamp('1971-01-01 00:00:00'), Timestamp('1971-02-01 00:00:00'), Timestamp('1971-03-01 00:00:00'), Timestamp('1971-04-01 00:00:00'), Timestamp('1971-05-01 00:00:00'), Timestamp('1971-06-01 00:00:00'), Timestamp('1971-07-01 00:00:00'), Timestamp('1971-08-01 00:00:00'), Timestamp('1971-09-01 00:00:00'), Timestamp('1971-10-01 00:00:00'), Timestamp('1971-11-01 00:00:00'), Timestamp('1971-12-01 00:00:00'), Timestamp('1972-01-01 00:00:00'), Timestamp('1972-02-01 00:00:00'), Timestamp('1972-03-01 00:00:00'), Timestamp('1972-04-01 00:00:00'), Timestamp('1972-05-01 00:00:00'), Timestamp('1972-06-01 00:00:00'), Timestamp('1972-07-01 00:00:00'), Timestamp('1972-08-01 00:00:00'), Timestamp('1972-09-01 00:00:00'), Timestamp('1972-10-01 00:00:00'), Timestamp('1972-11-01 00:00:00'), Timestamp('1972-12-01 00:00:00'), Timestamp('1973-01-01 00:00:00'), Timestamp('1973-02-01 00:00:00'), Timestamp('1973-03-01 00:00:00'), Timestamp('1973-04-01 00:00:00'), Timestamp('1973-05-01 00:00:00'), Timestamp('1973-06-01 00:00:00'), Timestamp('1973-07-01 00:00:00'), Timestamp('1973-08-01 00:00:00'), Timestamp('1973-09-01 00:00:00'), Timestamp('1973-10-01 00:00:00'), Timestamp('1973-11-01 00:00:00'), Timestamp('1973-12-01 00:00:00'), Timestamp('1974-01-01 00:00:00'), Timestamp('1974-02-01 00:00:00'), Timestamp('1974-03-01 00:00:00'), Timestamp('1974-04-01 00:00:00'), Timestamp('1974-05-01 00:00:00'), Timestamp('1974-06-01 00:00:00'), Timestamp('1974-07-01 00:00:00'), Timestamp('1974-08-01 00:00:00'), Timestamp('1974-09-01 00:00:00'), Timestamp('1974-10-01 00:00:00'), Timestamp('1974-11-01 00:00:00'), Timestamp('1974-12-01 00:00:00'), Timestamp('1975-01-01 00:00:00'), Timestamp('1975-02-01 00:00:00'), Timestamp('1975-03-01 00:00:00'), Timestamp('1975-04-01 00:00:00'), Timestamp('1975-05-01 00:00:00'), Timestamp('1975-06-01 00:00:00'), Timestamp('1975-07-01 00:00:00'), Timestamp('1975-08-01 00:00:00'), Timestamp('1975-09-01 00:00:00'), Timestamp('1975-10-01 00:00:00'), Timestamp('1975-11-01 00:00:00'), Timestamp('1975-12-01 00:00:00'), Timestamp('1976-01-01 00:00:00'), Timestamp('1976-02-01 00:00:00'), Timestamp('1976-03-01 00:00:00'), Timestamp('1976-04-01 00:00:00'), Timestamp('1976-05-01 00:00:00'), Timestamp('1976-06-01 00:00:00'), Timestamp('1976-07-01 00:00:00'), Timestamp('1976-08-01 00:00:00'), Timestamp('1976-09-01 00:00:00'), Timestamp('1976-10-01 00:00:00'), Timestamp('1976-11-01 00:00:00'), Timestamp('1976-12-01 00:00:00'), Timestamp('1977-01-01 00:00:00'), Timestamp('1977-02-01 00:00:00'), Timestamp('1977-03-01 00:00:00'), Timestamp('1977-04-01 00:00:00'), Timestamp('1977-05-01 00:00:00'), Timestamp('1977-06-01 00:00:00'), Timestamp('1977-07-01 00:00:00'), Timestamp('1977-08-01 00:00:00'), Timestamp('1977-09-01 00:00:00'), Timestamp('1977-10-01 00:00:00'), Timestamp('1977-11-01 00:00:00'), Timestamp('1977-12-01 00:00:00'), Timestamp('1978-01-01 00:00:00'), Timestamp('1978-02-01 00:00:00'), Timestamp('1978-03-01 00:00:00'), Timestamp('1978-04-01 00:00:00'), Timestamp('1978-05-01 00:00:00'), Timestamp('1978-06-01 00:00:00'), Timestamp('1978-07-01 00:00:00'), Timestamp('1978-08-01 00:00:00'), Timestamp('1978-09-01 00:00:00'), Timestamp('1978-10-01 00:00:00'), Timestamp('1978-11-01 00:00:00'), Timestamp('1978-12-01 00:00:00'), Timestamp('1979-01-01 00:00:00'), Timestamp('1979-02-01 00:00:00'), Timestamp('1979-03-01 00:00:00'), Timestamp('1979-04-01 00:00:00'), Timestamp('1979-05-01 00:00:00'), Timestamp('1979-06-01 00:00:00'), Timestamp('1979-07-01 00:00:00'), Timestamp('1979-08-01 00:00:00'), Timestamp('1979-09-01 00:00:00'), Timestamp('1979-10-01 00:00:00'), Timestamp('1979-11-01 00:00:00'), Timestamp('1979-12-01 00:00:00'), Timestamp('1980-01-01 00:00:00'), Timestamp('1980-02-01 00:00:00'), Timestamp('1980-03-01 00:00:00'), Timestamp('1980-04-01 00:00:00'), Timestamp('1980-05-01 00:00:00'), Timestamp('1980-06-01 00:00:00'), Timestamp('1980-07-01 00:00:00'), Timestamp('1980-08-01 00:00:00'), Timestamp('1980-09-01 00:00:00'), Timestamp('1980-10-01 00:00:00'), Timestamp('1980-11-01 00:00:00'), Timestamp('1980-12-01 00:00:00'), Timestamp('1981-01-01 00:00:00'), Timestamp('1981-02-01 00:00:00'), Timestamp('1981-03-01 00:00:00'), Timestamp('1981-04-01 00:00:00'), Timestamp('1981-05-01 00:00:00'), Timestamp('1981-06-01 00:00:00'), Timestamp('1981-07-01 00:00:00'), Timestamp('1981-08-01 00:00:00'), Timestamp('1981-09-01 00:00:00'), Timestamp('1981-10-01 00:00:00'), Timestamp('1981-11-01 00:00:00'), Timestamp('1981-12-01 00:00:00'), Timestamp('1982-01-01 00:00:00'), Timestamp('1982-02-01 00:00:00'), Timestamp('1982-03-01 00:00:00'), Timestamp('1982-04-01 00:00:00'), Timestamp('1982-05-01 00:00:00'), Timestamp('1982-06-01 00:00:00'), Timestamp('1982-07-01 00:00:00'), Timestamp('1982-08-01 00:00:00'), Timestamp('1982-09-01 00:00:00'), Timestamp('1982-10-01 00:00:00'), Timestamp('1982-11-01 00:00:00'), Timestamp('1982-12-01 00:00:00'), Timestamp('1983-01-01 00:00:00'), Timestamp('1983-02-01 00:00:00'), Timestamp('1983-03-01 00:00:00'), Timestamp('1983-04-01 00:00:00'), Timestamp('1983-05-01 00:00:00'), Timestamp('1983-06-01 00:00:00'), Timestamp('1983-07-01 00:00:00'), Timestamp('1983-08-01 00:00:00'), Timestamp('1983-09-01 00:00:00'), Timestamp('1983-10-01 00:00:00'), Timestamp('1983-11-01 00:00:00'), Timestamp('1983-12-01 00:00:00'), Timestamp('1984-01-01 00:00:00'), Timestamp('1984-02-01 00:00:00'), Timestamp('1984-03-01 00:00:00'), Timestamp('1984-04-01 00:00:00'), Timestamp('1984-05-01 00:00:00'), Timestamp('1984-06-01 00:00:00'), Timestamp('1984-07-01 00:00:00'), Timestamp('1984-08-01 00:00:00'), Timestamp('1984-09-01 00:00:00'), Timestamp('1984-10-01 00:00:00'), Timestamp('1984-11-01 00:00:00'), Timestamp('1984-12-01 00:00:00'), Timestamp('1985-01-01 00:00:00'), Timestamp('1985-02-01 00:00:00'), Timestamp('1985-03-01 00:00:00'), Timestamp('1985-04-01 00:00:00'), Timestamp('1985-05-01 00:00:00'), Timestamp('1985-06-01 00:00:00'), Timestamp('1985-07-01 00:00:00'), Timestamp('1985-08-01 00:00:00'), Timestamp('1985-09-01 00:00:00'), Timestamp('1985-10-01 00:00:00'), Timestamp('1985-11-01 00:00:00'), Timestamp('1985-12-01 00:00:00'), Timestamp('1986-01-01 00:00:00'), Timestamp('1986-02-01 00:00:00'), Timestamp('1986-03-01 00:00:00'), Timestamp('1986-04-01 00:00:00'), Timestamp('1986-05-01 00:00:00'), Timestamp('1986-06-01 00:00:00'), Timestamp('1986-07-01 00:00:00'), Timestamp('1986-08-01 00:00:00'), Timestamp('1986-09-01 00:00:00'), Timestamp('1986-10-01 00:00:00'), Timestamp('1986-11-01 00:00:00'), Timestamp('1986-12-01 00:00:00'), Timestamp('1987-01-01 00:00:00'), Timestamp('1987-02-01 00:00:00'), Timestamp('1987-03-01 00:00:00'), Timestamp('1987-04-01 00:00:00'), Timestamp('1987-05-01 00:00:00'), Timestamp('1987-06-01 00:00:00'), Timestamp('1987-07-01 00:00:00'), Timestamp('1987-08-01 00:00:00'), Timestamp('1987-09-01 00:00:00'), Timestamp('1987-10-01 00:00:00'), Timestamp('1987-11-01 00:00:00'), Timestamp('1987-12-01 00:00:00'), Timestamp('1988-01-01 00:00:00'), Timestamp('1988-02-01 00:00:00'), Timestamp('1988-03-01 00:00:00'), Timestamp('1988-04-01 00:00:00'), Timestamp('1988-05-01 00:00:00'), Timestamp('1988-06-01 00:00:00'), Timestamp('1988-07-01 00:00:00'), Timestamp('1988-08-01 00:00:00'), Timestamp('1988-09-01 00:00:00'), Timestamp('1988-10-01 00:00:00'), Timestamp('1988-11-01 00:00:00'), Timestamp('1988-12-01 00:00:00'), Timestamp('1989-01-01 00:00:00'), Timestamp('1989-02-01 00:00:00'), Timestamp('1989-03-01 00:00:00'), Timestamp('1989-04-01 00:00:00'), Timestamp('1989-05-01 00:00:00'), Timestamp('1989-06-01 00:00:00'), Timestamp('1989-07-01 00:00:00'), Timestamp('1989-08-01 00:00:00'), Timestamp('1989-09-01 00:00:00'), Timestamp('1989-10-01 00:00:00'), Timestamp('1989-11-01 00:00:00'), Timestamp('1989-12-01 00:00:00'), Timestamp('1990-01-01 00:00:00'), Timestamp('1990-02-01 00:00:00'), Timestamp('1990-03-01 00:00:00'), Timestamp('1990-04-01 00:00:00'), Timestamp('1990-05-01 00:00:00'), Timestamp('1990-06-01 00:00:00'), Timestamp('1990-07-01 00:00:00'), Timestamp('1990-08-01 00:00:00'), Timestamp('1990-09-01 00:00:00'), Timestamp('1990-10-01 00:00:00'), Timestamp('1990-11-01 00:00:00'), Timestamp('1990-12-01 00:00:00'), Timestamp('1991-01-01 00:00:00'), Timestamp('1991-02-01 00:00:00'), Timestamp('1991-03-01 00:00:00'), Timestamp('1991-04-01 00:00:00'), Timestamp('1991-05-01 00:00:00'), Timestamp('1991-06-01 00:00:00'), Timestamp('1991-07-01 00:00:00'), Timestamp('1991-08-01 00:00:00'), Timestamp('1991-09-01 00:00:00'), Timestamp('1991-10-01 00:00:00'), Timestamp('1991-11-01 00:00:00'), Timestamp('1991-12-01 00:00:00'), Timestamp('1992-01-01 00:00:00')]\n",
      "Column Index: 57, Column Name: AMDMUOx\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00'), Timestamp('1960-01-01 00:00:00'), Timestamp('1960-02-01 00:00:00'), Timestamp('1960-03-01 00:00:00'), Timestamp('1960-04-01 00:00:00'), Timestamp('1960-05-01 00:00:00'), Timestamp('1960-06-01 00:00:00'), Timestamp('1960-07-01 00:00:00'), Timestamp('1960-08-01 00:00:00'), Timestamp('1960-09-01 00:00:00'), Timestamp('1960-10-01 00:00:00'), Timestamp('1960-11-01 00:00:00'), Timestamp('1960-12-01 00:00:00'), Timestamp('1961-01-01 00:00:00'), Timestamp('1961-02-01 00:00:00'), Timestamp('1961-03-01 00:00:00'), Timestamp('1961-04-01 00:00:00'), Timestamp('1961-05-01 00:00:00'), Timestamp('1961-06-01 00:00:00'), Timestamp('1961-07-01 00:00:00'), Timestamp('1961-08-01 00:00:00'), Timestamp('1961-09-01 00:00:00'), Timestamp('1961-10-01 00:00:00'), Timestamp('1961-11-01 00:00:00'), Timestamp('1961-12-01 00:00:00'), Timestamp('1962-01-01 00:00:00'), Timestamp('1962-02-01 00:00:00'), Timestamp('1962-03-01 00:00:00'), Timestamp('1962-04-01 00:00:00'), Timestamp('1962-05-01 00:00:00'), Timestamp('1962-06-01 00:00:00'), Timestamp('1962-07-01 00:00:00'), Timestamp('1962-08-01 00:00:00'), Timestamp('1962-09-01 00:00:00'), Timestamp('1962-10-01 00:00:00'), Timestamp('1962-11-01 00:00:00'), Timestamp('1962-12-01 00:00:00'), Timestamp('1963-01-01 00:00:00'), Timestamp('1963-02-01 00:00:00'), Timestamp('1963-03-01 00:00:00'), Timestamp('1963-04-01 00:00:00'), Timestamp('1963-05-01 00:00:00'), Timestamp('1963-06-01 00:00:00'), Timestamp('1963-07-01 00:00:00'), Timestamp('1963-08-01 00:00:00'), Timestamp('1963-09-01 00:00:00'), Timestamp('1963-10-01 00:00:00'), Timestamp('1963-11-01 00:00:00'), Timestamp('1963-12-01 00:00:00'), Timestamp('1964-01-01 00:00:00'), Timestamp('1964-02-01 00:00:00'), Timestamp('1964-03-01 00:00:00'), Timestamp('1964-04-01 00:00:00'), Timestamp('1964-05-01 00:00:00'), Timestamp('1964-06-01 00:00:00'), Timestamp('1964-07-01 00:00:00'), Timestamp('1964-08-01 00:00:00'), Timestamp('1964-09-01 00:00:00'), Timestamp('1964-10-01 00:00:00'), Timestamp('1964-11-01 00:00:00'), Timestamp('1964-12-01 00:00:00'), Timestamp('1965-01-01 00:00:00'), Timestamp('1965-02-01 00:00:00'), Timestamp('1965-03-01 00:00:00'), Timestamp('1965-04-01 00:00:00'), Timestamp('1965-05-01 00:00:00'), Timestamp('1965-06-01 00:00:00'), Timestamp('1965-07-01 00:00:00'), Timestamp('1965-08-01 00:00:00'), Timestamp('1965-09-01 00:00:00'), Timestamp('1965-10-01 00:00:00'), Timestamp('1965-11-01 00:00:00'), Timestamp('1965-12-01 00:00:00'), Timestamp('1966-01-01 00:00:00'), Timestamp('1966-02-01 00:00:00'), Timestamp('1966-03-01 00:00:00'), Timestamp('1966-04-01 00:00:00'), Timestamp('1966-05-01 00:00:00'), Timestamp('1966-06-01 00:00:00'), Timestamp('1966-07-01 00:00:00'), Timestamp('1966-08-01 00:00:00'), Timestamp('1966-09-01 00:00:00'), Timestamp('1966-10-01 00:00:00'), Timestamp('1966-11-01 00:00:00'), Timestamp('1966-12-01 00:00:00'), Timestamp('1967-01-01 00:00:00'), Timestamp('1967-02-01 00:00:00'), Timestamp('1967-03-01 00:00:00'), Timestamp('1967-04-01 00:00:00'), Timestamp('1967-05-01 00:00:00'), Timestamp('1967-06-01 00:00:00'), Timestamp('1967-07-01 00:00:00'), Timestamp('1967-08-01 00:00:00'), Timestamp('1967-09-01 00:00:00'), Timestamp('1967-10-01 00:00:00'), Timestamp('1967-11-01 00:00:00'), Timestamp('1967-12-01 00:00:00'), Timestamp('1968-01-01 00:00:00'), Timestamp('1968-02-01 00:00:00'), Timestamp('1968-03-01 00:00:00'), Timestamp('1968-04-01 00:00:00'), Timestamp('1968-05-01 00:00:00'), Timestamp('1968-06-01 00:00:00'), Timestamp('1968-07-01 00:00:00'), Timestamp('1968-08-01 00:00:00'), Timestamp('1968-09-01 00:00:00'), Timestamp('1968-10-01 00:00:00'), Timestamp('1968-11-01 00:00:00'), Timestamp('1968-12-01 00:00:00'), Timestamp('1969-01-01 00:00:00'), Timestamp('1969-02-01 00:00:00'), Timestamp('1969-03-01 00:00:00'), Timestamp('1969-04-01 00:00:00'), Timestamp('1969-05-01 00:00:00'), Timestamp('1969-06-01 00:00:00'), Timestamp('1969-07-01 00:00:00'), Timestamp('1969-08-01 00:00:00'), Timestamp('1969-09-01 00:00:00'), Timestamp('1969-10-01 00:00:00'), Timestamp('1969-11-01 00:00:00'), Timestamp('1969-12-01 00:00:00'), Timestamp('1970-01-01 00:00:00'), Timestamp('1970-02-01 00:00:00'), Timestamp('1970-03-01 00:00:00'), Timestamp('1970-04-01 00:00:00'), Timestamp('1970-05-01 00:00:00'), Timestamp('1970-06-01 00:00:00'), Timestamp('1970-07-01 00:00:00'), Timestamp('1970-08-01 00:00:00'), Timestamp('1970-09-01 00:00:00'), Timestamp('1970-10-01 00:00:00'), Timestamp('1970-11-01 00:00:00'), Timestamp('1970-12-01 00:00:00'), Timestamp('1971-01-01 00:00:00'), Timestamp('1971-02-01 00:00:00'), Timestamp('1971-03-01 00:00:00'), Timestamp('1971-04-01 00:00:00'), Timestamp('1971-05-01 00:00:00'), Timestamp('1971-06-01 00:00:00'), Timestamp('1971-07-01 00:00:00'), Timestamp('1971-08-01 00:00:00'), Timestamp('1971-09-01 00:00:00'), Timestamp('1971-10-01 00:00:00'), Timestamp('1971-11-01 00:00:00'), Timestamp('1971-12-01 00:00:00'), Timestamp('1972-01-01 00:00:00'), Timestamp('1972-02-01 00:00:00'), Timestamp('1972-03-01 00:00:00'), Timestamp('1972-04-01 00:00:00'), Timestamp('1972-05-01 00:00:00'), Timestamp('1972-06-01 00:00:00'), Timestamp('1972-07-01 00:00:00'), Timestamp('1972-08-01 00:00:00'), Timestamp('1972-09-01 00:00:00'), Timestamp('1972-10-01 00:00:00'), Timestamp('1972-11-01 00:00:00'), Timestamp('1972-12-01 00:00:00'), Timestamp('1973-01-01 00:00:00'), Timestamp('1973-02-01 00:00:00'), Timestamp('1973-03-01 00:00:00'), Timestamp('1973-04-01 00:00:00'), Timestamp('1973-05-01 00:00:00'), Timestamp('1973-06-01 00:00:00'), Timestamp('1973-07-01 00:00:00'), Timestamp('1973-08-01 00:00:00'), Timestamp('1973-09-01 00:00:00'), Timestamp('1973-10-01 00:00:00'), Timestamp('1973-11-01 00:00:00'), Timestamp('1973-12-01 00:00:00'), Timestamp('1974-01-01 00:00:00'), Timestamp('1974-02-01 00:00:00'), Timestamp('1974-03-01 00:00:00'), Timestamp('1974-04-01 00:00:00'), Timestamp('1974-05-01 00:00:00'), Timestamp('1974-06-01 00:00:00'), Timestamp('1974-07-01 00:00:00'), Timestamp('1974-08-01 00:00:00'), Timestamp('1974-09-01 00:00:00'), Timestamp('1974-10-01 00:00:00'), Timestamp('1974-11-01 00:00:00'), Timestamp('1974-12-01 00:00:00'), Timestamp('1975-01-01 00:00:00'), Timestamp('1975-02-01 00:00:00'), Timestamp('1975-03-01 00:00:00'), Timestamp('1975-04-01 00:00:00'), Timestamp('1975-05-01 00:00:00'), Timestamp('1975-06-01 00:00:00'), Timestamp('1975-07-01 00:00:00'), Timestamp('1975-08-01 00:00:00'), Timestamp('1975-09-01 00:00:00'), Timestamp('1975-10-01 00:00:00'), Timestamp('1975-11-01 00:00:00'), Timestamp('1975-12-01 00:00:00'), Timestamp('1976-01-01 00:00:00'), Timestamp('1976-02-01 00:00:00'), Timestamp('1976-03-01 00:00:00'), Timestamp('1976-04-01 00:00:00'), Timestamp('1976-05-01 00:00:00'), Timestamp('1976-06-01 00:00:00'), Timestamp('1976-07-01 00:00:00'), Timestamp('1976-08-01 00:00:00'), Timestamp('1976-09-01 00:00:00'), Timestamp('1976-10-01 00:00:00'), Timestamp('1976-11-01 00:00:00'), Timestamp('1976-12-01 00:00:00'), Timestamp('1977-01-01 00:00:00'), Timestamp('1977-02-01 00:00:00'), Timestamp('1977-03-01 00:00:00'), Timestamp('1977-04-01 00:00:00'), Timestamp('1977-05-01 00:00:00'), Timestamp('1977-06-01 00:00:00'), Timestamp('1977-07-01 00:00:00'), Timestamp('1977-08-01 00:00:00'), Timestamp('1977-09-01 00:00:00'), Timestamp('1977-10-01 00:00:00'), Timestamp('1977-11-01 00:00:00'), Timestamp('1977-12-01 00:00:00'), Timestamp('1978-01-01 00:00:00'), Timestamp('1978-02-01 00:00:00'), Timestamp('1978-03-01 00:00:00'), Timestamp('1978-04-01 00:00:00'), Timestamp('1978-05-01 00:00:00'), Timestamp('1978-06-01 00:00:00'), Timestamp('1978-07-01 00:00:00'), Timestamp('1978-08-01 00:00:00'), Timestamp('1978-09-01 00:00:00'), Timestamp('1978-10-01 00:00:00'), Timestamp('1978-11-01 00:00:00'), Timestamp('1978-12-01 00:00:00'), Timestamp('1979-01-01 00:00:00'), Timestamp('1979-02-01 00:00:00'), Timestamp('1979-03-01 00:00:00'), Timestamp('1979-04-01 00:00:00'), Timestamp('1979-05-01 00:00:00'), Timestamp('1979-06-01 00:00:00'), Timestamp('1979-07-01 00:00:00'), Timestamp('1979-08-01 00:00:00'), Timestamp('1979-09-01 00:00:00'), Timestamp('1979-10-01 00:00:00'), Timestamp('1979-11-01 00:00:00'), Timestamp('1979-12-01 00:00:00'), Timestamp('1980-01-01 00:00:00'), Timestamp('1980-02-01 00:00:00'), Timestamp('1980-03-01 00:00:00'), Timestamp('1980-04-01 00:00:00'), Timestamp('1980-05-01 00:00:00'), Timestamp('1980-06-01 00:00:00'), Timestamp('1980-07-01 00:00:00'), Timestamp('1980-08-01 00:00:00'), Timestamp('1980-09-01 00:00:00'), Timestamp('1980-10-01 00:00:00'), Timestamp('1980-11-01 00:00:00'), Timestamp('1980-12-01 00:00:00'), Timestamp('1981-01-01 00:00:00'), Timestamp('1981-02-01 00:00:00'), Timestamp('1981-03-01 00:00:00'), Timestamp('1981-04-01 00:00:00'), Timestamp('1981-05-01 00:00:00'), Timestamp('1981-06-01 00:00:00'), Timestamp('1981-07-01 00:00:00'), Timestamp('1981-08-01 00:00:00'), Timestamp('1981-09-01 00:00:00'), Timestamp('1981-10-01 00:00:00'), Timestamp('1981-11-01 00:00:00'), Timestamp('1981-12-01 00:00:00'), Timestamp('1982-01-01 00:00:00'), Timestamp('1982-02-01 00:00:00'), Timestamp('1982-03-01 00:00:00'), Timestamp('1982-04-01 00:00:00'), Timestamp('1982-05-01 00:00:00'), Timestamp('1982-06-01 00:00:00'), Timestamp('1982-07-01 00:00:00'), Timestamp('1982-08-01 00:00:00'), Timestamp('1982-09-01 00:00:00'), Timestamp('1982-10-01 00:00:00'), Timestamp('1982-11-01 00:00:00'), Timestamp('1982-12-01 00:00:00'), Timestamp('1983-01-01 00:00:00'), Timestamp('1983-02-01 00:00:00'), Timestamp('1983-03-01 00:00:00'), Timestamp('1983-04-01 00:00:00'), Timestamp('1983-05-01 00:00:00'), Timestamp('1983-06-01 00:00:00'), Timestamp('1983-07-01 00:00:00'), Timestamp('1983-08-01 00:00:00'), Timestamp('1983-09-01 00:00:00'), Timestamp('1983-10-01 00:00:00'), Timestamp('1983-11-01 00:00:00'), Timestamp('1983-12-01 00:00:00'), Timestamp('1984-01-01 00:00:00'), Timestamp('1984-02-01 00:00:00'), Timestamp('1984-03-01 00:00:00'), Timestamp('1984-04-01 00:00:00'), Timestamp('1984-05-01 00:00:00'), Timestamp('1984-06-01 00:00:00'), Timestamp('1984-07-01 00:00:00'), Timestamp('1984-08-01 00:00:00'), Timestamp('1984-09-01 00:00:00'), Timestamp('1984-10-01 00:00:00'), Timestamp('1984-11-01 00:00:00'), Timestamp('1984-12-01 00:00:00'), Timestamp('1985-01-01 00:00:00'), Timestamp('1985-02-01 00:00:00'), Timestamp('1985-03-01 00:00:00'), Timestamp('1985-04-01 00:00:00'), Timestamp('1985-05-01 00:00:00'), Timestamp('1985-06-01 00:00:00'), Timestamp('1985-07-01 00:00:00'), Timestamp('1985-08-01 00:00:00'), Timestamp('1985-09-01 00:00:00'), Timestamp('1985-10-01 00:00:00'), Timestamp('1985-11-01 00:00:00'), Timestamp('1985-12-01 00:00:00'), Timestamp('1986-01-01 00:00:00'), Timestamp('1986-02-01 00:00:00'), Timestamp('1986-03-01 00:00:00'), Timestamp('1986-04-01 00:00:00'), Timestamp('1986-05-01 00:00:00'), Timestamp('1986-06-01 00:00:00'), Timestamp('1986-07-01 00:00:00'), Timestamp('1986-08-01 00:00:00'), Timestamp('1986-09-01 00:00:00'), Timestamp('1986-10-01 00:00:00'), Timestamp('1986-11-01 00:00:00'), Timestamp('1986-12-01 00:00:00'), Timestamp('1987-01-01 00:00:00'), Timestamp('1987-02-01 00:00:00'), Timestamp('1987-03-01 00:00:00'), Timestamp('1987-04-01 00:00:00'), Timestamp('1987-05-01 00:00:00'), Timestamp('1987-06-01 00:00:00'), Timestamp('1987-07-01 00:00:00'), Timestamp('1987-08-01 00:00:00'), Timestamp('1987-09-01 00:00:00'), Timestamp('1987-10-01 00:00:00'), Timestamp('1987-11-01 00:00:00'), Timestamp('1987-12-01 00:00:00'), Timestamp('1988-01-01 00:00:00'), Timestamp('1988-02-01 00:00:00'), Timestamp('1988-03-01 00:00:00'), Timestamp('1988-04-01 00:00:00'), Timestamp('1988-05-01 00:00:00'), Timestamp('1988-06-01 00:00:00'), Timestamp('1988-07-01 00:00:00'), Timestamp('1988-08-01 00:00:00'), Timestamp('1988-09-01 00:00:00'), Timestamp('1988-10-01 00:00:00'), Timestamp('1988-11-01 00:00:00'), Timestamp('1988-12-01 00:00:00'), Timestamp('1989-01-01 00:00:00'), Timestamp('1989-02-01 00:00:00'), Timestamp('1989-03-01 00:00:00'), Timestamp('1989-04-01 00:00:00'), Timestamp('1989-05-01 00:00:00'), Timestamp('1989-06-01 00:00:00'), Timestamp('1989-07-01 00:00:00'), Timestamp('1989-08-01 00:00:00'), Timestamp('1989-09-01 00:00:00'), Timestamp('1989-10-01 00:00:00'), Timestamp('1989-11-01 00:00:00'), Timestamp('1989-12-01 00:00:00'), Timestamp('1990-01-01 00:00:00'), Timestamp('1990-02-01 00:00:00'), Timestamp('1990-03-01 00:00:00'), Timestamp('1990-04-01 00:00:00'), Timestamp('1990-05-01 00:00:00'), Timestamp('1990-06-01 00:00:00'), Timestamp('1990-07-01 00:00:00'), Timestamp('1990-08-01 00:00:00'), Timestamp('1990-09-01 00:00:00'), Timestamp('1990-10-01 00:00:00'), Timestamp('1990-11-01 00:00:00'), Timestamp('1990-12-01 00:00:00'), Timestamp('1991-01-01 00:00:00'), Timestamp('1991-02-01 00:00:00'), Timestamp('1991-03-01 00:00:00'), Timestamp('1991-04-01 00:00:00'), Timestamp('1991-05-01 00:00:00'), Timestamp('1991-06-01 00:00:00'), Timestamp('1991-07-01 00:00:00'), Timestamp('1991-08-01 00:00:00'), Timestamp('1991-09-01 00:00:00'), Timestamp('1991-10-01 00:00:00'), Timestamp('1991-11-01 00:00:00'), Timestamp('1991-12-01 00:00:00')]\n",
      "Column Index: 58, Column Name: BUSINVx\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 59, Column Name: ISRATIOx\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 67, Column Name: NONREVSL\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 68, Column Name: CONSPI\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 71, Column Name: S&P PE ratio\n",
      "  Missing Dates: [Timestamp('2024-08-01 00:00:00'), Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 116, Column Name: DTCOLNVHFNM\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 117, Column Name: DTCTHFNM\n",
      "  Missing Dates: [Timestamp('2024-09-01 00:00:00')]\n",
      "Column Index: 119, Column Name: VIXCLSx\n",
      "  Missing Dates: [Timestamp('1959-01-01 00:00:00'), Timestamp('1959-02-01 00:00:00'), Timestamp('1959-03-01 00:00:00'), Timestamp('1959-04-01 00:00:00'), Timestamp('1959-05-01 00:00:00'), Timestamp('1959-06-01 00:00:00'), Timestamp('1959-07-01 00:00:00'), Timestamp('1959-08-01 00:00:00'), Timestamp('1959-09-01 00:00:00'), Timestamp('1959-10-01 00:00:00'), Timestamp('1959-11-01 00:00:00'), Timestamp('1959-12-01 00:00:00'), Timestamp('1960-01-01 00:00:00'), Timestamp('1960-02-01 00:00:00'), Timestamp('1960-03-01 00:00:00'), Timestamp('1960-04-01 00:00:00'), Timestamp('1960-05-01 00:00:00'), Timestamp('1960-06-01 00:00:00'), Timestamp('1960-07-01 00:00:00'), Timestamp('1960-08-01 00:00:00'), Timestamp('1960-09-01 00:00:00'), Timestamp('1960-10-01 00:00:00'), Timestamp('1960-11-01 00:00:00'), Timestamp('1960-12-01 00:00:00'), Timestamp('1961-01-01 00:00:00'), Timestamp('1961-02-01 00:00:00'), Timestamp('1961-03-01 00:00:00'), Timestamp('1961-04-01 00:00:00'), Timestamp('1961-05-01 00:00:00'), Timestamp('1961-06-01 00:00:00'), Timestamp('1961-07-01 00:00:00'), Timestamp('1961-08-01 00:00:00'), Timestamp('1961-09-01 00:00:00'), Timestamp('1961-10-01 00:00:00'), Timestamp('1961-11-01 00:00:00'), Timestamp('1961-12-01 00:00:00'), Timestamp('1962-01-01 00:00:00'), Timestamp('1962-02-01 00:00:00'), Timestamp('1962-03-01 00:00:00'), Timestamp('1962-04-01 00:00:00'), Timestamp('1962-05-01 00:00:00'), Timestamp('1962-06-01 00:00:00')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#### Optional steps to be performed in case the series require to be seasonally adjusted (part 1) ####\n",
    "\n",
    "############ Utility method for webscraping #########################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_series_seasonality_and_frequency(series_name):\n",
    "    \"\"\"\n",
    "    Check if the given FRED series is 'Not Seasonally Adjusted' or 'Seasonally Adjusted'\n",
    "    and determine its frequency based on specific DOM elements.\n",
    "    Args:\n",
    "        series_name (str): The name of the FRED series.\n",
    "    Returns:\n",
    "        dict: A dictionary with the seasonality and frequency information.\n",
    "    \"\"\"\n",
    "    # Construct the URL for the FRED series page\n",
    "    url = f\"https://fred.stlouisfed.org/series/{series_name}\"\n",
    "\n",
    "    # Fetch the web page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for {series_name}: HTTP {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Selector for seasonality (corresponding to XPath /html/body/div/div[1]/div/div[3]/div[1]/div[3]/span)\n",
    "    seasonality_section = soup.select_one('html > body > div > div:nth-of-type(1) > div > div:nth-of-type(3) > div:nth-of-type(1) > div:nth-of-type(3) > span')\n",
    "    if not seasonality_section:\n",
    "        print(f\"Seasonality section not found for {series_name}\")\n",
    "        return None\n",
    "\n",
    "    # Selector for frequency (corresponding to XPath /html/body/div/div[1]/div/div[3]/div[1]/div[4]/span/span)\n",
    "    frequency_section = soup.select_one('html > body > div > div:nth-of-type(1) > div > div:nth-of-type(3) > div:nth-of-type(1) > div:nth-of-type(4) > span > span')\n",
    "    if not frequency_section:\n",
    "        print(f\"Frequency section not found for {series_name}\")\n",
    "        return None\n",
    "\n",
    "    # Extract text for seasonality and frequency\n",
    "    seasonality_text = seasonality_section.get_text(strip=True)\n",
    "    frequency_text = frequency_section.get_text(strip=True)\n",
    "\n",
    "    # Determine seasonality\n",
    "    if \"Not Seasonally Adjusted\" in seasonality_text:\n",
    "        seasonality = \"Not Seasonally Adjusted\"\n",
    "    elif \"Seasonally Adjusted Annual Rate\" in seasonality_text or \"Seasonally Adjusted\" in seasonality_text:\n",
    "        seasonality = \"Seasonally Adjusted\"\n",
    "    else:\n",
    "        seasonality = \"Unknown\"\n",
    "\n",
    "    # Determine frequency\n",
    "    if \"Monthly\" in frequency_text:\n",
    "        frequency = \"Monthly\"\n",
    "    elif \"Quarterly\" in frequency_text:\n",
    "        frequency = \"Quarterly\"\n",
    "    elif \"Annual\" in frequency_text:\n",
    "        frequency = \"Annual\"\n",
    "    else:\n",
    "        frequency = \"Other\"\n",
    "\n",
    "    return {\"series_name\": series_name, \"seasonality\": seasonality, \"frequency\": frequency}\n",
    "\n",
    "# Example DataFrame (replace with your actual DataFrame)\n",
    "df_columns = df.columns.tolist()  # Replace df with your actual DataFrame variable\n",
    "\n",
    "# Create a report for each series in the DataFrame\n",
    "report = []\n",
    "for series in df_columns:\n",
    "    print(f\"Checking series: {series}\")\n",
    "    result = check_series_seasonality_and_frequency(series)\n",
    "    if result:\n",
    "        report.append(result)\n",
    "\n",
    "# Display the report\n",
    "for entry in report:\n",
    "    print(f\"Series: {entry['series_name']}, Seasonality: {entry['seasonality']}, Frequency: {entry['frequency']}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "6ASCMYT0Vel3"
   },
   "id": "6ASCMYT0Vel3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#### Optional steps to be performed in case the series require to be seasonally adjusted (part 2) ####\n",
    "\n",
    "# Manually categorize series that were not found\n",
    "not_found_seasonally_adjusted = [\n",
    "    \"CMRMTSPLx\", \"RETAILx\", \"CLAIMSx\", \"AMDMNOx\", \"ANDENOx\",\n",
    "    \"AMDMUOx\", \"BUSINVx\", \"ISRATIOx\", \"CONSPI\"\n",
    "]\n",
    "\n",
    "not_found_not_seasonally_adjusted = [\n",
    "    \"S&P 500\", \"S&P div yield\", \"S&P PE ratio\", \"CP3Mx\", \"COMPAPFFx\",\n",
    "    \"EXSZUSx\", \"EXJPUSx\", \"EXUSUKx\", \"EXCAUSx\", \"OILPRICEx\", \"VIXCLSx\"\n",
    "]\n",
    "\n",
    "# Separate found series based on seasonality\n",
    "seasonally_adjusted = []\n",
    "not_seasonally_adjusted = []\n",
    "\n",
    "for entry in report:\n",
    "    if entry['seasonality'] == \"Seasonally Adjusted\":\n",
    "        seasonally_adjusted.append(entry['series_name'])\n",
    "    elif entry['seasonality'] == \"Not Seasonally Adjusted\":\n",
    "        not_seasonally_adjusted.append(entry['series_name'])\n",
    "\n",
    "# Add manually specified series to the appropriate groups\n",
    "seasonally_adjusted.extend(not_found_seasonally_adjusted)\n",
    "not_seasonally_adjusted.extend(not_found_not_seasonally_adjusted)\n",
    "\n",
    "# Display results\n",
    "print(\"Seasonally Adjusted Series:\")\n",
    "print(seasonally_adjusted)\n",
    "\n",
    "print(\"\\nNot Seasonally Adjusted Series:\")\n",
    "print(not_seasonally_adjusted)\n",
    "##########################################\n",
    "######## Validation method ###############\n",
    "##########################################\n",
    "# Combine grouped lists for validation\n",
    "grouped_list = seasonally_adjusted + not_seasonally_adjusted\n",
    "\n",
    "# Get the list of column names from the DataFrame\n",
    "df_columns = list(df.columns)\n",
    "\n",
    "# Check for any column names not included in the grouped lists\n",
    "missing_columns = [col for col in df_columns if col not in grouped_list]\n",
    "\n",
    "# Report the results\n",
    "if missing_columns:\n",
    "    print(\"The following column names are missing from both arrays:\")\n",
    "    print(missing_columns)\n",
    "else:\n",
    "    print(\"All column names are included in one of the arrays.\")\n",
    "\n"
   ],
   "metadata": {
    "id": "hIE-aYPl5MxM"
   },
   "id": "hIE-aYPl5MxM",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Appplying the transformations to the series following the methodology used in the paper."
   ],
   "metadata": {
    "id": "2hlEHr79mxxD"
   },
   "id": "2hlEHr79mxxD"
  },
  {
   "cell_type": "code",
   "source": [
    "#####################################################################\n",
    "############ RESTART EXECUTING CODE FROM HERE! ######################\n",
    "#####################################################################\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# Apply Log-Transformed Monthly Growth Rate transformation (perform after adjusting for seasonality) TRANSFORMATION 1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "df = pd.read_csv('updated_dataframe.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Print basic information about the loaded DataFrame\n",
    "print(f\"Loaded DataFrame type: {type(df)}\")\n",
    "print(f\"Loaded DataFrame shape: {df.shape}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary to store transformed columns\n",
    "monthly_growth_rate_columns = {}\n",
    "\n",
    "# Perform the monthly growth rate calculation\n",
    "for idx in monthly_growth_rate_indices:\n",
    "    # Get the column name\n",
    "    column_name = df.columns[idx]\n",
    "\n",
    "    # Calculate the growth rate based on adjacent rows (dates)\n",
    "    growth_rate = (df.iloc[:, idx] - df.iloc[:, idx].shift(1)) / df.iloc[:, idx].shift(1)\n",
    "\n",
    "    # Add the transformed column to the dictionary for creating a new DataFrame\n",
    "    monthly_growth_rate_columns[column_name] = growth_rate\n",
    "\n",
    "# Create a new DataFrame with only the transformed series\n",
    "monthly_growth_rate_df = pd.DataFrame(monthly_growth_rate_columns, index=df.index)  # Retain the original time index\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "monthly_growth_rate_df.to_csv(\"monthly_growth_rate_transformed.csv\", index=True)  # Save with the index\n",
    "\n",
    "# Print the transformed DataFrame\n",
    "print(\"Transformed DataFrame (Monthly Growth Rates):\")\n",
    "print(monthly_growth_rate_df)\n",
    "\n",
    "\n",
    "########### Checking dates with missing values #########\n",
    "\n",
    "def extract_missing_dates_and_columns(monthly_growth_rate_df):\n",
    "    # Dictionary to hold missing columns grouped by missing dates\n",
    "    missing_dates_columns = {}\n",
    "\n",
    "    # Iterate over each row (date) to find missing values\n",
    "    for date, row in monthly_growth_rate_df.iterrows():\n",
    "        # Find the columns with missing values for the current date\n",
    "        missing_columns = row[row.isna()].index.tolist()\n",
    "\n",
    "        # If there are missing values, store the corresponding columns in the dictionary\n",
    "        if missing_columns:\n",
    "            missing_dates_columns[date] = missing_columns\n",
    "\n",
    "    return missing_dates_columns\n",
    "\n",
    "# Example usage\n",
    "missing_dates_columns = extract_missing_dates_and_columns(monthly_growth_rate_df)\n",
    "\n",
    "# Print the missing dates and corresponding columns\n",
    "if missing_dates_columns:\n",
    "    print(\"Missing dates and corresponding columns:\")\n",
    "    for date, columns in missing_dates_columns.items():\n",
    "        print(f\"Date: {date.strftime('%Y-%m-%d')} -> Missing columns: {columns}\")\n",
    "else:\n",
    "    print(\"No missing data found for any dates.\")\n",
    "\n",
    "### The diagnostic tool reveals the presence of three columns with a significant amount of missing data: 'AMDMNOx', 'ANDENOx', 'AMDMUOx'\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# Apply Monthly difference transformation (perform after adjusting for seasonality) TRANSFORMATION 2\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "monthly_diff_columns = {}\n",
    "\n",
    "# Apply Monthly Difference transformation (without modifying the original DataFrame)\n",
    "for idx in monthly_difference_indices:\n",
    "    # Get the column name\n",
    "    column_name = df.columns[idx]\n",
    "\n",
    "    # Calculate the monthly difference\n",
    "    monthly_difference = df.iloc[:, idx].diff()\n",
    "\n",
    "    # Add the transformed column to the dictionary\n",
    "    monthly_diff_columns[column_name] = monthly_difference\n",
    "\n",
    "# Create a new DataFrame with only the affected columns (monthly differences)\n",
    "monthly_diff_df = pd.DataFrame(monthly_diff_columns, index=df.index)  # Retain the original time index\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "monthly_diff_df.to_csv(\"monthly_diff_transformed.csv\", index=True)  # Save with the index\n",
    "\n",
    "# Print the transformed DataFrame (monthly differences)\n",
    "print(\"Transformed DataFrame (Monthly Differences):\")\n",
    "print(monthly_diff_df)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "def extract_missing_dates_and_columns(monthly_diff_df):\n",
    "    # Dictionary to hold missing columns grouped by missing dates\n",
    "    missing_dates_columns = {}\n",
    "\n",
    "    # Iterate over each row (date) to find missing values\n",
    "    for date, row in monthly_diff_df.iterrows():\n",
    "        # Find the columns with missing values for the current date\n",
    "        missing_columns = row[row.isna()].index.tolist()\n",
    "\n",
    "        # If there are missing values, store the corresponding columns in the dictionary\n",
    "        if missing_columns:\n",
    "            missing_dates_columns[date] = missing_columns\n",
    "\n",
    "    return missing_dates_columns\n",
    "\n",
    "# Example usage\n",
    "missing_dates_columns = extract_missing_dates_and_columns(monthly_diff_df)\n",
    "\n",
    "# Print the missing dates and corresponding columns\n",
    "if missing_dates_columns:\n",
    "    print(\"Missing dates and corresponding columns:\")\n",
    "    for date, columns in missing_dates_columns.items():\n",
    "        print(f\"Date: {date.strftime('%Y-%m-%d')} -> Missing columns: {columns}\")\n",
    "else:\n",
    "    print(\"No missing data found for any dates.\")\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# Apply Monthly differences of yearly growth rate transformation (perform after adjusting for seasonality) TRANSFORMATION 3\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a dictionary to store transformed columns\n",
    "transformed_monthly_diff_growth_rate_columns = {}\n",
    "\n",
    "# Apply Monthly Difference of Yearly Growth Rate transformation (log-transformed)\n",
    "for idx in monthly_diff_yearly_growth_rate_indices:\n",
    "    # Get the column name\n",
    "    column_name = df.columns[idx]\n",
    "\n",
    "    # Log-transform the series to stabilize variance\n",
    "    log_transformed_series = np.log(df.iloc[:, idx])\n",
    "\n",
    "    # Calculate the yearly growth rate: Difference between log-transformed values lagged by 12 months\n",
    "    yearly_growth_rate = log_transformed_series - log_transformed_series.shift(12)  # Shift by 12 months for yearly growth rate\n",
    "\n",
    "    # Calculate the monthly difference of the yearly growth rate\n",
    "    monthly_diff_of_yearly_growth_rate = yearly_growth_rate - yearly_growth_rate.shift(1)  # Shift by 1 month for monthly difference\n",
    "\n",
    "    # Add the transformed column to the dictionary (without modifying the original DataFrame)\n",
    "    transformed_monthly_diff_growth_rate_columns[column_name] = monthly_diff_of_yearly_growth_rate\n",
    "\n",
    "    # Optionally, you can print the transformed column to check\n",
    "    print(f\"Transformed column for {column_name}:\")\n",
    "    print(monthly_diff_of_yearly_growth_rate.head())  # Print the first few rows of the transformed series\n",
    "\n",
    "# Create a new DataFrame for the transformed columns (monthly difference of yearly growth rates)\n",
    "transformed_monthly_diff_growth_rate_df = pd.DataFrame(\n",
    "    transformed_monthly_diff_growth_rate_columns,\n",
    "    index=df.index  # Retain the original time index\n",
    ")\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "transformed_monthly_diff_growth_rate_df.to_csv(\"monthly_diff_yearly_growth_rate_transformed.csv\", index=True)  # Save with the index\n",
    "\n",
    "# Print the transformed DataFrame\n",
    "print(\"Transformed DataFrame (Monthly Difference of Yearly Growth Rate):\")\n",
    "print(transformed_monthly_diff_growth_rate_df)\n",
    "\n",
    "#####\n",
    "\n",
    "def extract_missing_dates_and_columns(transformed_monthly_diff_growth_rate_df):\n",
    "    # Dictionary to hold missing columns grouped by missing dates\n",
    "    missing_dates_columns = {}\n",
    "\n",
    "    # Iterate over each row (date) to find missing values\n",
    "    for date, row in transformed_monthly_diff_growth_rate_df.iterrows():\n",
    "        # Find the columns with missing values for the current date\n",
    "        missing_columns = row[row.isna()].index.tolist()\n",
    "\n",
    "        # If there are missing values, store the corresponding columns in the dictionary\n",
    "        if missing_columns:\n",
    "            missing_dates_columns[date] = missing_columns\n",
    "\n",
    "    return missing_dates_columns\n",
    "\n",
    "# Example usage\n",
    "missing_dates_columns = extract_missing_dates_and_columns(transformed_monthly_diff_growth_rate_df)\n",
    "\n",
    "# Print the missing dates and corresponding columns\n",
    "if missing_dates_columns:\n",
    "    print(\"Missing dates and corresponding columns:\")\n",
    "    for date, columns in missing_dates_columns.items():\n",
    "        print(f\"Date: {date.strftime('%Y-%m-%d')} -> Missing columns: {columns}\")\n",
    "else:\n",
    "    print(\"No missing data found for any dates.\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Method to print the entire column associated with a specific index in the transformed DataFrame\n",
    "def print_transformed_column_full(df, column_index):\n",
    "    \"\"\"\n",
    "    Print the entire column associated with the given column index from the transformed DataFrame\n",
    "    without truncating the output.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing transformed data.\n",
    "        column_index (str): The column index (name) to retrieve and print.\n",
    "    \"\"\"\n",
    "    if column_index in df.columns:\n",
    "        # Temporarily change Pandas display settings to show all rows\n",
    "        with pd.option_context('display.max_rows', None):  # Set 'max_rows' to None to display all rows\n",
    "            print(f\"Column '{column_index}' in transformed data:\")\n",
    "            print(df[column_index])  # Print the entire column without truncation\n",
    "    else:\n",
    "        print(f\"Column '{column_index}' not found in the DataFrame.\")\n",
    "\n",
    "# Example usage: Print the entire column for index 'WPSFD49207'\n",
    "column_index = 'WPSFD49207'\n",
    "print_transformed_column_full(transformed_monthly_diff_growth_rate_df, column_index)\n",
    "\n",
    "#####################################################################################\n",
    "### Putting together the transformed dateframes preserving the correct time index ###\n",
    "#####################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_original_data(original_file):\n",
    "    \"\"\"Read the original DataFrame to preserve its index order.\"\"\"\n",
    "    print(\"Step 1: Reading the original DataFrame...\")\n",
    "    original_df = pd.read_csv(original_file, index_col=0)  # Ensure the original index is used\n",
    "    print(\"Original DataFrame (first 5 rows):\")\n",
    "    print(original_df.head())  # Print the first few rows for inspection\n",
    "    return original_df\n",
    "\n",
    "def read_transformed_data(growth_rate_file, diff_file, yearly_growth_rate_file):\n",
    "    \"\"\"Read the transformed DataFrames.\"\"\"\n",
    "    print(\"\\nStep 2: Reading the transformed DataFrames...\")\n",
    "    monthly_growth_rate_df = pd.read_csv(growth_rate_file, index_col=0)\n",
    "    monthly_diff_df = pd.read_csv(diff_file, index_col=0)\n",
    "    monthly_diff_yearly_growth_rate_df = pd.read_csv(yearly_growth_rate_file, index_col=0)\n",
    "\n",
    "    print(\"\\nMonthly Growth Rate DataFrame (first 5 rows):\")\n",
    "    print(monthly_growth_rate_df.head())  # Print the first few rows for inspection\n",
    "\n",
    "    print(\"\\nMonthly Difference DataFrame (first 5 rows):\")\n",
    "    print(monthly_diff_df.head())  # Print the first few rows for inspection\n",
    "\n",
    "    print(\"\\nMonthly Difference of Yearly Growth Rate DataFrame (first 5 rows):\")\n",
    "    print(monthly_diff_yearly_growth_rate_df.head())  # Print the first few rows for inspection\n",
    "\n",
    "    return monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df\n",
    "\n",
    "def identify_columns(original_df, monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df):\n",
    "    \"\"\"Identify untransformed columns from the original DataFrame.\"\"\"\n",
    "    print(\"\\nStep 3: Identifying untransformed columns...\")\n",
    "    transformed_columns = (\n",
    "        set(monthly_growth_rate_df.columns) |\n",
    "        set(monthly_diff_df.columns) |\n",
    "        set(monthly_diff_yearly_growth_rate_df.columns)\n",
    "    )\n",
    "    untransformed_columns = [col for col in original_df.columns if col not in transformed_columns]\n",
    "\n",
    "    print(\"\\nTransformed Columns:\")\n",
    "    print(transformed_columns)  # Print the set of transformed columns\n",
    "\n",
    "    print(\"\\nUntransformed Columns:\")\n",
    "    print(untransformed_columns)  # Print the list of untransformed columns\n",
    "\n",
    "    return untransformed_columns\n",
    "\n",
    "def extract_untransformed_columns(original_df, untransformed_columns):\n",
    "    \"\"\"Extract the untransformed columns from the original DataFrame.\"\"\"\n",
    "    print(\"\\nStep 4: Extracting untransformed columns...\")\n",
    "    untransformed_df = original_df[untransformed_columns]\n",
    "    print(\"\\nUntransformed DataFrame (first 5 rows):\")\n",
    "    print(untransformed_df.head())  # Print the untransformed columns for inspection\n",
    "    return untransformed_df\n",
    "\n",
    "# Example usage:\n",
    "# Call each method step by step to visualize the intermediate results\n",
    "original_df = read_original_data(\"updated_dataframe.csv\")\n",
    "monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df = read_transformed_data(\n",
    "    \"monthly_growth_rate_transformed.csv\", \"monthly_diff_transformed.csv\", \"monthly_diff_yearly_growth_rate_transformed.csv\"\n",
    ")\n",
    "untransformed_columns = identify_columns(\n",
    "    original_df, monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df\n",
    ")\n",
    "untransformed_df = extract_untransformed_columns(original_df, untransformed_columns)\n",
    "\n",
    "############\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_associate_indices(original_file):\n",
    "    \"\"\"Extract indices and associate them with the column names.\"\"\"\n",
    "    print(\"Step: Extracting and associating indices with column names...\")\n",
    "\n",
    "    # Read the original DataFrame\n",
    "    original_df = pd.read_csv(original_file, index_col=0)  # Ensure the original index is used\n",
    "\n",
    "    # Extract the index (row labels) and associate them with column names\n",
    "    index_column_mapping = {idx: col for idx, col in enumerate(original_df.columns)}\n",
    "\n",
    "    # Print the index-column association\n",
    "    print(\"\\nIndex to Column Mapping:\")\n",
    "    print(index_column_mapping)\n",
    "\n",
    "    return index_column_mapping\n",
    "\n",
    "# Example usage:\n",
    "index_column_mapping = extract_and_associate_indices(\"updated_dataframe.csv\")\n",
    "\n",
    "##################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def combine_dataframes(untransformed_df, monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df):\n",
    "    \"\"\"Combine the untransformed and transformed DataFrames, preserving the time index.\"\"\"\n",
    "    print(\"Step: Combining the DataFrames while preserving the time index...\")\n",
    "\n",
    "    # Combine all DataFrames along the columns (axis=1)\n",
    "    combined_df = pd.concat([\n",
    "        untransformed_df,\n",
    "        monthly_growth_rate_df,\n",
    "        monthly_diff_df,\n",
    "        monthly_diff_yearly_growth_rate_df\n",
    "    ], axis=1)\n",
    "\n",
    "    # Print the combined DataFrame (first 5 rows) to verify\n",
    "    print(\"\\nCombined DataFrame with all series (first 5 rows):\")\n",
    "    print(combined_df.head())  # Print the first few rows for inspection\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you already have the DataFrames: untransformed_df, monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df\n",
    "combined_df = combine_dataframes(untransformed_df, monthly_growth_rate_df, monthly_diff_df, monthly_diff_yearly_growth_rate_df)\n",
    "\n",
    "#######\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def reorder_columns_by_original_order(combined_df, original_df):\n",
    "    \"\"\"Reorder the columns of combined_df to match the order of columns in original_df.\"\"\"\n",
    "    print(\"Step: Reordering columns in combined_df to match the original DataFrame's column order...\")\n",
    "\n",
    "    # Extract the column order from the original DataFrame\n",
    "    original_column_order = original_df.columns\n",
    "\n",
    "    # Reorder the columns in combined_df to match the original column order\n",
    "    reordered_combined_df = combined_df[original_column_order]\n",
    "\n",
    "    # Print the reordered DataFrame (first 5 rows) to verify\n",
    "    print(\"\\nReordered Combined DataFrame (first 5 rows):\")\n",
    "    print(reordered_combined_df.head())\n",
    "\n",
    "    return reordered_combined_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you already have the DataFrames: combined_df and original_df\n",
    "reordered_combined_df = reorder_columns_by_original_order(combined_df, original_df)\n",
    "\n",
    "####\n",
    "\n",
    "def save_reordered_combined_df(reordered_combined_df, file_name=\"reordered_combined_data.csv\"):\n",
    "    \"\"\"Save the reordered combined DataFrame to a CSV file.\"\"\"\n",
    "    print(f\"\\nSaving the reordered combined DataFrame to {file_name}...\")\n",
    "\n",
    "    # Save the DataFrame to a CSV file with the index\n",
    "    reordered_combined_df.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f\"\\nReordered DataFrame saved successfully to {file_name}.\")\n",
    "\n",
    "# Example usage:\n",
    "save_reordered_combined_df(reordered_combined_df)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Check for NaN values\n",
    "nan_data = reordered_combined_df.isna()\n",
    "\n",
    "# Loop through rows and print missing columns\n",
    "for date, row in nan_data.iterrows():\n",
    "    missing_columns = row[row].index.tolist()  # Get columns with NaN values\n",
    "    if missing_columns:  # If any columns have NaN values\n",
    "        print(f\"Date: {date} | Missing Columns: {missing_columns}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Drop columns with more than 50 NaN values from reordered_combined_df\n",
    "columns_to_drop = reordered_combined_df.columns[reordered_combined_df.isna().sum() > 50]  # Find columns with more than 50 NaNs\n",
    "columns_removed_indices = reordered_combined_df.columns.get_indexer(columns_to_drop).tolist()  # Get column indices for dropped columns\n",
    "\n",
    "# Drop the identified columns\n",
    "df_cleaned = reordered_combined_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the intermediate DataFrame to a CSV file\n",
    "df_cleaned.to_csv(\"cleaned_after_column_dropping.csv\", index=True)  # Save with the index\n",
    "\n",
    "# Print the columns removed and their indices\n",
    "print(f\"Columns removed due to more than 50 NaN values (column names): {columns_to_drop.tolist()}\")\n",
    "print(f\"Indices of removed columns in reordered_combined_df: {columns_removed_indices}\")\n",
    "print(columns_removed_indices)\n",
    "\n",
    "# Print the saved file's name for confirmation\n",
    "print(\"Intermediate DataFrame saved as 'cleaned_after_column_dropping.csv'\")\n",
    "\n",
    "#####\n",
    "\n",
    "# Steps 2 & 3: create a compact dataset dropping rows with missing values for dates at the beginning and at the end of the dataframe\n",
    "## (rows with missing values located at \"inner\" dates were already dealt with in the previous data manipulation where we used a simple linear interpolation method)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_compact_dataset_with_terminal_check_and_validation(input_file, output_file):\n",
    "    # Step 1: Load the DataFrame\n",
    "    df_cleaned = pd.read_csv(input_file, index_col=0, parse_dates=True)\n",
    "\n",
    "    # Step 2: Determine the earliest date with complete data\n",
    "    earliest_date_all_series = df_cleaned.dropna().index.min()\n",
    "\n",
    "    # Step 3: Determine the latest date with complete data\n",
    "    latest_date_all_series = df_cleaned.dropna().index.max()\n",
    "\n",
    "    # Step 4: Filter the DataFrame\n",
    "    # Keep rows between the earliest and latest dates with complete data\n",
    "    df_filtered = df_cleaned.loc[earliest_date_all_series:latest_date_all_series]\n",
    "\n",
    "    # Step 5: Save the compact dataset\n",
    "    df_filtered.to_csv(output_file)\n",
    "    print(f\"Compact dataset saved to: {output_file}\")\n",
    "    print(\"Resulting DataFrame preview:\")\n",
    "    print(df_filtered.head())\n",
    "    print(df_filtered.tail())\n",
    "\n",
    "    # Step 6: Final Check for Missing Data\n",
    "    missing_report = {}\n",
    "    if df_filtered.isna().any().any():  # Check if there are any missing values\n",
    "        print(\"Warning: Missing data detected in the compact dataset.\")\n",
    "        for column in df_filtered.columns:\n",
    "            missing_dates = df_filtered[df_filtered[column].isna()].index.tolist()\n",
    "            if missing_dates:\n",
    "                missing_report[column] = missing_dates\n",
    "                print(f\"Column '{column}' has missing data on the following dates:\")\n",
    "                for date in missing_dates:\n",
    "                    print(f\"  - {date.strftime('%Y-%m-%d')}\")\n",
    "    else:\n",
    "        print(\"No missing data detected in the compact dataset.\")\n",
    "\n",
    "    return df_filtered, missing_report\n",
    "\n",
    "# Example usage\n",
    "compact_df, report = create_compact_dataset_with_terminal_check_and_validation(\n",
    "    \"cleaned_after_column_dropping.csv\",\n",
    "    \"compact_dataset.csv\"\n",
    ")\n",
    "\n",
    "## At the end of the procedure we should obtain a \"compact\" dataset without missing data,\n",
    "## with series starting at 1962-08-01 and ending at 2024-07-01\n",
    "\n",
    "\n",
    "\n",
    "# Step 7: Update the grouping indices based on the cleaned DataFrame\n",
    "# After dropping columns with excessive NaNs, we need to adjust the grouping indices\n",
    "# Create a list of remaining column indices in df_cleaned\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming reordered_combined_df is already loaded\n",
    "# Example of loading the DataFrame (if not already loaded)\n",
    "# reordered_combined_df = pd.read_csv('path_to_your_dataframe.csv')\n",
    "\n",
    "# Define the original group_indices\n",
    "group_indices = {\n",
    "    \"Output and income\": [0, 1] + list(range(5, 19)),\n",
    "    \"Labor market\": list(range(19, 45)) + list(range(113, 116)),\n",
    "    \"Housing\": list(range(45, 55)),\n",
    "    \"Consumption orders and inventories\": list(range(2, 5)) + list(range(55, 60)),\n",
    "    \"Money and credit\": list(range(60, 69)) + list(range(116, 119)) + [120],\n",
    "    \"Interest rate\": list(range(72, 81)),\n",
    "    \"Prices\": list(range(93, 113)),\n",
    "    \"Stock market\": list(range(69, 72)) + [119],\n",
    "    \"Yield spread\": list(range(81, 89)),\n",
    "    \"Exchange rate\": list(range(89, 93))\n",
    "}\n",
    "\n",
    "# Get the column labels (names) from the DataFrame\n",
    "column_labels = reordered_combined_df.columns.tolist()\n",
    "\n",
    "# Create a dictionary to store the grouped labels\n",
    "grouped_labels = {}\n",
    "\n",
    "# Group the labels based on the provided indexes in group_indices\n",
    "for group_name, indices in group_indices.items():\n",
    "    # Get the column names for the current group by mapping indices to column labels\n",
    "    group_labels = [column_labels[idx] for idx in indices if idx < len(column_labels)]  # Ensure valid index range\n",
    "    grouped_labels[group_name] = group_labels\n",
    "\n",
    "# Print the grouped labels for each group\n",
    "for group_name, labels in grouped_labels.items():\n",
    "    print(f\"{group_name}: {labels}\")\n",
    "\n",
    "####\n",
    "\n",
    "columns_removed_labels = columns_to_drop.tolist()\n",
    "\n",
    "# Print the columns to be removed and their corresponding labels\n",
    "print(f\"Columns to be removed (due to more than 50 NaNs): {columns_removed_labels}\")\n",
    "print(f\"Indices of removed columns: {columns_removed_indices}\")\n",
    "\n",
    "####\n",
    "\n",
    "for group_name, labels in grouped_labels.items():\n",
    "    # Filter out the labels that are in the columns_removed_labels list\n",
    "    updated_labels = [label for label in labels if label not in columns_removed_labels]\n",
    "    grouped_labels[group_name] = updated_labels\n",
    "\n",
    "# Step 5: Print the updated grouped labels after removing the unwanted columns\n",
    "for group_name, labels in grouped_labels.items():\n",
    "    print(f\"{group_name}: {labels}\")\n",
    "\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the compact dataset\n",
    "df_compact = pd.read_csv(\"compact_dataset.csv\")  # Load the transformed dataframe\n",
    "column_labels_updated = df_compact.columns.tolist()  # Get the column labels from the compact dataset\n",
    "\n",
    "# Step 2: Create a reverse mapping of column labels to indices in the compact dataset\n",
    "label_to_index = {label: idx for idx, label in enumerate(column_labels_updated)}\n",
    "\n",
    "# Step 3: Convert grouped labels into grouped indices\n",
    "grouped_indices = {}\n",
    "\n",
    "# Iterate through the grouped labels and convert each label into its corresponding index\n",
    "for group_name, labels in grouped_labels.items():\n",
    "    # Map each label to its index using the label_to_index mapping\n",
    "    updated_indices = [label_to_index[label] for label in labels if label in label_to_index]\n",
    "    grouped_indices[group_name] = updated_indices\n",
    "\n",
    "# Step 4: Print the updated grouped indices\n",
    "for group_name, indices in grouped_indices.items():\n",
    "    print(f\"{group_name}: {indices}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "###         Performing the ADF test and grouping the indexes by type         ###\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Step 1: Load the compact dataset with a DateTime index\n",
    "df_compact = pd.read_csv(\"compact_dataset.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Step 2: Initialize dictionaries to store stationary and non-stationary series\n",
    "stationary_indices = []\n",
    "stationary_labels = []\n",
    "non_stationary_indices = []\n",
    "non_stationary_labels = []\n",
    "\n",
    "# Step 3: Perform the Augmented Dickey-Fuller test on each column\n",
    "for idx, column in enumerate(df_compact.columns):\n",
    "    # Perform ADF test on each column (dropna to handle missing values)\n",
    "    result = adfuller(df_compact[column].dropna())  # dropna to handle missing values\n",
    "\n",
    "    # Get the p-value from the result\n",
    "    p_value = result[1]\n",
    "\n",
    "    # Classify as stationary or non-stationary based on p-value\n",
    "    if p_value < 0.05:\n",
    "        stationary_indices.append(idx)\n",
    "        stationary_labels.append(column)\n",
    "    else:\n",
    "        non_stationary_indices.append(idx)\n",
    "        non_stationary_labels.append(column)\n",
    "\n",
    "# Step 4: Return the stationary and non-stationary series by index and label\n",
    "print(\"Stationary series:\")\n",
    "for idx, label in zip(stationary_indices, stationary_labels):\n",
    "    print(f\"Index: {idx}, Label: {label}\")\n",
    "\n",
    "print(\"\\nNon-stationary series:\")\n",
    "for idx, label in zip(non_stationary_indices, non_stationary_labels):\n",
    "    print(f\"Index: {idx}, Label: {label}\")\n",
    "\n",
    "### After running this method, we confirm that all the series are indeed stationary\n",
    "\n",
    "#### Possible logic for handling non-stationary series (not necessary for the \"compact\" dataset) ####\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5RL4e48mvLt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733176034981,
     "user_tz": -60,
     "elapsed": 4835,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "2d2dfae2-0d41-4999-e78e-e02d725edb2d"
   },
   "id": "Y5RL4e48mvLt",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stationary series:\n",
      "Index: 0, Label: RPI\n",
      "Index: 1, Label: W875RX1\n",
      "Index: 2, Label: DPCERA3M086SBEA\n",
      "Index: 3, Label: CMRMTSPLx\n",
      "Index: 4, Label: RETAILx\n",
      "Index: 5, Label: INDPRO\n",
      "Index: 6, Label: IPFPNSS\n",
      "Index: 7, Label: IPFINAL\n",
      "Index: 8, Label: IPCONGD\n",
      "Index: 9, Label: IPDCONGD\n",
      "Index: 10, Label: IPNCONGD\n",
      "Index: 11, Label: IPBUSEQ\n",
      "Index: 12, Label: IPMAT\n",
      "Index: 13, Label: IPDMAT\n",
      "Index: 14, Label: IPNMAT\n",
      "Index: 15, Label: IPMANSICS\n",
      "Index: 16, Label: IPB51222S\n",
      "Index: 17, Label: IPFUELS\n",
      "Index: 18, Label: CUMFNS\n",
      "Index: 19, Label: CLF16OV\n",
      "Index: 20, Label: CE16OV\n",
      "Index: 21, Label: UNRATE\n",
      "Index: 22, Label: UEMPMEAN\n",
      "Index: 23, Label: UEMPLT5\n",
      "Index: 24, Label: UEMP5TO14\n",
      "Index: 25, Label: UEMP15OV\n",
      "Index: 26, Label: UEMP15T26\n",
      "Index: 27, Label: UEMP27OV\n",
      "Index: 28, Label: CLAIMSx\n",
      "Index: 29, Label: PAYEMS\n",
      "Index: 30, Label: USGOOD\n",
      "Index: 31, Label: CES1021000001\n",
      "Index: 32, Label: USCONS\n",
      "Index: 33, Label: MANEMP\n",
      "Index: 34, Label: DMANEMP\n",
      "Index: 35, Label: NDMANEMP\n",
      "Index: 36, Label: SRVPRD\n",
      "Index: 37, Label: USTPU\n",
      "Index: 38, Label: USWTRADE\n",
      "Index: 39, Label: USTRADE\n",
      "Index: 40, Label: USFIRE\n",
      "Index: 41, Label: USGOVT\n",
      "Index: 42, Label: CES0600000007\n",
      "Index: 43, Label: AWOTMAN\n",
      "Index: 44, Label: AWHMAN\n",
      "Index: 45, Label: HOUST\n",
      "Index: 46, Label: HOUSTNE\n",
      "Index: 47, Label: HOUSTMW\n",
      "Index: 48, Label: HOUSTS\n",
      "Index: 49, Label: HOUSTW\n",
      "Index: 50, Label: PERMIT\n",
      "Index: 51, Label: PERMITNE\n",
      "Index: 52, Label: PERMITMW\n",
      "Index: 53, Label: PERMITS\n",
      "Index: 54, Label: PERMITW\n",
      "Index: 55, Label: BUSINVx\n",
      "Index: 56, Label: ISRATIOx\n",
      "Index: 57, Label: M1SL\n",
      "Index: 58, Label: M2SL\n",
      "Index: 59, Label: M2REAL\n",
      "Index: 60, Label: BOGMBASE\n",
      "Index: 61, Label: TOTRESNS\n",
      "Index: 62, Label: BUSLOANS\n",
      "Index: 63, Label: REALLN\n",
      "Index: 64, Label: NONREVSL\n",
      "Index: 65, Label: CONSPI\n",
      "Index: 66, Label: S&P 500\n",
      "Index: 67, Label: S&P div yield\n",
      "Index: 68, Label: S&P PE ratio\n",
      "Index: 69, Label: FEDFUNDS\n",
      "Index: 70, Label: CP3Mx\n",
      "Index: 71, Label: TB3MS\n",
      "Index: 72, Label: TB6MS\n",
      "Index: 73, Label: GS1\n",
      "Index: 74, Label: GS5\n",
      "Index: 75, Label: GS10\n",
      "Index: 76, Label: AAA\n",
      "Index: 77, Label: BAA\n",
      "Index: 78, Label: COMPAPFFx\n",
      "Index: 79, Label: TB3SMFFM\n",
      "Index: 80, Label: TB6SMFFM\n",
      "Index: 81, Label: T1YFFM\n",
      "Index: 82, Label: T5YFFM\n",
      "Index: 83, Label: T10YFFM\n",
      "Index: 84, Label: AAAFFM\n",
      "Index: 85, Label: BAAFFM\n",
      "Index: 86, Label: EXSZUSx\n",
      "Index: 87, Label: EXJPUSx\n",
      "Index: 88, Label: EXUSUKx\n",
      "Index: 89, Label: EXCAUSx\n",
      "Index: 90, Label: WPSFD49207\n",
      "Index: 91, Label: WPSFD49502\n",
      "Index: 92, Label: WPSID61\n",
      "Index: 93, Label: WPSID62\n",
      "Index: 94, Label: OILPRICEx\n",
      "Index: 95, Label: PPICMM\n",
      "Index: 96, Label: CPIAUCSL\n",
      "Index: 97, Label: CPIAPPSL\n",
      "Index: 98, Label: CPITRNSL\n",
      "Index: 99, Label: CPIMEDSL\n",
      "Index: 100, Label: CUSR0000SAC\n",
      "Index: 101, Label: CUSR0000SAD\n",
      "Index: 102, Label: CUSR0000SAS\n",
      "Index: 103, Label: CPIULFSL\n",
      "Index: 104, Label: CUSR0000SA0L2\n",
      "Index: 105, Label: CUSR0000SA0L5\n",
      "Index: 106, Label: PCEPI\n",
      "Index: 107, Label: DDURRG3M086SBEA\n",
      "Index: 108, Label: DNDGRG3M086SBEA\n",
      "Index: 109, Label: DSERRG3M086SBEA\n",
      "Index: 110, Label: CES0600000008\n",
      "Index: 111, Label: CES2000000008\n",
      "Index: 112, Label: CES3000000008\n",
      "Index: 113, Label: DTCOLNVHFNM\n",
      "Index: 114, Label: DTCTHFNM\n",
      "Index: 115, Label: INVEST\n",
      "Index: 116, Label: VIXCLSx\n",
      "Index: 117, Label: BORROW\n",
      "\n",
      "Non-stationary series:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load the compact dataset from CSV, ensuring the time index is recognized\n",
    "df_compact = pd.read_csv(\"compact_dataset.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Step 2: Standardize the numeric data (ignoring the index and any non-numeric columns)\n",
    "# Select only the numeric columns (ignore the index and non-numeric columns if any)\n",
    "numeric_columns = df_compact.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 3: Standardize the numeric columns while preserving the time index\n",
    "df_standardized = df_compact.copy()  # Copy the original dataframe to preserve it\n",
    "df_standardized[numeric_columns] = scaler.fit_transform(df_compact[numeric_columns])\n",
    "\n",
    "# Step 4: Optionally, save the standardized data to a new CSV file, preserving the time index\n",
    "df_standardized.to_csv(\"standardized_compact_dataset.csv\")\n",
    "\n",
    "# Step 5: Print out the first few rows to check the results, including the time index\n",
    "print(df_standardized.head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idoiqnrxyGf8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733176997353,
     "user_tz": -60,
     "elapsed": 303,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "0d5cb0d0-5edb-44ff-afd6-dbb2e2849aae"
   },
   "id": "idoiqnrxyGf8",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 RPI   W875RX1  DPCERA3M086SBEA  CMRMTSPLx   RETAILx  \\\n",
      "Date                                                                   \n",
      "1962-08-01 -0.084064 -0.208777         0.017413   1.777556  0.082297   \n",
      "1962-09-01 -0.176108 -0.197333         1.067156  -1.526084 -0.121265   \n",
      "1962-10-01  0.212630  0.145574        -0.954391   1.495279  0.680337   \n",
      "1962-11-01  0.083496  0.381470         1.334141   0.721807  0.019433   \n",
      "1962-12-01  0.164745  0.458573         0.367552  -2.519030 -0.322105   \n",
      "\n",
      "              INDPRO   IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  ...  \\\n",
      "Date                                                          ...   \n",
      "1962-08-01 -0.092267 -0.190862 -0.380945 -1.236353 -0.508730  ...   \n",
      "1962-09-01  0.472062  0.476963  0.225621  0.472409  0.326698  ...   \n",
      "1962-10-01 -0.093099 -0.522685 -0.078114 -0.336347  0.055647  ...   \n",
      "1962-11-01  0.242798  0.474481  0.223506  0.469855 -0.044289  ...   \n",
      "1962-12-01 -0.205360 -0.080599  0.121431  0.265478  0.154070  ...   \n",
      "\n",
      "            DNDGRG3M086SBEA  DSERRG3M086SBEA  CES0600000008  CES2000000008  \\\n",
      "Date                                                                         \n",
      "1962-08-01         0.261691         0.028121      -0.046169      -0.489930   \n",
      "1962-09-01         1.479141        -0.873851      -0.006174      -0.506620   \n",
      "1962-10-01        -0.164820         0.126340      -2.385453       0.000912   \n",
      "1962-11-01        -0.027181         0.072394      -0.035664      -0.517763   \n",
      "1962-12-01        -0.146434        -0.073353      -0.006174       0.447050   \n",
      "\n",
      "            CES3000000008  DTCOLNVHFNM  DTCTHFNM    INVEST   VIXCLSx    BORROW  \n",
      "Date                                                                            \n",
      "1962-08-01      -0.037215     0.340482  0.457586 -0.465181 -1.029615 -0.036315  \n",
      "1962-09-01       2.140716    -0.500227 -0.065853 -0.193879  0.701969 -0.037848  \n",
      "1962-10-01      -2.186611     0.008159  0.100127  1.253240  1.970147 -0.037579  \n",
      "1962-11-01      -0.008932     0.218885  0.348996 -0.598859 -1.821984 -0.035092  \n",
      "1962-12-01      -1.071727    -0.244038  0.523922  0.666098 -0.693402 -0.034592  \n",
      "\n",
      "[5 rows x 118 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Soybilgen and Yazgan, pp. 391-392"
   ],
   "metadata": {
    "id": "RY3I6G5DH1c-"
   },
   "id": "RY3I6G5DH1c-"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kalman update diag method (is required by the Kalman filter diag  method)"
   ],
   "metadata": {
    "id": "dZu4qOIXIuP4"
   },
   "id": "dZu4qOIXIuP4"
  },
  {
   "cell_type": "code",
   "source": [
    "# run_cell = False  # Change to True to run this block\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv, det\n",
    "\n",
    "def kalman_update_diag(A, C, Q, R, y, x, V, initial):\n",
    "    \"\"\"\n",
    "    Perform a one-step Kalman filter update.\n",
    "\n",
    "    Parameters:\n",
    "    A, C, Q, R : np.array\n",
    "        System and observation matrices and covariances.\n",
    "    y : np.array\n",
    "        Observation vector for current time step.\n",
    "    x : np.array\n",
    "        Prior state mean estimate.\n",
    "    V : np.array\n",
    "        Prior covariance estimate.\n",
    "    initial : bool\n",
    "        Whether this is the initial step.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        xnew : np.array\n",
    "            Updated state estimate.\n",
    "        Vnew : np.array\n",
    "            Updated covariance estimate.\n",
    "        VVnew : np.array\n",
    "            Cross-covariance estimate.\n",
    "        loglik : float\n",
    "            Log-likelihood of the current observation.\n",
    "    \"\"\"\n",
    "    ss = A.shape[0]  # State size\n",
    "\n",
    "    # Prediction step\n",
    "    if initial:\n",
    "        xpred = x\n",
    "        Vpred = V\n",
    "    else:\n",
    "        xpred = A @ x\n",
    "        Vpred = A @ V @ A.T + Q\n",
    "\n",
    "    # Innovation\n",
    "    e = y - C @ xpred\n",
    "    S = C @ Vpred @ C.T + R\n",
    "    Sinv = np.linalg.inv(S)\n",
    "\n",
    "    # Log-likelihood calculation\n",
    "    detS = det(S)\n",
    "    loglik = -0.5 * (np.log(detS) + e.T @ Sinv @ e + len(e) * np.log(2 * np.pi))\n",
    "\n",
    "    # Kalman gain\n",
    "    K = Vpred @ C.T @ Sinv\n",
    "\n",
    "    # State and covariance update\n",
    "    xnew = xpred + K @ e\n",
    "    Vnew = (np.eye(ss) - K @ C) @ Vpred\n",
    "\n",
    "    # Cross-covariance\n",
    "    VVnew = (np.eye(ss) - K @ C) @ A @ V\n",
    "\n",
    "    return {\"xnew\": xnew, \"Vnew\": Vnew, \"VVnew\": VVnew, \"loglik\": loglik.item()\n",
    "    }\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "QL9T_r-cI2nZ"
   },
   "id": "QL9T_r-cI2nZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kalman filter diag method (requires Kalman update diag method; is required by Kalman smoother diag method)"
   ],
   "metadata": {
    "id": "daoQc38vIVZ_"
   },
   "id": "daoQc38vIVZ_"
  },
  {
   "cell_type": "code",
   "source": [
    "# run_cell = False  # Change to True to run this block\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def kalman_filter_diag(y, A, C, Q, R, init_x, init_V, model):\n",
    "    \"\"\"\n",
    "    Kalman filter implementation.\n",
    "\n",
    "    Parameters:\n",
    "    y : np.array\n",
    "        Observations matrix of shape (N, T).\n",
    "    A : np.array\n",
    "        State transition matrices of shape (ss, ss, T).\n",
    "    C : np.array\n",
    "        Observation matrices of shape (N, ss, T).\n",
    "    Q : np.array\n",
    "        Process noise covariance matrices of shape (ss, ss, T).\n",
    "    R : np.array\n",
    "        Measurement noise covariance matrices of shape (N, N, T).\n",
    "    init_x : np.array\n",
    "        Initial state estimate of shape (ss,).\n",
    "    init_V : np.array\n",
    "        Initial covariance estimate of shape (ss, ss).\n",
    "    model : list or range\n",
    "        Sequence of model indices for each time step.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        x : np.array\n",
    "            Filtered state estimates of shape (ss, T).\n",
    "        V : np.array\n",
    "            Filtered state covariances of shape (ss, ss, T).\n",
    "        VV : np.array\n",
    "            Cross-covariances of shape (ss, ss, T).\n",
    "    \"\"\"\n",
    "    os = y.shape[0]  # Number of observations\n",
    "    T = y.shape[1]  # Number of time steps\n",
    "    ss = A.shape[0]  # State space size\n",
    "\n",
    "    x = np.zeros((ss, T))\n",
    "    V = np.zeros((ss, ss, T))\n",
    "    VV = np.zeros((ss, ss, T))\n",
    "    loglik = 0\n",
    "\n",
    "    for t in range(T):\n",
    "        m = model[t]\n",
    "        if t == 0:\n",
    "            prevx = init_x\n",
    "            prevV = init_V\n",
    "            initial = True\n",
    "        else:\n",
    "            prevx = x[:, t-1].reshape(-1, 1)\n",
    "            prevV = V[:, :, t-1]\n",
    "            initial = False\n",
    "\n",
    "        result_kud = kalman_update_diag(\n",
    "            A[:, :, m-1], C[:, :, m-1], Q[:, :, m-1], R[:, :, m-1],\n",
    "            y[:, t].reshape(-1, 1), prevx, prevV, initial\n",
    "        )\n",
    "\n",
    "        x[:, t] = result_kud[\"xnew\"].flatten()\n",
    "        V[:, :, t] = result_kud[\"Vnew\"]\n",
    "        VV[:, :, t] = result_kud[\"VVnew\"]\n",
    "        loglik += result_kud[\"loglik\"]\n",
    "\n",
    "    return {\"x\": x, \"V\": V, \"VV\": VV}\n",
    "\n"
   ],
   "metadata": {
    "id": "D8e7YOx3ISme"
   },
   "id": "D8e7YOx3ISme",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Smooth update method (is required by Kalman smoother diag)"
   ],
   "metadata": {
    "id": "S-eNTuycuzfk"
   },
   "id": "S-eNTuycuzfk"
  },
  {
   "cell_type": "code",
   "source": [
    "# run_cell = False  # Change to True to run this block\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if the file already exists\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "def smooth_update(xsmooth_future, Vsmooth_future, xfilt, Vfilt, Vfilt_future, VVfilt_future, A, Q):\n",
    "    \"\"\"\n",
    "    Perform one step of the backwards RTS smoothing equations.\n",
    "\n",
    "    Parameters:\n",
    "    xsmooth_future : np.array\n",
    "        E[X_t+1|T], Smoothed state estimate at t+1.\n",
    "    Vsmooth_future : np.array\n",
    "        Cov[X_t+1|T], Smoothed covariance at t+1.\n",
    "    xfilt : np.array\n",
    "        E[X_t|t], Filtered state estimate at time t.\n",
    "    Vfilt : np.array\n",
    "        Cov[X_t|t], Filtered covariance at time t.\n",
    "    Vfilt_future : np.array\n",
    "        Cov[X_t+1|t+1], Filtered covariance at time t+1.\n",
    "    VVfilt_future : np.array\n",
    "        Cov[X_t+1, X_t|t+1], Cross-covariance at time t+1.\n",
    "    A : np.array\n",
    "        State transition matrix at time t+1.\n",
    "    Q : np.array\n",
    "        Process noise covariance at time t+1.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        xsmooth : np.array\n",
    "            E[X_t|T], Smoothed state estimate at time t.\n",
    "        Vsmooth : np.array\n",
    "            Cov[X_t|T], Smoothed covariance at time t.\n",
    "    \"\"\"\n",
    "    # Prediction step\n",
    "    xpred = A @ xfilt\n",
    "    Vpred = A @ Vfilt @ A.T + Q\n",
    "\n",
    "    # Smoother gain matrix\n",
    "    J = Vfilt @ A.T @ pinv(Vpred)\n",
    "\n",
    "    # Smoothed estimates\n",
    "    xsmooth = xfilt + J @ (xsmooth_future - xpred)\n",
    "    Vsmooth = Vfilt + J @ (Vsmooth_future - Vpred) @ J.T\n",
    "\n",
    "    return {\"xsmooth\": xsmooth, \"Vsmooth\": Vsmooth}\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "HcStPthZwwiO"
   },
   "id": "HcStPthZwwiO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kalman smoother diag (requires Kalman filter diag and Smoother update; is required by Factor extraction)"
   ],
   "metadata": {
    "id": "3-dyYJuYgRg_"
   },
   "id": "3-dyYJuYgRg_"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "# from smooth_update import smooth_update\n",
    "# from kalman_filter_diag import kalman_filter_diag\n",
    "\n",
    "\n",
    "def kalman_smoother_diag(y, A, C, Q, R, init_x, init_V, model):\n",
    "    \"\"\"\n",
    "    Custom implementation of the Kalman smoother for a time-varying state-space model.\n",
    "\n",
    "    Parameters:\n",
    "    y : np.array\n",
    "        Observations matrix of shape (N, T) where N is the number of observed variables and T is the number of time steps.\n",
    "    A : np.array\n",
    "        State transition matrices of shape (ss, ss, T).\n",
    "    C : np.array\n",
    "        Observation matrices of shape (N, ss, T).\n",
    "    Q : np.array\n",
    "        Process noise covariance matrices of shape (ss, ss, T).\n",
    "    R : np.array\n",
    "        Measurement noise covariance matrices of shape (N, N, T).\n",
    "    init_x : np.array\n",
    "        Initial state estimate of shape (ss,).\n",
    "    init_V : np.array\n",
    "        Initial covariance estimate of shape (ss, ss).\n",
    "    model : list or range\n",
    "        Sequence of model time steps, typically range(1, T+1).\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        Contains xsmooth (smoothed state estimates) and Vsmooth (smoothed covariance estimates).\n",
    "    \"\"\"\n",
    "    T = y.shape[1]\n",
    "    ss = A.shape[0]\n",
    "\n",
    "    # Initialize smoothed state and covariance arrays\n",
    "    xsmooth = np.zeros((ss, T))\n",
    "    Vsmooth = np.zeros((ss, ss, T))\n",
    "\n",
    "    # Forward pass: Run the Kalman filter\n",
    "    kfd_result = kalman_filter_diag(y, A, C, Q, R, init_x, init_V, model)\n",
    "    xfilt = kfd_result[\"x\"]\n",
    "    Vfilt = kfd_result[\"V\"]\n",
    "    VVfilt = kfd_result[\"VV\"]\n",
    "\n",
    "    # Backward pass: Run the RTS smoother\n",
    "    xsmooth[:, T-1] = xfilt[:, T-1]\n",
    "    Vsmooth[:, :, T-1] = Vfilt[:, :, T-1]\n",
    "\n",
    "    for t in range(T-2, -1, -1):\n",
    "        m = model[t+1]\n",
    "        result_s_update = smooth_update(\n",
    "            xsmooth[:, t+1].reshape(-1, 1),\n",
    "            Vsmooth[:, :, t+1],\n",
    "            xfilt[:, t].reshape(-1, 1),\n",
    "            Vfilt[:, :, t],\n",
    "            Vfilt[:, :, t+1],\n",
    "            VVfilt[:, :, t+1],\n",
    "            A[:, :, m-1],\n",
    "            Q[:, :, m-1]\n",
    "        )\n",
    "\n",
    "        xsmooth[:, t] = result_s_update[\"xsmooth\"].flatten()\n",
    "        Vsmooth[:, :, t] = result_s_update[\"Vsmooth\"]\n",
    "\n",
    "    return {\"xsmooth\": xsmooth, \"Vsmooth\": Vsmooth}\n",
    "\n"
   ],
   "metadata": {
    "id": "6S5BUj4vgPky"
   },
   "id": "6S5BUj4vgPky",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### RicSW (is required by Factor extraction)"
   ],
   "metadata": {
    "id": "XoRnkO5QhEx1"
   },
   "id": "XoRnkO5QhEx1"
  },
  {
   "cell_type": "code",
   "source": [
    "### Testing code\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import eig, pinv, block_diag\n",
    "from numpy.linalg import inv\n",
    "\n",
    "x = standardized_df.values  # Assuming standardized_df is a DataFrame\n",
    "T, N = x.shape\n",
    "\n",
    "# Output the values of T, N, and the first rows of x\n",
    "print(\"T (Number of rows):\", T)\n",
    "print(\"N (Number of columns):\", N)\n",
    "print(\"First 5 rows and 5 columns of x:\\n\", x[:5, :5])\n",
    "\n",
    "r = 2\n",
    "q = 2\n",
    "p = 1\n",
    "nlag = p - 1\n",
    "\n",
    "A_temp = np.zeros((r, r * p)) # Creates a temporary matrix filled with zeros of dimension r x r*p\n",
    "I = np.eye(r * p) # Creates an indentiy matrix of size r*p x r*p\n",
    "\n",
    "print(type(A_temp))\n",
    "print(\"Size (number of elements):\", A_temp.size)\n",
    "print(\"Shape (dimensions):\", A_temp.shape)\n",
    "print(\"Number of dimensions:\", A_temp.ndim)\n",
    "print(A_temp[:2])\n",
    "\n",
    "print(type(I))\n",
    "LL = I.shape[0]\n",
    "if p != 1:\n",
    "    A = np.vstack((A_temp.T, I[:LL-r, :]))  # Equivalent to rbind(A_temp, I[1:(LL-r), ])\n",
    "else:\n",
    "    A = np.vstack((A_temp.T, np.empty((0, r * p))))  # Equivalent to rbind(t(A_temp), I[0, ]) # Modified due to different indexing between R and Python\n",
    "\n",
    "\n",
    "print(\"Size (number of elements):\", A.size)\n",
    "print(\"Shape (dimensions):\", A.shape)\n",
    "print(\"Number of dimensions:\", A.ndim)\n",
    "print(A[:3]) # 2 x 2 mtrix of zeros\n",
    "\n",
    "Q = np.zeros((r * p, r * p)) # 2 x 2 identiy matrix\n",
    "Q[:r, :r] = np.eye(r)\n",
    "print(\"Size (number of elements):\", Q.size)\n",
    "print(\"Shape (dimensions):\", Q.shape)\n",
    "print(\"Number of dimensions:\", Q.ndim)\n",
    "print(Q[:3])\n",
    "\n",
    "cov_x = np.cov(x, rowvar=False)\n",
    "print(\"Size (number of elements):\", cov_x.size)\n",
    "print(\"Shape (dimensions):\", cov_x.shape)\n",
    "print(\"Number of dimensions:\", cov_x.ndim)\n",
    "print(cov_x[:3])\n",
    "eigvals, eigvecs = eig(cov_x)\n",
    "# Print eigenvalues\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigvals)\n",
    "print(\"Size (number of elements):\", eigvals.size)\n",
    "\n",
    "# Print eigenvectors\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigvecs)\n",
    "print(\"Size (number of elements):\", eigvecs.shape)\n",
    "\n",
    "\n",
    "idx = eigvals.argsort()[::-1]\n",
    "print(idx)\n",
    "\n",
    "eigvals, eigvecs = eigvals[idx][:r], eigvecs[:, idx][:, :r]\n",
    "print(eigvals.shape)\n",
    "print(eigvecs.shape)\n",
    "\n",
    "F = x @ eigvecs\n",
    "print(x.size)\n",
    "print(x.shape)\n",
    "print(F.shape)\n",
    "print(F) # 379 x 2 matrix\n",
    "\n",
    "R = np.diag(np.diag(np.cov(x - F @ eigvecs.T, rowvar=False)))\n",
    "print(R)\n",
    "print(R.shape) # 118 x 118 matrix\n",
    "\n",
    "\n",
    "# VAR model estimation\n",
    "Z = F[:-1, :]\n",
    "z = F[1:, :]\n",
    "print(Z.shape)\n",
    "print(Z)\n",
    "print(z)\n",
    "print(z.shape)\n",
    "\n",
    "\n",
    "A_temp = inv(Z.T @ Z) @ Z.T @ z\n",
    "A[:r, :r * p] = A_temp.T\n",
    "print(A_temp.shape)\n",
    "\n",
    "e = z - Z @ A_temp  # VAR residuals\n",
    "print(e.shape)\n",
    "print(e)\n",
    "H = np.cov(e, rowvar=False)\n",
    "print(H.shape)\n",
    "\n",
    "Q[:r, :r] = H\n",
    "print(Q.shape)\n",
    "print(Q)\n",
    "\n",
    "#### Execute until here ####\n",
    "print(A.shape)\n",
    "\n",
    "initx = F[0, :]\n",
    "kron_A = np.kron(A, A)\n",
    "print(kron_A.shape)\n",
    "print(kron_A)\n",
    "\n",
    "matrix_1 = np.eye(r * p**2)\n",
    "print(matrix_1.shape)\n",
    "print(matrix_1)\n",
    "\n",
    "Q_flatten = Q.flatten(order='F').reshape(-1, 1)\n",
    "print(Q_flatten.shape)\n",
    "\n",
    "diag_matrix = np.eye(kron_A.shape[0])\n",
    "matrix_2 = pinv(diag_matrix - kron_A)\n",
    "print(matrix_2.shape)\n",
    "\n",
    "initV = pinv(diag_matrix - kron_A) @ Q.flatten(order='F').reshape(-1, 1)\n",
    "print(initV.shape)\n",
    "initV = initV.reshape((r * p, r * p), order='F')\n",
    "print(initV)\n",
    "C = np.hstack((eigvecs, np.zeros((N, r * nlag))))\n",
    "print(C)\n",
    "\n",
    "print(C.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "OV1m82c7eqdn",
    "executionInfo": {
     "status": "error",
     "timestamp": 1732298154168,
     "user_tz": -60,
     "elapsed": 288,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "afb81c62-d69a-49ac-bb8e-3b49a0e0f5f6"
   },
   "id": "OV1m82c7eqdn",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-60-085b32debabd>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mC\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'C' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.linalg import eig, pinv, block_diag\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def ricSW(standardized_df, q, r, p):\n",
    "    \"\"\"\n",
    "    Computes parameters for a factor model using standardized data.\n",
    "\n",
    "    Parameters:\n",
    "    standardized_df : np.array\n",
    "        Standardized and balanced panel data of size\n",
    "    q : int\n",
    "        Rank for reduced Q covariance matrix (if applicable).\n",
    "    r : int\n",
    "        Number of factors.\n",
    "    p : int\n",
    "        Lag order for VAR.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        A dictionary containing factor model parameters.\n",
    "    \"\"\"\n",
    "    x = standardized_df.values  # Assuming standardized_df is a DataFrame\n",
    "    T, N = x.shape # T:number of rows N: number of columns\n",
    "    nlag = p - 1  # Order of lags in the VAR model for the factors. Typically zero if p=1 (number of additional lags beyond t-1)\n",
    "\n",
    "    # Companion form initialization\n",
    "    # State-space representation: In a dynamic factor model, factors evolve according to a VAR process.\n",
    "    # This requires representing the state transitions and noise in a specific structured form: xt​=Axt−1​+εt​, εt​∼N(0,Q)\n",
    "    A_temp = np.zeros((r, r * p)) # Initializes A, the state transition matrix, to store the VAR coefficients for the first r variables (factors) across p lags.\n",
    "    # Initially, this is all zeros. Dimensions r=number of factors; r*p= total number of variables in the companion form, accounting for p lags.\n",
    "    I = np.eye(r * p) # Creates an identity matrix of size r*p x r*p\n",
    "    # Logic for the creation of the companion matrix A, which represents how the factors evolve over time\n",
    "    if p != 1:\n",
    "        A = np.vstack((A_temp, I[:-r, :])) # Matrix obtained by stacking the matrix of the VAR coefficient and the identity matrix that shifts lagged factors (slice of the identity matrix). Size r*(p−1) x r*p\n",
    "    else:\n",
    "        A = np.vstack((A_temp.T, np.zeros((1, r * p))))\n",
    "\n",
    "    Q = np.zeros((r * p, r * p)) # Initializes a matrix Q of zeros\n",
    "    Q[:r, :r] = np.eye(r) # Fills the top left of the matrix Q with an identity matrix\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors of the covariance matrix\n",
    "    cov_x = np.cov(x, rowvar=False) # Computing the covariance of the data in x. With Rowvar = False, we are calculating the covariance between columns (features)\n",
    "    eigvals, eigvecs = eig(cov_x) # Computing the eigenvalues and eigenvectors of the covariance matrix\n",
    "    idx = eigvals.argsort()[::-1]  # Sort eigenvalues in descending order\n",
    "    eigvals, eigvecs = eigvals[idx][:r], eigvecs[:, idx][:, :r] # Selecting the top r eigenvalues and their corresponding eigenvectors\n",
    "\n",
    "    # Principal component estimates\n",
    "    F = x @ eigvecs # Transforms the original data x into a new space defined by the eigenvectors.\n",
    "    # Essentially, it projects the data onto the new axes (principal components) defined by eigvecs. The matrix F now represents the data in the reduced principal component space.\n",
    "    R = np.diag(np.diag(np.cov(x - F @ eigvecs.T, rowvar=False)))\n",
    "    # 1) eigvecs.T is the transpose of the matrix of eigenvectors. The matrix multiplication F @ eigvecs.T reconstructs the approximation of the original data points from their projections (F),\n",
    "    # by multiplying the transformed data (F) by the eigenvectors. F @ eigvecs.T brings the data back to the original feature space (though the approximation may have some loss due to dimensionality reduction).\n",
    "    # 2) x - F @ eigvecs.T computes the difference between the original data x and the approximation of the data obtained by projecting it back to the original feature space.\n",
    "    # This difference represents the reconstruction error or the residuals between the original data and the approximated data.\n",
    "    # 3) np.cov() computes the covariance matrix of the reconstruction error\n",
    "    # 4) np.diag(np.diag(...)) takes the covariance matrix of the residuals and extracts the diagonal elements\n",
    "    # In brief, R tells us how much variance remains in the data for each feature after projecting it onto the principal component space defined by eigvecs.\n",
    "    # It captures the variance that was not explained by the selected principal components.\n",
    "\n",
    "    # VAR model estimation\n",
    "    if p == 1:\n",
    "    # For VAR(1), we just need one lag: F_{t-1}\n",
    "        Z = F[:-1, :]         # Lagged values: F_{t-1}. Z contains the lagged values of the state vector F (the variables we're modeling).\n",
    "        z = F[1:, :]          # Current values: F_t\n",
    "    else:\n",
    "    # For VAR(p), we need p lags: F_{t-1}, F_{t-2}, ..., F_{t-p}\n",
    "        Z = np.hstack([F[p - kk - 1:-(kk + 1), :] for kk in range(p)])  # Stack lags F_{t-1}, F_{t-2}, ..., F_{t-p}\n",
    "        z = F[p:, :]          # Current values: F_t\n",
    "\n",
    "\n",
    "    # 1) For p=1 (VAR(1)):\n",
    "    # We only need the first lag, Ft−1​, which is the immediate previous value of F.\n",
    "    # The design matrix Z is simply F[:−1,:], which takes all rows except the last (i.e., the lagged values).\n",
    "    # The response matrix Ft is F[1:,:], which contains the current values of F starting from the second row.\n",
    "\n",
    "    # 2) For p>1 (VAR(p)):\n",
    "    # We need to include multiple lags: Ft−1,Ft−2,…,Ft−p​.\n",
    "    # The design matrix Z is built by horizontally stacking the lagged values of F from Ft−1 to Ft−p​ using np.hstack.\n",
    "    # The response matrix Ft​ is built by taking all rows starting from index p (i.e., excluding the first p rows).\n",
    "    # This is the case when we have a higher-order VAR model, where each observation depends on several past observations.\n",
    "    # This is the standard case for a VAR(1) model where each observation depends only on the previous observation.\n",
    "\n",
    "    # 3) Result: Z is a matrix of shape (n_samples - nlag, p * n_features) that combines the p previous lags (time steps) of data from F.\n",
    "    # This matrix Z forms the \"design matrix\" used in VAR estimation.\n",
    "\n",
    "    A_temp = inv(Z.T @ Z) @ Z.T @ z\n",
    "    # performs the OLS estimation for the VAR model. It estimates the coefficients (weights) of the model that relate the lagged values (from the design matrix Z) to the current values (in z)\n",
    "    A[:r, :r * p] = A_temp.T\n",
    "    # This places the transposed estimated coefficients A_temp.T into the matrix A.\n",
    "    # The result is that the coefficient matrix A is populated with the estimated values for the VAR model from A_temp.\n",
    "\n",
    "    # Compute Q\n",
    "    e = z - Z @ A_temp  # VAR residuals\n",
    "    # Residuals represent the differences between the observed values (z) and the predicted values (Z @ A_temp).\n",
    "    # These residuals capture the error in the VAR model's predictions.\n",
    "    # The residuals, e, represent the part of the observed data that cannot be explained by the lagged values of the series.\n",
    "    H = np.cov(e, rowvar=False) # Covariance matrix of the residuals e.\n",
    "    # This matrix, H, gives an estimate of how the errors in the VAR model are related to each other.\n",
    "    # Specifically, it tells you whether the residuals (or errors) across different variables are correlated with each other.\n",
    "\n",
    "    if r == q:\n",
    "        Q[:r, :r] = H # H is the covariance matrix of residuals, and the top-left r x r block of Q is now populated with the values from H. The rest of Q remains zero.\n",
    "        # r=q, there is no need to perform eigenvalue decomposition to extract specific components because we are considering the entire variance structure of H.\n",
    "        # The matrix Q is directly updated with H, as H already fully represents the covariance structure.\n",
    "    else:\n",
    "        eigvals_H, eigvecs_H = eig(H)\n",
    "        idx = eigvals_H.argsort()[::-1][:q]  # Sorts the eigenvalues in descending order ([::-1]) and selects the top q eigenvalues.\n",
    "        eigvals_H, eigvecs_H = eigvals_H[idx], eigvecs_H[:, idx] # Updates the eigenvalues and eigenvectors arrays to keep only the top q eigenvalues and their corresponding eigenvectors.\n",
    "        Q[:r, :r] = eigvecs_H @ np.diag(eigvals_H) @ eigvecs_H.T # Updates the top-left r x r block of Q by replacing it with a matrix computed from the eigenvectors and eigenvalues.\n",
    "\n",
    "    # 1) np.diag(eigvals_H) : Creates a diagonal matrix from the selected eigenvalues.\n",
    "    # 2) eigvecs_H @ np.diag(eigvals_H) @ eigvecs_H.T : Forms the diagonalized covariance matrix using the top q eigenvalues and corresponding eigenvectors.\n",
    "    # If q < r, we only want to retain a reduced-rank approximation of H that captures the most important variance information (top q eigenvalues and their corresponding eigenvectors).\n",
    "    # The covariance matrix HH may contain noise or redundant information in its lower-ranked eigenvalues and eigenvectors.\n",
    "    # By retaining only the top q components, we simplify H while preserving the most significant variance contributions.\n",
    "\n",
    "    # Initialize Kalman filter parameters\n",
    "    initx = F[0, :]\n",
    "    # Selects the first row of F, corresponding to the latent representation of the first time step in the reduced space.\n",
    "    # initx serves as the initial state for the system in the latent r-dimensional space.\n",
    "\n",
    "    kron_A = np.kron(A, A)\n",
    "    # The Kronecker product models pairwise interactions between the coefficients of A.\n",
    "    # Each element of A is expanded into a block of size r×(r⋅p), which reflects all possible combinations of coefficients in A.\n",
    "    initV = pinv(np.eye(r * p**2) - kron_A) @ Q.flatten(order='F')\n",
    "    # 1) Subtracting kron_A from the identity matrix representing a transformation or propagation operator in the context of covariance dynamics\n",
    "    # 2) The Moore-Penrose pseudoinverse is computed for the matrix\n",
    "    # 3) Reshaping Q into a 1D array by stacking its columns sequentially (column-major order, Fortran-style). Why?\n",
    "    # The vectorized form of Q is required because the system equation, after vectorization, operates on 1D arrays rather than 2D matrices.\n",
    "    # Specifically, the vectorized form of the Lyapunov equation for the steady-state covariance matrix Σ (initV),\n",
    "    # which encodes how noise from Q propagates through the dynamics of the system, represented by A, to create long-run variances and covariances of the system's state variables.\n",
    "    initV = initV.reshape((r * p, r * p), order='F')\n",
    "    #  steady-state covariance matrix of the VAR system's state vector, capturing variances and covariances of all variables and their lags in the long run,\n",
    "    # reshaped from vectorized to matrix form. Matrix of dimensions r*p x r*p\n",
    "\n",
    "    C = np.hstack((eigvecs, np.zeros((N, r * nlag))))\n",
    "    # This code constructs a matrix C by horizontally stacking two components:\n",
    "    # 1) eigvecs: a matrix of size N×rN×r, containing eigenvectors derived earlier (likely capturing the system's dominant modes or components)\n",
    "    # 2) np.zeros((N, r * nlag)): a matrix of zeros of size N×(r⋅nlag), added as padding.\n",
    "    # C therefore combines the contributions from the current state and the lagged terms, with the lagged terms initially set to zero.\n",
    "\n",
    "    return {\n",
    "        \"A\": A, \"C\": C, \"Q\": Q, \"R\": R, \"initx\": initx, \"initV\": initV\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "XvklZseEH0P7"
   },
   "id": "XvklZseEH0P7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Factor extraction (main); requires RicSW and Kalman smoother diag"
   ],
   "metadata": {
    "id": "o_U46RZeah77"
   },
   "id": "o_U46RZeah77"
  },
  {
   "cell_type": "code",
   "source": [
    "# %run kalman_update_diag.py\n",
    "# %run kalman_filter_diag.py\n",
    "# %run smooth_update.py\n",
    "# %run kalman_smoother_diag.py\n",
    "# %run ricSW.py\n",
    "\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "\n",
    "\n",
    "# Integrate kalman_smoother_diag into the factor_extraction function\n",
    "# from ricSW import ricSW\n",
    "# from kalman_smoother_diag import kalman_smoother_diag\n",
    "def factor_extraction(standardized_df, q, r, p):\n",
    "    \"\"\"\n",
    "    Extracts common factors from a balanced, standardized dataset.\n",
    "    \"\"\"\n",
    "    if r < q:\n",
    "        raise ValueError(\"q must be less than or equal to r.\")\n",
    "    if p < 1:\n",
    "        raise ValueError(\"p must be greater than or equal to 1.\")\n",
    "\n",
    "    # Step 1: Use ricSW to estimate parameters\n",
    "    result_ricsw = ricSW(standardized_df, q, r, p)\n",
    "    A, C, Q, R = result_ricsw[\"A\"], result_ricsw[\"C\"], result_ricsw[\"Q\"], result_ricsw[\"R\"]\n",
    "    initx, initV = result_ricsw[\"initx\"], result_ricsw[\"initV\"]\n",
    "\n",
    "    # Step 2: Kalman filter initializations\n",
    "    T = standardized_df.shape[0]\n",
    "    AA = np.repeat(A[:, :, np.newaxis], T, axis=2)\n",
    "    QQ = np.repeat(Q[:, :, np.newaxis], T, axis=2)\n",
    "    CC = np.repeat(C[:, :, np.newaxis], T, axis=2)\n",
    "    RR = np.repeat(R[:, :, np.newaxis], T, axis=2)\n",
    "\n",
    "    # Handle missing data by assigning large noise variance to missing values\n",
    "    for jt in range(T):\n",
    "        miss = np.isnan(standardized_df.iloc[jt, :].values)\n",
    "        Rtemp = np.diag(R)\n",
    "        Rtemp[miss] = 1e+32\n",
    "        RR[:, :, jt] = np.diag(Rtemp)\n",
    "\n",
    "    xx = standardized_df.values\n",
    "    xx[np.isnan(xx)] = 0\n",
    "\n",
    "    # Step 3: Run the Kalman smoother\n",
    "    model = range(1, T+1)\n",
    "    smoother_result = kalman_smoother_diag(xx.T, AA, CC, QQ, RR, initx, initV, model)\n",
    "\n",
    "    F = smoother_result[\"xsmooth\"].T\n",
    "    VF = smoother_result[\"Vsmooth\"]\n",
    "\n",
    "    return {\"F\": F, \"VF\": VF, \"A\": A, \"C\": C, \"Q\": Q, \"R\": R, \"initx\": initx, \"initV\": initV}\n",
    "\n",
    "\n",
    "standardized_df.info()\n",
    "\n"
   ],
   "metadata": {
    "id": "glNFww0iahK_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732115398379,
     "user_tz": -60,
     "elapsed": 12,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dfd1e637-86a4-4fd2-eaf0-0b7119054ad8"
   },
   "id": "glNFww0iahK_",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 379 entries, 1992-03-01 to 2024-09-01\n",
      "Columns: 118 entries, RPI to SBCACBW027SBOG\n",
      "dtypes: float64(118)\n",
      "memory usage: 352.4 KB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### End session and restart kernel"
   ],
   "metadata": {
    "id": "ROzLH-nF_3er"
   },
   "id": "ROzLH-nF_3er"
  },
  {
   "cell_type": "code",
   "source": [
    "# run_cell = False  # Change to True to run this block\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# cwd = os.getcwd()\n",
    "# for filename in os.listdir(cwd):\n",
    "#     file_path = os.path.join(cwd, filename)\n",
    "#     try:\n",
    "#         if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "#             os.unlink(file_path)  # Remove file or link\n",
    "#         elif os.path.isdir(file_path):\n",
    "#             shutil.rmtree(file_path)  # Remove directory\n",
    "#     except Exception as e:\n",
    "#         print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "\n",
    "# !pwd\n",
    "\n"
   ],
   "metadata": {
    "id": "E6q1lT6u_xaV"
   },
   "id": "E6q1lT6u_xaV",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1-vXxhICvRcCm1JjNONfXkYdcYyuQnEzX",
     "timestamp": 1733177969354
    },
    {
     "file_id": "1E-gduTxNvMjHxhAeYurS8mPzUaWWaOjx",
     "timestamp": 1732284938951
    },
    {
     "file_id": "1HBSn1-UhCmbdgf7DyLsikbmYva2NAxwM",
     "timestamp": 1732015356261
    },
    {
     "file_id": "1jZmM96LVYf4oX_0o6l2xireXhSPf7ZEE",
     "timestamp": 1731868777236
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
