{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sZNz3KerUSFSCbEz4yS85kN2pzS8n1mr",
     "timestamp": 1733857386202
    }
   ],
   "authorship_tag": "ABX9TyM+wGCKp0StmmnjDau4Ez/C"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/drive/My Drive/Colab Notebooks/standardized_compact_dataset.csv'\n",
    "data = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FofVe2FPm8iY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733844830963,
     "user_tz": -60,
     "elapsed": 23636,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "e0d85694-9d8b-4566-d2a2-9ac6685bbb4c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoPSFzL-fQCh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733844914846,
     "user_tz": -60,
     "elapsed": 283,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "8d442613-a61d-49b4-cbf1-a30b8cd593d8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2,)\n",
      "[ 2.83020381 -0.77517111]\n",
      "Steady-State Covariance Matrix V:\n",
      "[[13.31039179  0.75266111]\n",
      " [ 0.75266111 11.53040787]]\n"
     ]
    }
   ],
   "source": [
    "### Testing code\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import eig, pinv, block_diag\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Filter the DataFrame for dates up to December 1999 to obtain the vintages\n",
    "filtered_data = data.loc[:'1999-12-31']\n",
    "\n",
    "# Extract the values as a compact dataset\n",
    "x = filtered_data.values\n",
    "\n",
    "T, N = x.shape\n",
    "print(x.shape) # 449x118 matrix\n",
    "print(x)\n",
    "\n",
    "test = x[0, :]\n",
    "print(test)\n",
    "\n",
    "# Output the values of T, N, and the first rows of x\n",
    "print(\"T (Number of rows):\", T)\n",
    "print(\"N (Number of columns):\", N)\n",
    "print(\"First 5 rows and 5 columns of x:\\n\", x[:5, :5])\n",
    "\n",
    "r = 2\n",
    "q = 2\n",
    "p = 1\n",
    "nlag = p - 1\n",
    "\n",
    "##########################\n",
    "\n",
    "cov_x = np.cov(x, rowvar=False) # covariance matrix of the vintage\n",
    "print(\"Size (number of elements):\", cov_x.size)\n",
    "print(\"Shape (dimensions):\", cov_x.shape)\n",
    "print(\"Number of dimensions:\", cov_x.ndim)\n",
    "print(cov_x[:3])\n",
    "eigvals, eigvecs = eig(cov_x)\n",
    "# Print eigenvalues\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigvals)\n",
    "print(\"Size (number of elements):\", eigvals.size)\n",
    "print(\"Shape (dimensions):\", eigvals.shape)\n",
    "print(\"Number of dimensions:\", eigvals.ndim)\n",
    "\n",
    "# Print eigenvectors\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigvecs)\n",
    "print(\"Size (number of elements):\", eigvecs.shape)\n",
    "\n",
    "\n",
    "idx = eigvals.argsort()[::-1]\n",
    "print(idx)\n",
    "\n",
    "eigvals, eigvecs = eigvals[idx][:r], eigvecs[:, idx][:, :r]\n",
    "print(eigvals.shape)\n",
    "print(eigvecs.shape)\n",
    "\n",
    "F = x @ eigvecs # linear transformation of the original data into a new space defined by the eigenvectors\n",
    "print(F.size)\n",
    "print(F.shape) # a 449 x 2 matrix\n",
    "print(F.shape)\n",
    "print(F)\n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "## Factor loadings ##\n",
    "\n",
    "C = eigvecs[:, :r]  # Take the first r columns of eigvecs to get the factor loadings for the top r factors\n",
    "\n",
    "print(C) # factor loadings matrix\n",
    "print(C.shape)\n",
    "C_transposed = C.T\n",
    "print(C_transposed)\n",
    "print(C_transposed.shape)\n",
    "\n",
    "## Formula: x = F * C transposed + e\n",
    "#########################\n",
    "\n",
    "R = np.diag(np.diag(np.cov(x - F @ eigvecs.T, rowvar=False)))\n",
    "# computes the covariance matrix of the residuals\n",
    "# (the difference between the original data and the transformation using eigenvectors,\n",
    "# then extracts its diagonal elements, then builds a matrix having on its diagonal the elements extracted and zeroes elsewhere.\n",
    "# R: covariance matrix of the idiosyncratic (residual) errors that are specific to the observed data\n",
    "\n",
    "print(R)\n",
    "print(R.shape) # 118 x 118 matrix\n",
    "\n",
    "####################################\n",
    "\n",
    "# VAR model estimation\n",
    "\n",
    "####################################\n",
    "\n",
    "A_temp = np.zeros((r, r * p)) # Creates a temporary matrix filled with zeros of dimension r x r*p\n",
    "I = np.eye(r * p) # Creates an indentiy matrix of size r*p x r*p\n",
    "\n",
    "print(type(A_temp))\n",
    "print(\"Size (number of elements):\", A_temp.size)\n",
    "print(\"Shape (dimensions):\", A_temp.shape)\n",
    "print(\"Number of dimensions:\", A_temp.ndim)\n",
    "print(A_temp[:2])\n",
    "\n",
    "print(I.shape)\n",
    "LL = I.shape[0]\n",
    "if p != 1:\n",
    "    A = np.vstack((A_temp.T, I[:LL-r, :]))  # Equivalent to rbind(A_temp, I[1:(LL-r), ])\n",
    "else:\n",
    "    A = np.vstack((A_temp.T, np.empty((0, r * p))))  # Equivalent to rbind(t(A_temp), I[0, ]) # Modified due to different indexing between R and Python\n",
    "\n",
    "\n",
    "print(\"Size (number of elements):\", A.size)\n",
    "print(\"Shape (dimensions):\", A.shape)\n",
    "print(\"Number of dimensions:\", A.ndim)\n",
    "print(A[:3]) # 2 x 2 mtrix of zeros\n",
    "\n",
    "Q = np.zeros((r * p, r * p)) # 2 x 2 identiy matrix. Q: covariance matrix of process noise\n",
    "Q[:r, :r] = np.eye(r)\n",
    "print(\"Size (number of elements):\", Q.size)\n",
    "print(\"Shape (dimensions):\", Q.shape)\n",
    "print(\"Number of dimensions:\", Q.ndim)\n",
    "print(Q[:3])\n",
    "\n",
    "####################################\n",
    "\n",
    "### Estimation of the matrix of autoregressive coefficients ###\n",
    "\n",
    "# Example data: F is the matrix of factors with shape (T, r)\n",
    "T, r = F.shape  # T: number of time intervals, r: number of factors\n",
    "\n",
    "# Step 1: Prepare lagged matrices\n",
    "Z = F[:-1, :]  # Factors excluding the last observation (shape: (T-1, r))\n",
    "z = F[1:, :]   # Factors excluding the first observation (shape: (T-1, r))\n",
    "\n",
    "# Step 2: Estimate VAR(1) coefficient matrix A\n",
    "A = np.linalg.inv(Z.T @ Z) @ Z.T @ z\n",
    "print(\"Estimated VAR(1) coefficient matrix A:\")\n",
    "print(A)\n",
    "\n",
    "# Step 3: Estimate residuals and covariance matrix H\n",
    "residuals = z - Z @ A\n",
    "H = np.cov(residuals, rowvar=False)\n",
    "print(\"Residuals covariance matrix H:\")\n",
    "print(H)\n",
    "\n",
    "# Step 4: Validate the model (check eigenvalues of A)\n",
    "eigvals = np.linalg.eigvals(A)\n",
    "print(\"Eigenvalues of A:\")\n",
    "print(eigvals)\n",
    "\n",
    "if np.all(np.abs(eigvals) < 1):\n",
    "    print(\"The VAR(1) process is stable.\")\n",
    "else:\n",
    "    print(\"The VAR(1) process is unstable; consider alternative models.\")\n",
    "\n",
    "####################################\n",
    "### Alternative method to estimate the matrix of autoregressive coefficients ###\n",
    "\n",
    "Z = F[:-1, :] # slicing, all rows except the last row\n",
    "z = F[1:, :] # slicing, start from the second row and include all subsequent rows\n",
    "print(Z.shape)\n",
    "print(z.shape)\n",
    "print(Z)\n",
    "print(z)\n",
    "\n",
    "\n",
    "A_temp = inv(Z.T @ Z) @ Z.T @ z # matrix of Ordinary Least Squares (OLS) estimators used to explain z based on Z\n",
    "# A[:r, :r * p] = A_temp.T # assigning a portion of A_temp.T to the first part of A ### investigate this line of code ###\n",
    "print(A_temp)\n",
    "print(A)\n",
    "\n",
    "###############\n",
    "\n",
    "e = z - Z @ A_temp  # VAR residuals\n",
    "print(e.shape)\n",
    "print(e)\n",
    "H = np.cov(e, rowvar=False) # covariance matrix of the residuals\n",
    "print(H.shape)\n",
    "print(H)\n",
    "\n",
    "Q[:r, :r] = H # covariance matrix of the process noise in the state-space model (no difference from H when r = 2)\n",
    "print(Q.shape)\n",
    "print(Q)\n",
    "##### Alternative method ends here ###\n",
    "\n",
    "#### Restart running code from here ####\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "# initx = F[0, :] # replaced with new code below\n",
    "\n",
    "### Initialization of the latent factors using the last estimated factors\n",
    "initx = F[-1, :]\n",
    "print(initx.shape)\n",
    "print(initx)\n",
    "\n",
    "## Initialization of the Steady-State Covariance Matrix\n",
    "## The steady-state covariance matrix (VV) represents the long-run variance of the state vector under the VAR(1) process\n",
    "## For a VAR(1) process: Ft=AFt−1+ϵt,ϵt∼N(0,Q) Ft​=AFt−1​+ϵt​,ϵt​∼N(0,Q)\n",
    "## The steady-state covariance matrix VV satisfies the Lyapunov equation: V=AVAT+Q\n",
    "\n",
    "\n",
    "from scipy.linalg import solve_discrete_lyapunov\n",
    "\n",
    "# Solve the Lyapunov equation for the steady-state covariance matrix\n",
    "V = solve_discrete_lyapunov(A, H)\n",
    "\n",
    "print(\"Steady-State Covariance Matrix V:\")\n",
    "print(V)\n",
    "\n",
    "####### The Lyapunov equation \"behind the scenes\" (do not run) ###\n",
    "\n",
    "kron_A = np.kron(A, A)\n",
    "print(kron_A.shape)\n",
    "print(kron_A)\n",
    "\n",
    "Q_flatten = Q.flatten(order='F').reshape(-1, 1)\n",
    "print(Q_flatten.shape)\n",
    "\n",
    "diag_matrix = np.eye(kron_A.shape[0])\n",
    "\n",
    "initV = pinv(diag_matrix - kron_A) @ Q.flatten(order='F').reshape(-1, 1)\n",
    "print(initV.shape)\n",
    "initV = initV.reshape((r * p, r * p), order='F')\n",
    "print(initV)\n",
    "print(initV.shape)\n",
    "\n",
    "### end of the custom Lyapunov method ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import eig, pinv, block_diag\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def ricSW(standardized_df, q, r, p, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Computes parameters for a factor model using standardized data.\n",
    "\n",
    "    Parameters:\n",
    "    standardized_df : pd.DataFrame\n",
    "        Standardized and balanced panel data (with date as index).\n",
    "    q : int\n",
    "        Rank for reduced Q covariance matrix (if applicable).\n",
    "    r : int\n",
    "        Number of factors.\n",
    "    p : int\n",
    "        Lag order for VAR.\n",
    "    start_date : str\n",
    "        Start date for data selection (e.g., '1962-09-01').\n",
    "    end_date : str\n",
    "        End date for data selection (e.g., '1999-12-01').\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        A dictionary containing factor model parameters.\n",
    "    \"\"\"\n",
    "    # Filter the data based on the given date range\n",
    "    standardized_df = data.loc[start_date:end_date]\n",
    "\n",
    "    # Convert the DataFrame to a NumPy array for numerical operations\n",
    "    x = standardized_df.values  # Assuming standardized_df is a DataFrame\n",
    "    T, N = x.shape  # T: number of rows (time periods), N: number of columns (features)\n",
    "    nlag = p - 1  # Order of lags in the VAR model for the factors. Typically zero if p=1 (number of additional lags beyond t-1)\n",
    "\n",
    "    # Compute covariance matrix of the data\n",
    "    cov_x = np.cov(x, rowvar=False)  # Computing the covariance of the data in x. Rowvar=False treats columns as variables\n",
    "\n",
    "    # Perform eigendecomposition of the covariance matrix\n",
    "    eigvals, eigvecs = eig(cov_x)\n",
    "    idx = eigvals.argsort()[::-1]  # Sort eigenvalues in descending order\n",
    "    eigvals, eigvecs = eigvals[idx][:r], eigvecs[:, idx][:, :r]  # Select the top r eigenvalues and eigenvectors\n",
    "\n",
    "    # Compute the principal components (factor estimates)\n",
    "    F = x @ eigvecs  # Transforms the original data x into a new space defined by the eigenvectors\n",
    "\n",
    "    # Estimate the covariance matrix of the idiosyncratic component (R)\n",
    "    R = np.diag(np.diag(np.cov(x - F @ eigvecs.T, rowvar=False)))\n",
    "\n",
    "    # For VAR(1), we just need the first lag: F_{t-1}\n",
    "    if p == 1:\n",
    "        Z = F[:-1, :]  # Lagged values: F_{t-1}\n",
    "        z = F[1:, :]  # Current values: F_t\n",
    "    else:\n",
    "        # For VAR(p), we need p lags: F_{t-1}, F_{t-2}, ..., F_{t-p}\n",
    "        Z = np.hstack([F[p - kk - 1:-(kk + 1), :] for kk in range(p)])  # Stack lags F_{t-1}, F_{t-2}, ..., F_{t-p}\n",
    "        z = F[p:, :]  # Current values: F_t\n",
    "\n",
    "    # Estimate the VAR coefficients using OLS\n",
    "    A_temp = inv(Z.T @ Z) @ Z.T @ z  # Ordinary least squares estimation for VAR(1)\n",
    "    A = np.zeros((r, r * p))  # Initialize the A matrix\n",
    "    A[:r, :r * p] = A_temp.T  # Store the estimated coefficients in the A matrix\n",
    "\n",
    "    # Compute the covariance matrix of the residuals (idiosyncratic errors)\n",
    "    e = z - Z @ A_temp  # VAR residuals\n",
    "    H = np.cov(e, rowvar=False)  # Covariance matrix of the residuals\n",
    "\n",
    "    # If r == q, we directly assign H to Q, otherwise, we reduce the rank of Q\n",
    "    Q = np.zeros((r * p, r * p))  # Initialize Q as a zero matrix\n",
    "    if r == q:\n",
    "        Q[:r, :r] = H  # Use the covariance matrix H if rank r equals q\n",
    "    else:\n",
    "        eigvals_H, eigvecs_H = eig(H)  # Eigenvalue decomposition of the residual covariance\n",
    "        idx = eigvals_H.argsort()[::-1][:q]  # Select the top q eigenvalues\n",
    "        eigvals_H, eigvecs_H = eigvals_H[idx], eigvecs_H[:, idx]\n",
    "        Q[:r, :r] = eigvecs_H @ np.diag(eigvals_H) @ eigvecs_H.T  # Update the covariance matrix Q\n",
    "\n",
    "    # Initialize Kalman filter parameters\n",
    "\n",
    "    initx = F[0, :]  # Initial state vector based on the first observation\n",
    "    kron_A = np.kron(A, A)  # Kronecker product of A for Kalman filter initialization\n",
    "    diag_matrix = np.eye(kron_A.shape[0])\n",
    "    initV = pinv(diag_matrix - kron_A) @ Q.flatten(order='F').reshape(-1, 1)  # Steady-state covariance of the system\n",
    "    initV = initV.reshape((r * p, r * p), order='F')  # Reshape to matrix form\n",
    "\n",
    "    # Create the matrix C for the measurement equation\n",
    "    C = np.hstack((eigvecs, np.zeros((N, r * nlag))))  # Stack eigenvectors with zeros for lagged terms\n",
    "\n",
    "    return {\n",
    "        \"A\": A, \"C\": C, \"Q\": Q, \"R\": R, \"initx\": initx, \"initV\": initV\n",
    "    }\n",
    "\n",
    "result = ricSW(data, q=2, r=2, p=1, start_date='1962-09-01', end_date='1999-12-01')\n",
    "\n",
    "# Print the entire dictionary of results\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(value)\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "id": "cXv4Gkhm5wDA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q represents the covariance matrix of the process noise in the state-space model. It defines the uncertainty in the evolution of the latent factors over time. In state-space models, the latent state (in this case, the factors) evolves according to some dynamics, and Q captures how much uncertainty or randomness there is in the evolution of these factors. Essentially, it measures the \"noise\" in the factor dynamics.\n",
    "\n",
    "Structure: The matrix Q is initialized as a block matrix with the top-left r x r block set to the identity matrix (np.eye(r)), which ensures that the factors have independent unit variance at the start. The rest of the matrix (Q[r:, r:]) is zero. Q is updated after estimating the residuals (the errors between the actual and predicted values of the factors) from the VAR model. This updated Q is typically used to describe the variance of the residuals that cannot be explained by the model.\n",
    "\n",
    "Dimensions: Q has size (r * p, r * p), where: r is the number of factors, p is the lag order in the VAR model.\n",
    "\n",
    "Interpretation in the model: The diagonal block of Q that corresponds to the factors (Q[:r, :r]) describes the variance of the factors. This tells us how much uncertainty exists in the factor process itself, while the off-diagonal blocks (which are zero in your initialization) would typically describe any cross-covariance terms if factors are related to each other."
   ],
   "metadata": {
    "id": "G_fDG4P891mH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "C represents the factor loadings matrix that links the observed data to the underlying factors in the model. In a factor model, the observed variables are typically assumed to be linear combinations of a smaller number of latent (unobserved) factors. The factor loadings matrix C maps the factors to the observed variables.\n",
    "\n",
    "Structure: The matrix C is constructed by horizontally stacking the eigenvectors of the covariance matrix of the observed data (eigvecs) and a matrix of zeros. The eigenvectors represent the directions in the data space that explain the most variance. The zeros in the matrix indicate that there are no contributions from lagged values of the factors initially.\n",
    "\n",
    "Dimensions: The size of C is (N, r * nlag), where: N is the number of observations (or variables), r is the number of factors, nlag is the number of lags considered (e.g., p - 1 in the model)."
   ],
   "metadata": {
    "id": "Dpwv566T-UOg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "R represents the covariance matrix of the idiosyncratic (residual) errors that are specific to the observed data. In factor models, after explaining the observed variables by the underlying factors, the remaining unexplained part is captured as the idiosyncratic error. This error is specific to each observed variable and cannot be explained by the common factors.\n",
    "\n",
    "Structure: The matrix R is computed by first determining the residuals of the factor model. This is done by calculating the covariance between the observed data and the factors and then subtracting the explained part (which is the projection onto the factors). The matrix R reflects the variance (or covariance) of these residuals, which is typically diagonal if the residuals are uncorrelated across variables.\n",
    "\n",
    "Dimensions: R has size (N, N) (for the number of observations), but it is typically assumed to be diagonal in many factor models, where the diagonal elements represent the variance of the idiosyncratic error for each observed variable.\n",
    "\n",
    "Interpretation in the model: R captures the noise or error in the observed data that is not explained by the common factors. If the residuals are small (low R values), it means the factors explain most of the variance in the observed data, while larger values of R indicate that a significant portion of the data’s variance remains unexplained by the factors."
   ],
   "metadata": {
    "id": "ipBwN-Rz-dwQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "initx represents the initial values of the latent factors (the unobserved components) at time t=0. In the state-space formulation of your factor model, the factors evolve over time according to a dynamic process (often a VAR model, as in your case). The vector initx is the starting point of the latent factors, reflecting their values at the beginning of the process.\n",
    "\n",
    "Structure: initx is set to the first row of the factor matrix F, i.e., F[0, :]. F is the matrix of principal components (factors) that are computed by projecting the standardized data onto the eigenvectors of the data’s covariance matrix.\n",
    "\n",
    "Therefore, initx is essentially the first set of values for the factors based on the data at the start of the series.\n",
    "\n",
    "Dimensions: initx is a vector with dimensions (r,), where r is the number of factors in your model. It contains the initial state of the latent factors.\n",
    "\n",
    "Role in the Model: initx provides the starting point for the Kalman filter or any other state estimation process you're using. This initial state vector is needed for making predictions and updating the state of the model as new data comes in. It determines the \"initial belief\" about the value of the latent factors before observing any new data."
   ],
   "metadata": {
    "id": "k-B3i64WEwVJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "initV represents the initial covariance matrix of the latent factors' state vector at the start of the time series. It encapsulates the uncertainty about the initial state of the factors. This matrix tells us how much variance there is in the initial values of the factors and how they might be correlated with each other.\n",
    "\n",
    "Structure: initV is derived from the Kronecker product of the matrix A (the transition matrix from the VAR model) and the identity matrix, then adjusted using the covariance matrix Q. In the line initV = pinv(diag_matrix - kron_A) @ Q.flatten(order='F').reshape(-1, 1), the initial covariance matrix is calculated through a process involving the transition matrix A, the covariance matrix Q, and the inverse of the transformation operator (represented by the Kronecker product). pinv(diag_matrix - kron_A) uses the Moore-Penrose pseudoinverse to handle any potential singularities in the matrix and compute a stable estimate of the covariance structure.\n",
    "\n",
    "Dimensions: initV has dimensions (r * p, r * p), where: r is the number of factors (latent variables), p is the lag order of the VAR model. r * p reflects the size of the state vector that incorporates both the factors and their lags.\n",
    "\n",
    "Role in the Model: initV is crucial for filtering and smoothing in state-space models (such as the Kalman filter). It reflects the uncertainty about the initial state of the system and is used to update beliefs about the factors as new data arrives. Specifically, it describes how the errors or shocks to the system propagate and evolve over time."
   ],
   "metadata": {
    "id": "8qqFAkFlE8RI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#### Comparison using packages ####\n",
    "\n",
    "q = 2  # Rank for reduced Q covariance matrix\n",
    "r = 2  # Number of factors\n",
    "p = 1  # Lag order for VAR (1)\n",
    "start_date = '1962-09-01'\n",
    "end_date = '1999-12-01'\n",
    "\n",
    "### Estimation Using statsmodels (for VAR) ###\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Select the data from the relevant date range\n",
    "df_subset = data.loc[start_date:end_date]\n",
    "\n",
    "# Fit a VAR model to the data\n",
    "model = VAR(df_subset)\n",
    "var_results = model.fit(1)  # Fit with 1 lag (p=1)\n",
    "\n",
    "# Extract the VAR coefficients and residual covariance\n",
    "A_statsmodels = var_results.coefs[0].T  # Coefficients for the VAR(1) model\n",
    "H = var_results.sigma_u  # Residual covariance matrix from VAR model\n",
    "\n",
    "print(A_statsmodels)\n",
    "print(H)\n",
    "\n",
    "### Estimation Using Kalman Filter ###\n",
    "\n",
    "### Do not execute! #################\n",
    "\n",
    "######################################\n",
    "\n",
    "###\n",
    "!pip install pykalman\n",
    "\n",
    "from pykalman import KalmanFilter\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of factors (latent states) r\n",
    "r = 2  # This could be set based on your model specification\n",
    "\n",
    "# Initialize Kalman Filter\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=A_statsmodels,  # VAR coefficients as state transition matrix\n",
    "    observation_matrices=np.eye(df_subset.shape[1]),  # Assuming identity observation matrix (for simplicity)\n",
    "    initial_state_mean=np.zeros(r),  # Initial state vector (zeros or first observed values)\n",
    "    initial_state_covariance=np.eye(r) * 1e-2,  # Initial state covariance (small diagonal matrix)\n",
    "    em_vars=['transition_covariance', 'observation_covariance']  # Allow Kalman Filter to estimate covariance matrices\n",
    ")\n",
    "\n",
    "# Estimate the latent factors (state means) and covariance (state covariances)\n",
    "state_means, state_covariances = kf.filter(df_subset.values)  # Pass the observed data (df_subset.values)\n",
    "\n",
    "# The latent factors (F) are the state estimates\n",
    "F_kf = state_means  # These are the estimated latent factors\n",
    "\n",
    "# Print the estimated latent factors\n",
    "print(\"Estimated Latent Factors (F_kf):\")\n",
    "print(F_kf)\n",
    "\n",
    "# If needed, you can also extract the Kalman gain and other details\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOhHqnVm982W",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733587262580,
     "user_tz": -60,
     "elapsed": 3360,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "dbb44477-3199-4b7b-b0ee-02db569fd97f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-4.02797136e-02  2.91916104e-01  6.40275736e-02 ... -4.63337081e-02\n",
      "  -1.36642932e-01 -4.76664897e-04]\n",
      " [ 3.79043371e-02 -3.22036540e-01 -1.26183789e-01 ... -1.14247676e-01\n",
      "  -6.25098323e-02  5.80736832e-05]\n",
      " [ 3.38290055e-02  6.49732508e-02 -2.43971174e-01 ...  2.14891030e-02\n",
      "  -7.74058417e-02 -3.61398628e-05]\n",
      " ...\n",
      " [-3.89087917e-02 -4.41082735e-02 -4.52515713e-02 ...  2.67853897e-01\n",
      "  -2.86284945e-02 -4.66710105e-05]\n",
      " [ 1.77327172e-02  4.73935675e-02 -2.86645183e-02 ...  5.61428885e-02\n",
      "  -2.23102409e-01  9.81949681e-05]\n",
      " [-2.34843510e+01 -3.13948626e+01 -1.85267272e+01 ... -1.14280836e+00\n",
      "   3.46492828e+00  5.84036998e-02]]\n",
      "                      RPI   W875RX1  DPCERA3M086SBEA  CMRMTSPLx   RETAILx  \\\n",
      "RPI              0.143856  0.259613         0.073947   0.048651  0.054780   \n",
      "W875RX1          0.259613  0.590359         0.134343   0.107775  0.107851   \n",
      "DPCERA3M086SBEA  0.073947  0.134343         0.433386   0.241867  0.350261   \n",
      "CMRMTSPLx        0.048651  0.107775         0.241867   0.654379  0.318368   \n",
      "RETAILx          0.054780  0.107851         0.350261   0.318368  0.592697   \n",
      "...                   ...       ...              ...        ...       ...   \n",
      "DTCOLNVHFNM     -0.020760 -0.016561         0.050399   0.021020  0.119516   \n",
      "DTCTHFNM         0.002876  0.044117         0.041423   0.033827  0.076252   \n",
      "INVEST           0.027785  0.055925         0.014760   0.029034  0.023683   \n",
      "VIXCLSx         -0.012952 -0.018109        -0.011621  -0.032639  0.004458   \n",
      "BORROW           0.000016  0.000002         0.000049  -0.000014  0.000022   \n",
      "\n",
      "                   INDPRO   IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  ...  \\\n",
      "RPI              0.057884  0.057897  0.056064  0.057036  0.040115  ...   \n",
      "W875RX1          0.132825  0.127666  0.118740  0.109639  0.090311  ...   \n",
      "DPCERA3M086SBEA  0.107690  0.098400  0.097261  0.106737  0.063470  ...   \n",
      "CMRMTSPLx        0.256436  0.230326  0.225473  0.234761  0.200170  ...   \n",
      "RETAILx          0.130565  0.108731  0.102302  0.124801  0.094881  ...   \n",
      "...                   ...       ...       ...       ...       ...  ...   \n",
      "DTCOLNVHFNM     -0.033161 -0.053606 -0.053233 -0.041071 -0.026387  ...   \n",
      "DTCTHFNM        -0.014435 -0.024048 -0.026805 -0.028557 -0.005170  ...   \n",
      "INVEST           0.051279  0.047746  0.049717  0.053790  0.044707  ...   \n",
      "VIXCLSx          0.057228  0.035866  0.043537  0.051999  0.034087  ...   \n",
      "BORROW           0.000007  0.000005  0.000007  0.000002  0.000039  ...   \n",
      "\n",
      "                 DNDGRG3M086SBEA  DSERRG3M086SBEA  CES0600000008  \\\n",
      "RPI                    -0.013789        -0.032897       0.012652   \n",
      "W875RX1                -0.033668        -0.059463       0.071912   \n",
      "DPCERA3M086SBEA        -0.037340        -0.023041       0.024272   \n",
      "CMRMTSPLx              -0.020022         0.008178       0.038211   \n",
      "RETAILx                 0.006567        -0.006076       0.008644   \n",
      "...                          ...              ...            ...   \n",
      "DTCOLNVHFNM            -0.003344         0.037450       0.019648   \n",
      "DTCTHFNM               -0.032463         0.004758       0.020178   \n",
      "INVEST                  0.005915        -0.012353      -0.015552   \n",
      "VIXCLSx                 0.043155         0.058069       0.001602   \n",
      "BORROW                 -0.000016         0.000031       0.000114   \n",
      "\n",
      "                 CES2000000008  CES3000000008  DTCOLNVHFNM  DTCTHFNM  \\\n",
      "RPI                  -0.017016       0.009333    -0.020760  0.002876   \n",
      "W875RX1              -0.026589       0.075394    -0.016561  0.044117   \n",
      "DPCERA3M086SBEA      -0.031930       0.080599     0.050399  0.041423   \n",
      "CMRMTSPLx            -0.081391       0.104060     0.021020  0.033827   \n",
      "RETAILx              -0.071891       0.086891     0.119516  0.076252   \n",
      "...                        ...            ...          ...       ...   \n",
      "DTCOLNVHFNM          -0.063036       0.041031     0.729855  0.210214   \n",
      "DTCTHFNM             -0.004582       0.021923     0.210214  0.259520   \n",
      "INVEST                0.008021      -0.039459    -0.036550 -0.004674   \n",
      "VIXCLSx              -0.033379       0.015076     0.062827  0.012675   \n",
      "BORROW               -0.000015       0.000156     0.000042  0.000004   \n",
      "\n",
      "                   INVEST   VIXCLSx    BORROW  \n",
      "RPI              0.027785 -0.012952  0.000016  \n",
      "W875RX1          0.055925 -0.018109  0.000002  \n",
      "DPCERA3M086SBEA  0.014760 -0.011621  0.000049  \n",
      "CMRMTSPLx        0.029034 -0.032639 -0.000014  \n",
      "RETAILx          0.023683  0.004458  0.000022  \n",
      "...                   ...       ...       ...  \n",
      "DTCOLNVHFNM     -0.036550  0.062827  0.000042  \n",
      "DTCTHFNM        -0.004674  0.012675  0.000004  \n",
      "INVEST           0.713022 -0.058486 -0.000044  \n",
      "VIXCLSx         -0.058486  0.838208  0.000014  \n",
      "BORROW          -0.000044  0.000014  0.000001  \n",
      "\n",
      "[118 rows x 118 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Comparison using libraries ###\n",
    "\n",
    "## Do not run! ##\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' is your DataFrame and contains the time series data with a DateTime index\n",
    "# Filter data between '1962-09-01' and '1999-12-01'\n",
    "data_filtered = data.loc['1962-09-01':'1999-12-01']\n",
    "\n",
    "# Perform PCA to extract the first r principal components\n",
    "r = 2  # Number of components (you can adjust this as needed)\n",
    "pca = PCA(n_components=r)\n",
    "pca.fit(data_filtered)\n",
    "\n",
    "# Get the first r principal components (F)\n",
    "F = pca.transform(data_filtered)\n",
    "\n",
    "# Get the factor loadings (the eigenvectors)\n",
    "factor_loadings = pca.components_.T  # Shape: (n_features, r)\n",
    "\n",
    "# Get the covariance matrix of the residuals (idiosyncratic component, R)\n",
    "# First, reconstruct the data from the principal components\n",
    "reconstructed_data = F @ factor_loadings.T\n",
    "residuals = data_filtered - reconstructed_data  # Residuals (idiosyncratic component)\n",
    "\n",
    "# Covariance matrix of the residuals (R)\n",
    "R = np.cov(residuals, rowvar=False)\n",
    "\n",
    "# Print results\n",
    "print(\"Factor Loadings (Eigenvectors):\")\n",
    "print(factor_loadings)\n",
    "print(\"\\nCovariance Matrix of the Idiosyncratic Component (R):\")\n",
    "print(R)\n",
    "\n",
    "# Optionally, if you want the variance explained by the factors\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"\\nExplained Variance (Proportion of variance explained by each component):\")\n",
    "print(explained_variance)\n"
   ],
   "metadata": {
    "id": "5gUY5-zEKkPT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### Comparison using libraries ###\n",
    "\n",
    "## Do not run! ##\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Assuming 'data' is the standardized dataset filtered for the desired period\n",
    "data_filtered = data.loc['1962-09-01':'1999-12-01']\n",
    "\n",
    "# Step 1: Perform PCA to extract the first r principal components\n",
    "r = 2  # Number of factors\n",
    "pca = PCA(n_components=r)\n",
    "F = pca.fit_transform(data_filtered)  # F is the matrix of factors (T x r)\n",
    "\n",
    "# Convert F into a DataFrame for easier manipulation in statsmodels\n",
    "F_df = pd.DataFrame(F, index=data_filtered.index, columns=[f\"Factor_{i+1}\" for i in range(r)])\n",
    "\n",
    "# Step 2: Fit a VAR model to the extracted factors\n",
    "var_model = VAR(F_df)\n",
    "var_result = var_model.fit(maxlags=1)  # Assuming a VAR(1) structure\n",
    "\n",
    "# Extract the autoregressive coefficient matrices (A)\n",
    "A = var_result.coefs  # Shape: (lags, r, r)\n",
    "# A[0] is the matrix of coefficients for the VAR(1) model\n",
    "\n",
    "# Extract the covariance matrix of residuals (H)\n",
    "H = var_result.sigma_u  # Covariance matrix of residuals\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(\"Autoregressive Coefficient Matrix (A):\")\n",
    "print(A[0])  # A[0] corresponds to the VAR(1) coefficient matrix\n",
    "\n",
    "print(\"\\nCovariance Matrix of Residuals (H):\")\n",
    "print(H)\n",
    "\n",
    "# Step 4: Optional - Explained Variance of the Principal Components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance (Proportion of variance explained by each factor):\")\n",
    "print(explained_variance)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16AgVAll2TRS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733587285331,
     "user_tz": -60,
     "elapsed": 333,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "b304d9ad-54c2-437a-c05d-d0a6f2c90717"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Autoregressive Coefficient Matrix (A):\n",
      "[[ 0.54153483  0.14582621]\n",
      " [-0.11952081  0.75600551]]\n",
      "\n",
      "Covariance Matrix of Residuals (H):\n",
      "          Factor_1  Factor_2\n",
      "Factor_1  9.158875 -0.432626\n",
      "Factor_2 -0.432626  4.844322\n",
      "\n",
      "Explained Variance (Proportion of variance explained by each factor):\n",
      "[0.1470868  0.12981402]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Do not run! ##\n",
    "\n",
    "## Kalman Filter (Forward Pass) Only:\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv, det\n",
    "\n",
    "# Assume the necessary matrices are already defined: y, A, C, Q, R, init_x, init_V, model\n",
    "\n",
    "# Step 1: Define the vintage cutoff date\n",
    "vintage_date = '1999-12-01'\n",
    "\n",
    "# Step 2: Filter data for rows strictly after the vintage date\n",
    "data_after_vintage = data.loc[vintage_date:]\n",
    "\n",
    "# Step 3: Drop the vintage date row itself (if necessary)\n",
    "data_after_vintage = data_after_vintage.iloc[1:]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Data After Vintages:\")\n",
    "print(data_after_vintage.shape)\n",
    "\n",
    "\n",
    "os = data_after_vintage.shape[1]  # Number of observed variables\n",
    "T = data_after_vintage.shape[0]   # Number of time steps (rows in y)\n",
    "ss = A.shape[0]  # State space size\n",
    "\n",
    "# Initialize filtered state and covariance arrays\n",
    "x = np.zeros((ss, T))          # Filtered state estimates\n",
    "V = np.zeros((ss, ss, T))      # State covariance matrices\n",
    "VV = np.zeros((ss, ss, T))     # Cross-covariance matrices\n",
    "loglik = 0                     # Log-likelihood accumulator\n",
    "\n",
    "T = data_after_vintage.shape[0]\n",
    "print(T)\n",
    "AA = np.repeat(A[:, :, np.newaxis], T, axis=2)\n",
    "QQ = np.repeat(Q[:, :, np.newaxis], T, axis=2)\n",
    "CC = np.repeat(C[:, :, np.newaxis], T, axis=2)\n",
    "RR = np.repeat(R[:, :, np.newaxis], T, axis=2)\n",
    "model = range(1, T+1)\n",
    "print(initV.shape)\n",
    "\n",
    "print(initx.shape)\n",
    "print(A.shape)\n",
    "\n",
    "# Forward pass: Run the Kalman filter\n",
    "for t in range(T):\n",
    "    m = model[t]\n",
    "\n",
    "    if t == 0:\n",
    "        prevx = initx # (2, ) 1-dimension array with two elements\n",
    "        prevV = initV # 2 by 2 matrix\n",
    "        initial = True\n",
    "    else:\n",
    "        prevx = x[:, t-1].reshape(-1, 1)\n",
    "        prevV = V[:, :, t-1]\n",
    "        initial = False\n",
    "\n",
    "    # Prediction step\n",
    "    if initial:\n",
    "        xpred = prevx\n",
    "        Vpred = prevV\n",
    "    else:\n",
    "        xpred = A @ prevx\n",
    "        Vpred = A @ prevV @ A.T + Q[:, :, t]\n",
    "\n",
    "    # Innovation\n",
    "    e = data_after_vintage[:, t].reshape(-1, 1) - C[:, :, t] @ xpred\n",
    "    S = C[:, :, t] @ Vpred @ C[:, :, t].T + R[:, :, t]\n",
    "    Sinv = np.linalg.inv(S)\n",
    "\n",
    "    # Log-likelihood calculation\n",
    "    detS = det(S)\n",
    "    loglik_step = -0.5 * (np.log(detS) + e.T @ Sinv @ e + len(e) * np.log(2 * np.pi))\n",
    "\n",
    "    # Kalman gain\n",
    "    K = Vpred @ C[:, :, t].T @ Sinv\n",
    "\n",
    "    # State and covariance update\n",
    "    x[:, t] = (xpred + K @ e).flatten()\n",
    "    V[:, :, t] = (np.eye(ss) - K @ C[:, :, t]) @ Vpred\n",
    "    VV[:, :, t] = (np.eye(ss) - K @ C[:, :, t]) @ A[:, :, m-1] @ Vpred\n",
    "\n",
    "    # Print current step outputs for verification\n",
    "    print(f\"Step {t + 1} - Kalman Filter Output\")\n",
    "    print(f\"x: {x[:, t]}\")\n",
    "    print(f\"V: {V[:, :, t]}\")\n",
    "    print(f\"Log-likelihood: {loglik_step.item()}\")\n",
    "    loglik += loglik_step.item()\n",
    "\n",
    "# After the forward pass, print the total log-likelihood\n",
    "print(f\"Total Log-Likelihood after Forward Pass: {loglik}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFdUUIUKO5TO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733739667057,
     "user_tz": -60,
     "elapsed": 294,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "4d60613b-40ee-4ade-8153-00857eb19c2a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "##############################\n",
    "### Restart execution here ###\n",
    "##############################\n",
    "\n",
    "## Kalman filter (forward pass) ##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assumed data\n",
    "vintage_date = '1999-12-01'\n",
    "\n",
    "# Step 1: Define the vintage cutoff date\n",
    "data_after_vintage = data.loc[vintage_date:]\n",
    "\n",
    "# Step 2: Drop the vintage date row itself\n",
    "data_after_vintage = data_after_vintage.iloc[1:]\n",
    "obs_data = data_after_vintage.values  # Dataset after the vintage\n",
    "\n",
    "# # Assumed parameters (from DFM estimation)\n",
    "\n",
    "print(A)\n",
    "print(V)\n",
    "print(C)\n",
    "print(R)\n",
    "print(A.shape)\n",
    "print(V.shape)\n",
    "print(C.shape)\n",
    "print(R.shape)\n",
    "# A = np.array([[0.8, 0.1], [0.1, 0.7]])  # VAR(1) coefficient matrix\n",
    "# V = np.array([[0.05, 0.0], [0.0, 0.05]])  # Process noise covariance matrix (replaces Q)\n",
    "# C = eigvecs  # Factor loadings matrix (from DFM estimation)\n",
    "# R = np.diag(np.diag(np.cov(x - F @ C.T, rowvar=False)))  # Covariance of idiosyncratic errors\n",
    "n_obs, n_factors = obs_data.shape[0], A.shape[0]\n",
    "\n",
    "# Initialization\n",
    "print(initx)\n",
    "print(V)\n",
    "x_0 = F[-1, :]  # Initial latent factors\n",
    "print(x_0)\n",
    "P_0 = V  # Steady-state covariance\n",
    "\n",
    "# Prepare storage\n",
    "F_estimates = []  # To store factor estimates\n",
    "P_estimates = []  # To store covariance estimates\n",
    "\n",
    "# Initialize\n",
    "F_t = x_0\n",
    "P_t = P_0\n",
    "\n",
    "# Forward pass\n",
    "for t in range(n_obs):\n",
    "    # Observation at current time step\n",
    "    x_t = obs_data[t, :]\n",
    "\n",
    "    # Prediction step\n",
    "    F_pred = A @ F_t  # Predicted state\n",
    "    P_pred = A @ P_t @ A.T + V  # Predicted covariance (using V)\n",
    "\n",
    "    # Update step\n",
    "    K_t = P_pred @ C.T @ np.linalg.inv(C @ P_pred @ C.T + R)  # Kalman gain\n",
    "    F_t = F_pred + K_t @ (x_t - C @ F_pred)  # Updated state estimate\n",
    "    P_t = (np.eye(n_factors) - K_t @ C) @ P_pred  # Updated covariance estimate\n",
    "\n",
    "    # Store results\n",
    "    F_estimates.append(F_t)\n",
    "    P_estimates.append(P_t)\n",
    "\n",
    "# Convert results to arrays\n",
    "F_estimates = np.array(F_estimates)\n",
    "P_estimates = np.array(P_estimates)\n",
    "\n",
    "print(\"Latent Factor Estimates:\")\n",
    "print(F_estimates) # F_estimates: A matrix where each row contains the estimated latent factors for each time step after the vintage date.\n",
    "print(\"State Covariance Estimates:\")\n",
    "print(P_estimates) # P_estimates: A matrix containing the covariance estimates of the latent factors for each time step.\n"
   ],
   "metadata": {
    "id": "UGEWv8Wpe6qd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Implementing a Kalman smoother ##\n",
    "\n",
    "# Assuming Kalman filter results: F_estimates (latent factors), P_estimates (covariance matrices)\n",
    "\n",
    "n_timesteps = F_estimates.shape[0]\n",
    "n_factors = F_estimates.shape[1]\n",
    "\n",
    "# Initialize smoother results\n",
    "F_smoothed = np.zeros_like(F_estimates)\n",
    "P_smoothed = np.zeros_like(P_estimates)\n",
    "\n",
    "# Start with the last Kalman filter estimates\n",
    "F_smoothed[-1] = F_estimates[-1]\n",
    "P_smoothed[-1] = P_estimates[-1]\n",
    "\n",
    "# Backward smoothing pass\n",
    "for t in range(n_timesteps - 2, -1, -1):  # Loop from T-1 to 0\n",
    "    # Smoother gain\n",
    "    P_t = P_estimates[t]\n",
    "    P_t1_pred = A @ P_t @ A.T + V\n",
    "    J_t = P_t @ A.T @ np.linalg.inv(P_t1_pred)\n",
    "\n",
    "    # Update smoothed state\n",
    "    F_smoothed[t] = F_estimates[t] + J_t @ (F_smoothed[t + 1] - A @ F_estimates[t])\n",
    "\n",
    "    # Update smoothed covariance\n",
    "    P_smoothed[t] = P_t + J_t @ (P_smoothed[t + 1] - P_t1_pred) @ J_t.T\n",
    "\n",
    "# Results\n",
    "print(\"Smoothed latent factors (F_smoothed):\")\n",
    "print(F_smoothed)\n",
    "print(\"Smoothed covariances (P_smoothed):\")\n",
    "print(P_smoothed)\n"
   ],
   "metadata": {
    "id": "UhzNRsnWk_LT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### Alternative implementation of the Kalman smoother (Real-Time Fixed-Lag Smoothing)\n",
    "\n",
    "# Kalman filter estimates (real-time)\n",
    "F_filtered = []  # Real-time factor estimates\n",
    "P_filtered = []  # Real-time covariance estimates\n",
    "L = 3\n",
    "\n",
    "# Real-time data stream (assume x_data is your observed data matrix)\n",
    "for t in range(n_obs):\n",
    "    # Kalman filter forward pass\n",
    "    F_t = A @ F_t_prev\n",
    "    P_t = A @ P_t_prev @ A.T + V\n",
    "\n",
    "    # Update with observation at time t\n",
    "    K_t = P_t @ C.T @ np.linalg.inv(C @ P_t @ C.T + R)\n",
    "    F_t = F_t + K_t @ (obs_data[t] - C @ F_t)\n",
    "    P_t = P_t - K_t @ C @ P_t\n",
    "\n",
    "    # Store estimates\n",
    "    F_filtered.append(F_t)\n",
    "    P_filtered.append(P_t)\n",
    "\n",
    "    # Perform fixed-lag smoothing\n",
    "    if t >= L:  # Smoothing possible after reaching lag size\n",
    "        for tau in range(t - L, t + 1):\n",
    "            J_tau = P_filtered[tau] @ A.T @ np.linalg.inv(P_filtered[tau + 1])\n",
    "            F_filtered[tau] = F_filtered[tau] + J_tau @ (F_filtered[tau + 1] - A @ F_filtered[tau])\n",
    "            P_filtered[tau] = P_filtered[tau] + J_tau @ (P_filtered[tau + 1] - P_filtered[tau + 1]) @ J_tau.T\n",
    "\n",
    "    # Store previous state for next iteration\n",
    "    F_t_prev = F_t\n",
    "    P_t_prev = P_t\n",
    "\n",
    "# Convert lists to arrays\n",
    "F_filtered = np.array(F_filtered)\n",
    "P_filtered = np.array(P_filtered)\n",
    "\n",
    "# Nowcast using the smoothed factors\n",
    "print(\"Real-time smoothed factors for nowcasting:\", F_filtered[-L:])\n"
   ],
   "metadata": {
    "id": "asQcl4RoBO-V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Kalman filter and smoother\n",
    "# Kalman filter and smoother initialization\n",
    "# print(T)\n",
    "# print(N)\n",
    "\n",
    "# Kalman filter initialization\n",
    "F_t_prev = initx  # Initial latent factors\n",
    "P_t_prev = V  # Initial covariance matrix\n",
    "\n",
    "F_filtered = []  # Store real-time filtered estimates\n",
    "P_filtered = []  # Store covariance matrices\n",
    "F_smoothed = []  # Store smoothed estimates\n",
    "\n",
    "T2 = len(obs_data)\n",
    "\n",
    "# Define a fixed lag (3 months for quarterly lag)\n",
    "L = 3  # Use 3 months for quarterly smoothing\n",
    "\n",
    "# print(obs_data)\n",
    "\n",
    "# Real-Time Fixed-Lag Smoothing (Quarterly)\n",
    "for t in range(T2):\n",
    "    # Kalman Filter Forward Pass\n",
    "    # Predict step\n",
    "    F_t_pred = A @ F_t_prev  # Predict latent factors\n",
    "    P_t_pred = A @ P_t_prev @ A.T + V  # Predict covariance\n",
    "\n",
    "    # Check if t is within the bounds of obs_data\n",
    "    if t < len(obs_data):\n",
    "        # Update step with observation obs_data[t]\n",
    "        K_t = P_t_pred @ C.T @ np.linalg.inv(C @ P_t_pred @ C.T + R)  # Kalman gain\n",
    "        F_t = F_t_pred + K_t @ (obs_data[t] - C @ F_t_pred)  # Update factors\n",
    "        P_t = P_t_pred - K_t @ C @ P_t_pred  # Update covariance\n",
    "\n",
    "    # Store filtered estimates\n",
    "    F_filtered.append(F_t)\n",
    "    P_filtered.append(P_t)\n",
    "\n",
    "    # Real-Time Fixed-Lag Smoothing (quarterly smoothing)\n",
    "    # Perform smoothing after each quarter (March, June, September, December)\n",
    "    if (t + 1) % 3 == 0:  # Check if t is the end of a quarter (March, June, September, December)\n",
    "        for tau in range(max(0, t - L + 1), t + 1):  # Ensure valid range\n",
    "            if tau + 1 < len(F_filtered):\n",
    "                # Smoothing formula: Use all earlier estimates (up to the end of the current quarter)\n",
    "                J_tau = P_filtered[tau] @ A.T @ np.linalg.inv(P_t_pred)\n",
    "                F_filtered[tau] = F_filtered[tau] + J_tau @ (F_filtered[tau + 1] - A @ F_filtered[tau])\n",
    "                P_filtered[tau] = P_filtered[tau] + J_tau @ (P_filtered[tau + 1] - P_t_pred) @ J_tau.T\n",
    "            else:\n",
    "                print(f\"Skipping tau: {tau}, t: {t} due to index out of range.\")\n",
    "\n",
    "    # Store smoothed estimates for debugging or further analysis\n",
    "    F_smoothed.append(F_filtered[t])\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    F_t_prev = F_t\n",
    "    P_t_prev = P_t\n",
    "\n",
    "# Convert lists to arrays for further analysis\n",
    "F_filtered = np.array(F_filtered)\n",
    "P_filtered = np.array(P_filtered)\n",
    "F_smoothed = np.array(F_smoothed)\n",
    "\n",
    "# Print smoothed factors\n",
    "print(\"Filtered latent factors (F_filtered):\", F_filtered.shape)\n",
    "print(\"Smoothed latent factors (F_smoothed):\", F_smoothed.shape)\n",
    "\n",
    "print(F_filtered)\n",
    "print(F_smoothed)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iflbq16nFRLr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733845853754,
     "user_tz": -60,
     "elapsed": 898,
     "user": {
      "displayName": "Eugenio Marinelli",
      "userId": "15230783770735112345"
     }
    },
    "outputId": "71844190-f099-45f6-fbea-a14d9018efa4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping tau: 2, t: 2 due to index out of range.\n",
      "Skipping tau: 5, t: 5 due to index out of range.\n",
      "Skipping tau: 8, t: 8 due to index out of range.\n",
      "Skipping tau: 11, t: 11 due to index out of range.\n",
      "Skipping tau: 14, t: 14 due to index out of range.\n",
      "Skipping tau: 17, t: 17 due to index out of range.\n",
      "Skipping tau: 20, t: 20 due to index out of range.\n",
      "Skipping tau: 23, t: 23 due to index out of range.\n",
      "Skipping tau: 26, t: 26 due to index out of range.\n",
      "Skipping tau: 29, t: 29 due to index out of range.\n",
      "Skipping tau: 32, t: 32 due to index out of range.\n",
      "Skipping tau: 35, t: 35 due to index out of range.\n",
      "Skipping tau: 38, t: 38 due to index out of range.\n",
      "Skipping tau: 41, t: 41 due to index out of range.\n",
      "Skipping tau: 44, t: 44 due to index out of range.\n",
      "Skipping tau: 47, t: 47 due to index out of range.\n",
      "Skipping tau: 50, t: 50 due to index out of range.\n",
      "Skipping tau: 53, t: 53 due to index out of range.\n",
      "Skipping tau: 56, t: 56 due to index out of range.\n",
      "Skipping tau: 59, t: 59 due to index out of range.\n",
      "Skipping tau: 62, t: 62 due to index out of range.\n",
      "Skipping tau: 65, t: 65 due to index out of range.\n",
      "Skipping tau: 68, t: 68 due to index out of range.\n",
      "Skipping tau: 71, t: 71 due to index out of range.\n",
      "Skipping tau: 74, t: 74 due to index out of range.\n",
      "Skipping tau: 77, t: 77 due to index out of range.\n",
      "Skipping tau: 80, t: 80 due to index out of range.\n",
      "Skipping tau: 83, t: 83 due to index out of range.\n",
      "Skipping tau: 86, t: 86 due to index out of range.\n",
      "Skipping tau: 89, t: 89 due to index out of range.\n",
      "Skipping tau: 92, t: 92 due to index out of range.\n",
      "Skipping tau: 95, t: 95 due to index out of range.\n",
      "Skipping tau: 98, t: 98 due to index out of range.\n",
      "Skipping tau: 101, t: 101 due to index out of range.\n",
      "Skipping tau: 104, t: 104 due to index out of range.\n",
      "Skipping tau: 107, t: 107 due to index out of range.\n",
      "Skipping tau: 110, t: 110 due to index out of range.\n",
      "Skipping tau: 113, t: 113 due to index out of range.\n",
      "Skipping tau: 116, t: 116 due to index out of range.\n",
      "Skipping tau: 119, t: 119 due to index out of range.\n",
      "Skipping tau: 122, t: 122 due to index out of range.\n",
      "Skipping tau: 125, t: 125 due to index out of range.\n",
      "Skipping tau: 128, t: 128 due to index out of range.\n",
      "Skipping tau: 131, t: 131 due to index out of range.\n",
      "Skipping tau: 134, t: 134 due to index out of range.\n",
      "Skipping tau: 137, t: 137 due to index out of range.\n",
      "Skipping tau: 140, t: 140 due to index out of range.\n",
      "Skipping tau: 143, t: 143 due to index out of range.\n",
      "Skipping tau: 146, t: 146 due to index out of range.\n",
      "Skipping tau: 149, t: 149 due to index out of range.\n",
      "Skipping tau: 152, t: 152 due to index out of range.\n",
      "Skipping tau: 155, t: 155 due to index out of range.\n",
      "Skipping tau: 158, t: 158 due to index out of range.\n",
      "Skipping tau: 161, t: 161 due to index out of range.\n",
      "Skipping tau: 164, t: 164 due to index out of range.\n",
      "Skipping tau: 167, t: 167 due to index out of range.\n",
      "Skipping tau: 170, t: 170 due to index out of range.\n",
      "Skipping tau: 173, t: 173 due to index out of range.\n",
      "Skipping tau: 176, t: 176 due to index out of range.\n",
      "Skipping tau: 179, t: 179 due to index out of range.\n",
      "Skipping tau: 182, t: 182 due to index out of range.\n",
      "Skipping tau: 185, t: 185 due to index out of range.\n",
      "Skipping tau: 188, t: 188 due to index out of range.\n",
      "Skipping tau: 191, t: 191 due to index out of range.\n",
      "Skipping tau: 194, t: 194 due to index out of range.\n",
      "Skipping tau: 197, t: 197 due to index out of range.\n",
      "Skipping tau: 200, t: 200 due to index out of range.\n",
      "Skipping tau: 203, t: 203 due to index out of range.\n",
      "Skipping tau: 206, t: 206 due to index out of range.\n",
      "Skipping tau: 209, t: 209 due to index out of range.\n",
      "Skipping tau: 212, t: 212 due to index out of range.\n",
      "Skipping tau: 215, t: 215 due to index out of range.\n",
      "Skipping tau: 218, t: 218 due to index out of range.\n",
      "Skipping tau: 221, t: 221 due to index out of range.\n",
      "Skipping tau: 224, t: 224 due to index out of range.\n",
      "Skipping tau: 227, t: 227 due to index out of range.\n",
      "Skipping tau: 230, t: 230 due to index out of range.\n",
      "Skipping tau: 233, t: 233 due to index out of range.\n",
      "Skipping tau: 236, t: 236 due to index out of range.\n",
      "Skipping tau: 239, t: 239 due to index out of range.\n",
      "Skipping tau: 242, t: 242 due to index out of range.\n",
      "Skipping tau: 245, t: 245 due to index out of range.\n",
      "Skipping tau: 248, t: 248 due to index out of range.\n",
      "Skipping tau: 251, t: 251 due to index out of range.\n",
      "Skipping tau: 254, t: 254 due to index out of range.\n",
      "Skipping tau: 257, t: 257 due to index out of range.\n",
      "Skipping tau: 260, t: 260 due to index out of range.\n",
      "Skipping tau: 263, t: 263 due to index out of range.\n",
      "Skipping tau: 266, t: 266 due to index out of range.\n",
      "Skipping tau: 269, t: 269 due to index out of range.\n",
      "Skipping tau: 272, t: 272 due to index out of range.\n",
      "Skipping tau: 275, t: 275 due to index out of range.\n",
      "Skipping tau: 278, t: 278 due to index out of range.\n",
      "Skipping tau: 281, t: 281 due to index out of range.\n",
      "Skipping tau: 284, t: 284 due to index out of range.\n",
      "Skipping tau: 287, t: 287 due to index out of range.\n",
      "Skipping tau: 290, t: 290 due to index out of range.\n",
      "Skipping tau: 293, t: 293 due to index out of range.\n",
      "Filtered latent factors (F_filtered): (295, 2)\n",
      "Smoothed latent factors (F_smoothed): (295, 2)\n"
     ]
    }
   ]
  }
 ]
}
